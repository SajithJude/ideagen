{
    "nodes": [
        {
            "id": "CNN",
            "Using for": "Classification",
            "Based on": "NaN",
            "Method": "Convolutional Neural Network",
            "Description": "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.\n\nCNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually refer to fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. However, CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.",
            "Publication date": 1989.0,
            "Publication link": "http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf",
            "Publication name": "Object Recognition with Gradient Based Learning",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Yann LeCun,Patrick Haffner, L\u00e9on Bottou, Yoshua Bengio",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "FOSD",
            "Using for": "Regression",
            "Based on": "NaN",
            "Method": "First-order stochastic dominance",
            "Description": "We consider the minimization of submodular functions subject to ordering con- straints. We show that this potentially non-convex optimization problem can be cast as a convex optimization problem on a space of uni-dimensional measures, with ordering constraints corresponding to first-order stochastic dominance. We propose new discretization schemes that lead to simple and efficient algorithms based on zero-th, first, or higher order oracles; these algorithms also lead to improvements without isotonic constraints. Finally, our experiments show that non-convex loss functions can be much more robust to outliers for isotonic regression, while still being solvable in polynomial time.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7286-efficient-algorithms-for-non-convex-isotonic-regression-through-submodular-optimization",
            "Publication name": "Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Francis Bach",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MLP",
            "Using for": "Regression, Classification",
            "Based on": "NaN",
            "Method": "Multilayer Perceptron",
            "Description": "Authors describe a learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure",
            "Publication date": 1986.0,
            "Publication link": "https://www.nature.com/articles/323533a0",
            "Publication name": "Learning internal representations by back-propagating errors.",
            "Released at": "Nature",
            "Interacts with": "NaN",
            "Authors": "David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams ",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LSTM",
            "Using for": "Recurrent Networks",
            "Based on": "RNN",
            "Method": "Long Short-Term Memory",
            "Description": "Unlike standard feedforward neural networks, LSTM has feedback connections that make it a \"general purpose computer\". RNNs using LSTM units partially solve the vanishing gradient problem, because LSTM units allow gradients to also flow unchanged. However, LSTM networks can still suffer from the exploding gradient problem. It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.",
            "Publication date": 1997.0,
            "Publication link": "https://www.bioinf.jku.at/publications/older/2604.pdf",
            "Publication name": "Long Short-Term Memory",
            "Released at": "Neural computation 9(8)",
            "Interacts with": "NaN",
            "Authors": "Sepp Hochreiter Jurgen Schmidhuber",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "STAC",
            "Using for": "Classification",
            "Based on": "CNN",
            "Method": "Structure-Aware Convolution",
            "Description": "The structure-aware convolution is developed to establish SACNNs to uniformly deal with both Euclidean and non-Euclidean structured data, which broadens the reach of convolution. Authors introduce the learnable local structure representations, which endow SACNNs with the capability of capturing the latent structures of data in a purely data-driven way. \nIn structure-aware convolution a single shareable filter suffices to aggregate local inputs with diverse topological structures. For this purpose, authors generalize the classical filters to univariate functions that can be effectively and efficiently parameterized under the guidance of the function approximation theory. Then, authors introduce local structure representations to quantificationally encode topological structures. By modeling these representations into the generalized filters, the corresponding local inputs can be aggregated based on the generalized filters consequently. In practice, Structure-Aware Convolutional Neural Networks (SACNNs) can be readily established by replacing the classical convolution in CNNs with this structure-awar convolution.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7287-structure-aware-convolutional-neural-networks.pdf",
            "Publication name": "Structure-Aware Convolutional Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "MLP, LSTM",
            "Authors": "Jianlong Chang, Jie Gu, Lingfeng Wang, GAOFENG MENG, SHIMING XIANG, Chunhong Pan",
            "Code link": "https://github.com/vector-1127/SACNNs.",
            "Data": "MNIST, Cifar-10, Cifar-100, STL-10, Image-10, ImageDog, 20News, Reuters, NTU, DPP4, TF-198"
        },
        {
            "id": "BN",
            "Using for": "Normalization",
            "Based on": "NaN",
            "Method": "Batch Normalization",
            "Description": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1502.03167",
            "Publication name": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Sergey Ioffe, Christian Szegedy",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GN",
            "Using for": "Normalization",
            "Based on": "NaN",
            "Method": "Group Normalization",
            "Description": "Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1803.08494",
            "Publication name": "Group Normalization",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Yuxin Wu, Kaiming He",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "KN",
            "Using for": "Normalization",
            "Based on": "BN, GN, NCMN, DRPT",
            "Method": "Kalman Normalization",
            "Description": "Unlike previous methods that normalized each hidden layer independently, KN treats the entire network as a whole. KN can be naturally generalized to other existing normalization methods to obtain gains. Extensive experiments suggest that KN is capable of strengthening several state-of-the-art neural networks by improving their training stability and convergence speed. More importantly, KN can handle the training with mini-batches of very small sizes. KN achieving more accurate estimation of the statistics (means and variances) of the internal representations in DNNs. Unlike BN where the statistics were estimated by only measuring the mini-batches within a certain layer, i.e.they considered each layer in the network as an isolated sub-system, KN shows that the estimated statistics have strong correlations among the sequential layers. And the estimations can be more accurate by jointly considering its preceding layers in the network. KN performs two steps in an iterative way. In the first step, KN estimates the statistics of the current layer conditioned on the estimations of the previous layer. In the second step, these estimations are combined with the observed batch sample means and variances calculated within a mini-batch.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7288-kalman-normalization-normalizing-internal-representations-across-network-layers",
            "Publication name": "Kalman Normalization: Normalizing Internal Representations Across Network Layers",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Guangrun Wang, jiefeng peng, Ping Luo, Xinjiang Wang, Liang Lin",
            "Code link": "None",
            "Data": "COCO 17"
        },
        {
            "id": "HOGWILDS",
            "Using for": "Optimization, Asynchronicity. Functions estimating",
            "Based on": "NaN",
            "Method": "HOGWILD!-SGD",
            "Description": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.",
            "Publication date": 2011.0,
            "Publication link": "https://arxiv.org/abs/1106.5730",
            "Publication name": "HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Feng Niu, Benjamin Recht, Christopher Re, Stephen J. Wright",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HOGWILDG",
            "Using for": "Optimization, Asynchronicity, Functions estimating",
            "Based on": "HOGWILD!-SGD",
            "Method": "HOGWILD!-Gibbs",
            "Description": "Goal in this paper is to push the theoretical understanding of HOGWILD!-Gibbs to estimate functions of all the variables in a graphical model. In particular, authors are interested in whether HOGWILD!-Gibbs can be used to accurately estimate the expectations of such functions. More generally, it is used to accurately estimate the function expectations of all model variables",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7289-hogwild-gibbs-can-be-panaccurate",
            "Publication name": "HOGWILD!-Gibbs can be PanAccurate",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Constantinos Daskalakis, Nishanth Dikkala, Siddhartha Jayanti",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "ENCDR",
            "Using for": "Autoencoders",
            "Based on": "NaN",
            "Method": "Encoder",
            "Description": "Idea of autoencoding where the goal is to learn a mapping from high-dimensional observations to a lower-dimensional representation space (by encoder), such that the original observations can be reconstructed (by decoder approximately) from the lower-dimensional representation.",
            "Publication date": 1987.0,
            "Publication link": "https://www.aaai.org/Papers/AAAI/1987/AAAI87-050.pdf",
            "Publication name": "Autoencoder",
            "Released at": "AAAI",
            "Interacts with": "DCDR",
            "Authors": "Dana H. Ballard",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DCDR",
            "Using for": "Autoencoders",
            "Based on": "NaN",
            "Method": "Decoder",
            "Description": "Idea of autoencoding where the goal is to learn a mapping from high-dimensional observations to a lower-dimensional representation space (by encoder), such that the original observations can be reconstructed (by decoder approximately) from the lower-dimensional representation.",
            "Publication date": 1987.0,
            "Publication link": "https://www.aaai.org/Papers/AAAI/1987/AAAI87-050.pdf",
            "Publication name": "Autoencoder",
            "Released at": "AAAI",
            "Interacts with": "ENCDR",
            "Authors": "Dana H. Ballard",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "AE",
            "Using for": "Autoencoders",
            "Based on": "ENCDR, DCDR,",
            "Method": "Auto-Encoder",
            "Description": "Idea of autoencoding where the goal is to learn a mapping from high-dimensional observations to a lower-dimensional representation space (by encoder), such that the original observations can be reconstructed (by decoder approximately) from the lower-dimensional representation.",
            "Publication date": 1987.0,
            "Publication link": "https://www.aaai.org/Papers/AAAI/1987/AAAI87-050.pdf",
            "Publication name": "Autoencoder",
            "Released at": "AAAI",
            "Interacts with": "NaN",
            "Authors": "Dana H. Ballard",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GENN",
            "Using for": "Adversarial Networks",
            "Based on": "NaN",
            "Method": "Generator network",
            "Description": "Framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two models: a generative model G\nthat captures the data distribution, and a discriminative model D that estimates\nthe probability that a sample came from the training data rather than G. The train-\ning procedure for G is to maximize the probability of D making a mistake. This\nframework corresponds to a minimax two-player game. In the space of arbitrary\nfunctions G and D, a unique solution exists, with G recovering the training data\ndistribution and D equal to 1 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples",
            "Publication date": 2014.0,
            "Publication link": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "Publication name": "Generative Adversarial Nets",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DIS",
            "Using for": "Adversarial Networks",
            "Based on": "NaN",
            "Method": "Discriminator network",
            "Description": "Framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two models: a generative model G\nthat captures the data distribution, and a discriminative model D that estimates\nthe probability that a sample came from the training data rather than G. The train-\ning procedure for G is to maximize the probability of D making a mistake. This\nframework corresponds to a minimax two-player game. In the space of arbitrary\nfunctions G and D, a unique solution exists, with G recovering the training data\ndistribution and D equal to 1 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples",
            "Publication date": 2014.0,
            "Publication link": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "Publication name": "Generative Adversarial Nets",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GAN",
            "Using for": "Adversarial Networks",
            "Based on": "GENN, DIS",
            "Method": "Generative adversarial networks",
            "Description": "Framework for estimating generative models via an adversar-\nial process, in which we simultaneously train two models: a generative model G\nthat captures the data distribution, and a discriminative model D that estimates\nthe probability that a sample came from the training data rather than G. The train-\ning procedure for G is to maximize the probability of D making a mistake. This\nframework corresponds to a minimax two-player game. In the space of arbitrary\nfunctions G and D, a unique solution exists, with G recovering the training data\ndistribution and D equal to 1 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples",
            "Publication date": 2014.0,
            "Publication link": "https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf",
            "Publication name": "Generative Adversarial Nets",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "TADIS",
            "Using for": "Manipulating images using natural language description, Adversarial Networks",
            "Based on": "DIS",
            "Method": "Text-adaptive discriminator",
            "Description": "The motivation of the text-adaptive discriminator is to provide the generator with a specified training signal to generate certain visual attributes. To achieve this, the discriminator classifies each attribute independently using word-level local discriminators. By doing so, the generator receives feedback from each local discriminator for each visual attribute. Instead of learning sentence-level correspondence, authors train the discriminator to first identify individual attributes in a sentence, then to find existence of each attribute in the image. Also, this method can be easily trained by the cross-entropy loss without the ranking loss due to our multiplicative aggregation of scores. Although this method does not use explicit spatial attention on images and does not need any hand-tuned hyperparameters.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7290-text-adaptive-generative-adversarial-networks-manipulating-images-with-natural-language",
            "Publication name": "Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language",
            "Released at": "NIPS",
            "Interacts with": "GENN, ENCDR, Text ENCDR, TAGAN",
            "Authors": "Seonghyeon Nam, Yunji Kim, Seon Joo Kim",
            "Code link": "https://github.com/woozzu/tagan",
            "Data": "CUB dataset, Oxford-102 dataset"
        },
        {
            "id": "TAGAN",
            "Using for": "Manipulating images using natural language description, Adversarial Networks",
            "Based on": "GENN, ENCDR, Text ENCDR, TADIS",
            "Method": "Text-adaptive GAN",
            "Description": "The motivation of the text-adaptive discriminator is to provide the generator with a specified training signal to generate certain visual attributes. To achieve this, the discriminator classifies each attribute independently using word-level local discriminators. By doing so, the generator receives feedback from each local discriminator for each visual attribute. Instead of learning sentence-level correspondence, authors train the discriminator to first identify individual attributes in a sentence, then to find existence of each attribute in the image. Also, this method can be easily trained by the cross-entropy loss without the ranking loss due to our multiplicative aggregation of scores. Although this method does not use explicit spatial attention on images and does not need any hand-tuned hyperparameters.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7290-text-adaptive-generative-adversarial-networks-manipulating-images-with-natural-language",
            "Publication name": "Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language",
            "Released at": "NIPS",
            "Interacts with": "GENN, ENCDR, Text ENCDR, TADIS",
            "Authors": "Seonghyeon Nam, Yunji Kim, Seon Joo Kim",
            "Code link": "https://github.com/woozzu/tagan",
            "Data": "CUB dataset, Oxford-102 dataset"
        },
        {
            "id": "IVI",
            "Using for": "Generation, Variational Inference",
            "Based on": "VAE",
            "Method": "Introspective Variational Inference",
            "Description": "IntroVAE trains in an introspective manner such that the model can self-estimate the differences between the generated samples and the training data and then updates itself to produce more realistic samples. To achieve this goal, one part of the model needs to discriminate the generated samples from the training data, and another part should mislead the former part, analogous to the generator and discriminator in GANs. Specifically, authors select the approximate inference model (or encoder) of VAEs as the discriminator of GANs and the generator model of VAEs as the generator of GANs. In addition to performing adversarial learning like GANs, the inference and generator models are also expected to train jointly for the given training data to preserve the advantages of VAEs",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7291-introvae-introspective-variational-autoencoders-for-photographic-image-synthesis",
            "Publication name": "IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis",
            "Released at": "NIPS",
            "Interacts with": "ENCDR, DCDR, INFM",
            "Authors": "Huaibo Huang, Zhihang li, Ran He, Zhenan Sun, Tieniu Tan",
            "Code link": "https://github.com/moskomule/introvae.pytorch",
            "Data": "CelebA, CelebA-HQ, LSUN BEDROOM"
        },
        {
            "id": "BLR",
            "Using for": "Probabilistic Methods, Regression",
            "Based on": "NaN",
            "Method": "Bayesian Linear Regression",
            "Description": "Is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model's parameters.",
            "Publication date": 1973.0,
            "Publication link": "https://leseprobe.buch.de/images-adb/7c/be/7cbe9b5f-07eb-4a3c-b338-00c98ce31390.pdf",
            "Publication name": "Bayesian Inference in Statistical Analysis.",
            "Released at": "Wiley",
            "Interacts with": "NaN",
            "Authors": "Box G. E. P., Tiao G. C.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RBOCPD",
            "Using for": "Changepoint detection",
            "Based on": "NaN",
            "Method": "Robust Bayesian Online Changepoint Detection",
            "Description": "This paper has presented the very first robust Bayesian on-line changepoint  detection algorithm and the first ever scalable General Bayesian Inference (GBI) method. While CP detection is a particularly salient example of unaddressed heterogeneity and outliers leading to poor inference, the capabilities of GBI and the Structural Variational approximations presented extend far beyond this setting. With an ever increasing interest in the field of machine learning to efficiently and reliably quantify uncertainty, robust probabilistic inference will only become more relevant. In this paper, we give a particularly striking demonstration of the inferential power that can be unlocked through divergence-based General Bayesian inference.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7292-doubly-robust-bayesian-inference-for-non-stationary-streaming-data-with-beta-divergences",
            "Publication name": "Doubly Robust Bayesian Inference for Non-Stationary Streaming Data with \\beta-Divergences",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jeremias Knoblauch, Jack E. Jewson, Theodoros Damoulas",
            "Code link": "https://github.com/alan-turing-institute/rbocpdms/.",
            "Data": "Well-log data set"
        },
        {
            "id": "HISTLOSS",
            "Using for": "Similarity",
            "Based on": "NaN",
            "Method": "Histogram Loss",
            "Description": "The new loss does not introduce parameters that need to be tuned and results in very good embeddings across a range of datasets and problems. The loss is computed by estimating two distribution of similarities for positive (matching) and negative (non-matching) sample pairs, and then computing the probability of a positive pair to have a lower similarity score than a negative pair based on the estimated similarity distributions. We show that such operations can be performed in a simple and piecewise-differentiable manner using 1D histograms with soft assignment operations. This makes the proposed loss suitable for learning deep embeddings using stochastic optimization. In the experiments, the new loss performs favourably compared to recently proposed alternatives.",
            "Publication date": 2016.0,
            "Publication link": "https://papers.nips.cc/paper/6464-learning-deep-embeddings-with-histogram-loss",
            "Publication name": "Learning Deep Embeddings with Histogram Loss",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Evgeniya Ustinova, Victor Lempitsky",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PROTONET",
            "Using for": "Few-Shot Learning, Zero-Shot Learning",
            "Based on": "NaN",
            "Method": "Prototypical Networks \n",
            "Description": "Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning.",
            "Publication date": 2017.0,
            "Publication link": "https://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning",
            "Publication name": "Prototypical Networks for Few-shot Learning.",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Snell, J., Swersky, K., and Zemel, R. \n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "EMBD",
            "Using for": "Embedding",
            "Based on": "NaN",
            "Method": "Embedding",
            "Description": "Class of methods that projecting an input into another more convenient representation space.",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ADEM",
            "Using for": "Transfer Learning",
            "Based on": "NaN",
            "Method": "Adapted-embedding",
            "Description": "The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Authors propose a hybrid approach, adapted embeddings, that combines loss functions for deep embeddings with weight adaptation in the target domain. This hybrid approach robustly outperforms every few-shot learning and every deep metric learning method previously proposed on k-ITL. The performance differences are not in tiny percentage error reductions that distinguish contemporary methods, but are systematic and meaningful: a mean error reduction of 34% over state-of-the-art.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7293-adapted-deep-embeddings-a-synthesis-of-methods-for-k-shot-inductive-transfer-learning",
            "Publication name": "Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Tyler Scott, Karl Ridgeway, Michael C. Mozer",
            "Code link": "https://github.com/tylersco/adapted_deep_embeddings.",
            "Data": "MNIST, Omniglot, Isolet, tinyImageNet"
        },
        {
            "id": "ADAPTHISTLOSS",
            "Using for": "Transfer Learning",
            "Based on": "ADEM, HISTLOSS",
            "Method": "Adapted-HISTLOSS",
            "Description": "The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Authors propose a hybrid approach, adapted embeddings, that combines loss functions for deep embeddings with weight adaptation in the target domain. This hybrid approach robustly outperforms every few-shot learning and every deep metric learning method previously proposed on k-ITL. The performance differences are not in tiny percentage error reductions that distinguish contemporary methods, but are systematic and meaningful: a mean error reduction of 34% over state-of-the-art.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7293-adapted-deep-embeddings-a-synthesis-of-methods-for-k-shot-inductive-transfer-learning",
            "Publication name": "Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Tyler Scott, Karl Ridgeway, Michael C. Mozer",
            "Code link": "https://github.com/tylersco/adapted_deep_embeddings.",
            "Data": "MNIST, Omniglot, Isolet, tinyImageNet"
        },
        {
            "id": "ADAPTPROTONET",
            "Using for": "Transfer Learning",
            "Based on": "ADEM, PROTONET",
            "Method": "Adapted-PROTONET",
            "Description": "The focus in machine learning has branched beyond training classifiers on a single task to investigating how previously acquired knowledge in a source domain can be leveraged to facilitate learning in a related target domain, known as inductive transfer learning. Authors propose a hybrid approach, adapted embeddings, that combines loss functions for deep embeddings with weight adaptation in the target domain. This hybrid approach robustly outperforms every few-shot learning and every deep metric learning method previously proposed on k-ITL. The performance differences are not in tiny percentage error reductions that distinguish contemporary methods, but are systematic and meaningful: a mean error reduction of 34% over state-of-the-art.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7293-adapted-deep-embeddings-a-synthesis-of-methods-for-k-shot-inductive-transfer-learning",
            "Publication name": "Adapted Deep Embeddings: A Synthesis of Methods for k-Shot Inductive Transfer Learning",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Tyler Scott, Karl Ridgeway, Michael C. Mozer",
            "Code link": "https://github.com/tylersco/adapted_deep_embeddings.",
            "Data": "MNIST, Omniglot, Isolet, tinyImageNet"
        },
        {
            "id": "FICOF",
            "Using for": "Online Learning",
            "Based on": "None",
            "Method": "Framework for imputing a convex objective function",
            "Description": "Authors presented a framework for imputing a convex objective function, based on observations of the underlying optimizing process. They are develop this framework by noting that if the data is optimal with respect to the imputed objective, the primal and dual variables must satisfy the Kharush-Kuhm-Tucker conditions; since the dual variables and the objective parameters enter these conditions linearly, we have a convex optimization problem. In the presence of noise or modeling errors, authors minimize the residuals of the Kharush-Kuhm-Tucker conditions under a suitable regularization function.",
            "Publication date": 2011.0,
            "Publication link": "http://www.web.stanford.edu/~boyd/papers/pdf/imputed_objective_msc.pdf",
            "Publication name": "Imputing a convex objective function",
            "Released at": "ISIC",
            "Interacts with": "None",
            "Authors": "Arezou Keshavarz, Yang Wang, and Stephen Boyd",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "FIOL",
            "Using for": "Online Learning",
            "Based on": "None",
            "Method": "Framework for inverse optimization through online learning",
            "Description": "This work is most related to the subject of inverse optimization with multiple observations. The goal is to find an objective function or constraints that explains the observations well.\nAuthors prove a regret bound for the implicit online learning algorithm under certain regularity conditions, and show the algorithm is statistically consistent, which guarantees that this algorithm will asymptotically achieves the best prediction error permitted by the inverse model. To the best of authors\u2019 knowledge, authors propose the first general framework for eliciting decision maker\u2019s PR using inverse optimization through online learning. This framework can learn general convex utility functions and constraints with observed (signal, noisy decision) pairs.\nFinally, authors illustrate the performance of this learning method on both a consumer behavior problem and a transshipment problem. Results show that this algorithm can learn the parameters with great accuracy and is very robust to noises, and achieves drastic improvement in computational efficacy over the batch learning approach.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7294-generalized-inverse-optimization-through-online-learning",
            "Publication name": "Generalized Inverse Optimization through Online Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Chaosheng Dong, Yiran Chen, Bo Zeng",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "GTD",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Gradient Time Difference",
            "Description": "NaN",
            "Publication date": 2011.0,
            "Publication link": "http://incompleteideas.net/Talks/gradient-TD-2011.pdf",
            "Publication name": "Gradient Temporal-Difference Learning Algorithms",
            "Released at": "PhD thesis, Uni- versity of Alberta.",
            "Interacts with": "None",
            "Authors": "Hamid Maei",
            "Code link": "https://github.com/chrodan/tdlearn",
            "Data": "NaN"
        },
        {
            "id": "LSTD",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Least-Squares Time Difference",
            "Description": "NaN",
            "Publication date": 2003.0,
            "Publication link": "http://web.mit.edu/dimitrib/www/Policy_Eval.pdf",
            "Publication name": "Least squares policy evaluation algorithms with linear function approximation",
            "Released at": "Discrete Event Dynamic Systems 13",
            "Interacts with": "None",
            "Authors": "A. Nedi \u0301c",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "EMPTD",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Emphatic TIme-Difference Learning",
            "Description": "The idea of this method is improving the performance of parametric temporal- difference (TD) learning algorithms by selectively emphasizing or de-emphasizing their updates on different time steps. In particular, authors show that varying the emphasis of linear TD(\u03bb)\u2019s updates in a particular way causes its expected update to become stable under off-policy training. Authors have introduced a way of varying the emphasis or strength of the updates of TD learning algorithms from step to step, based on importance sampling. Compared to gradient-TD methods, emphatic TD(\u03bb) is simpler in that it has a single parameter vector and a single step size rather than two of each. The per-time-step complexities of gradient-TD and emphatic-TD methods are both linear in the number of parameters; both are much simpler than quadratic complexity methods such LSTD(\u03bb) and its off-policy variants.",
            "Publication date": 2016.0,
            "Publication link": "http://jmlr.org/papers/volume17/14-488/14-488.pdf",
            "Publication name": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning",
            "Released at": "The Journal of Machine Learning Research",
            "Interacts with": "None",
            "Authors": "Richard S Sutton, A R Mahmood, and Martha White",
            "Code link": "https://github.com/Pulkit-Khandelwal/Reinforcement-Learning-Notebooks/tree/master/Emphatic%20Temporal-Difference%20Learning",
            "Data": "MDP"
        },
        {
            "id": "AC",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Actor-Critic",
            "Description": "(a) Actor-only methods work with a parameterized family of policies. The gradient of the performance, with respect to the actor parameters, is directly estimated by simulation, and the parameters are updated in a direction of improvement. A possible drawback of such methods is that the gradient estimators may have a large variance. Furthermore, as the policy changes, a new gradient is estimated independently of past estimates. Hence, there is no \"learning,\" in the sense of accumulation and consolidation of older information.\n(b) Critic-only methods rely exclusively on value function approximation and aim at learning an approximate solution to the Bellman equation, which will then hopefully prescribe a near-optimal policy. Such methods are indirect in the sense that they do not try to optimize directly over a policy space. A method of this type may succeed in constructing a \"good\" approximation of the value function, yet lack reliable guarantees in terms of near-optimality of the resulting policy.",
            "Publication date": 2000.0,
            "Publication link": "https://papers.nips.cc/paper/1786-actor-critic-algorithms.pdf",
            "Publication name": "Actor-Critic Algorithms",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Vijay R. Konda, John N. Tsitsiklis",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "EMPW",
            "Using for": "Reinforcement Learning",
            "Based on": "EMPTD",
            "Method": "Emphatic Weightings",
            "Description": "In off-policy learning, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, authors solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of emphatic weightings. The key insight is that the gradient can be simplified if the gradient in each state is weighted with an emphatic weighting. Authors demonstrate both the theorem and the counterexample under stochastic and deterministic policies, and that ACE converges to the optimal solution.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7295-an-off-policy-policy-gradient-theorem-using-emphatic-weightings",
            "Publication name": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings",
            "Released at": "NIPS",
            "Interacts with": "AC",
            "Authors": "Ehsan Imani, Eric Graves, Martha White",
            "Code link": "None",
            "Data": "MDP"
        },
        {
            "id": "ACE",
            "Using for": "Reinforcement Learning",
            "Based on": "AC, EMPW",
            "Method": "Actor Critic with Emphatic weightings",
            "Description": "Authors develop a new actor-critic algorithm\u2014called Actor Critic with Emphatic weightings (EMPW)\u2014that approximates the simplified gradients provided by the theorem.  Using this theorem, we derived an off-policy actor-critic algorithm that follows the gradient of the objective function. Algorithm follows the gradient, reaches the optimal solution, both for an idealized setting given the true critic and when learning the critic. Authors conclude with a result suggesting that more work needs to be done to effectively estimate emphatic weightings, and that important next steps for developing Actor-Critic algorithm for the off-policy setting are to improve estimation of these weightings.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7295-an-off-policy-policy-gradient-theorem-using-emphatic-weightings",
            "Publication name": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings",
            "Released at": "NIPS",
            "Interacts with": "EMPW",
            "Authors": "Ehsan Imani, Eric Graves, Martha White",
            "Code link": "None",
            "Data": "MDP"
        },
        {
            "id": "ADGB",
            "Using for": "Multi-Task Learning",
            "Based on": "NaN",
            "Method": "Algorithm-Dependent Generalization Bounds for Multi-Task Learning",
            "Description": "Often, tasks are collected for multi-task learning (MTL) because they share similar feature structures. Based on this observation, in this paper, we present novel algorithm-dependent generalization bounds for MTL by exploiting the notion of algorithmic stability. We focus on the performance of one particular task and the average performance over multiple tasks by analyzing the generalization ability of a common parameter that is shared in MTL. When focusing on one particular task, with the help of a mild assumption on the feature structures, we interpret the function of the other tasks as a regularizer that produces a specific inductive bias. The algorithm for learning the common parameter, as well as the predictor, is thereby uniformly stable with respect to the domain of the particular task and has a generalization bound with a fast convergence rate of order O(1/n), where n is the sample size of the particular task. When focusing on the average performance over multiple tasks, we prove that a similar inductive bias exists under certain conditions on the feature structures. Thus, the corresponding algorithm for learning the common parameter is also uniformly stable with respect to the domains of the multiple tasks, and its generalization bound is of the order O(1/T), where T is the number of tasks. These theoretical analyses naturally show that the similarity of feature structures in MTL will lead to specific regularizations for predicting, which enables the learning algorithms to generalize fast and correctly from a few examples.",
            "Publication date": "NaN",
            "Publication link": "https://ieeexplore.ieee.org/document/7437460",
            "Publication name": "Algorithm-Dependent Generalization Bounds for Multi-Task Learning",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Tongliang Liu. Dacheng Tao, Mingli Song, Stephen J. Maybank",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LNN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Linear Neural Network",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SAE",
            "Using for": "Supervised Learning, Generalization",
            "Based on": "LNN, AE",
            "Method": "Supervised auto-encoders",
            "Description": "Is a method for neural network which consists of the addition of reconstruction error to a linear neural network\u2014 provides uniform stability and so a bound on generalization error. Authors showed theoretically that the addition of reconstruction error improves generalization performance",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7296-supervised-autoencoders-improving-generalization-performance-with-unsupervised-regularizers",
            "Publication name": "Supervised autoencoders: Improving generalization performance with unsupervised regularizers",
            "Released at": "NIPS",
            "Interacts with": "LNN",
            "Authors": "Lei Le, Andrew Patterson, Martha White",
            "Code link": "None",
            "Data": "SUSY, Deterding, CIFAR-10, MNIST"
        },
        {
            "id": "DCGAN",
            "Using for": "Adversarial Networks",
            "Based on": "CNN, GAN",
            "Method": "Deep Convolutional Generative Adversarial Networks",
            "Description": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1511.06434",
            "Publication name": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "Released at": "ICLR",
            "Interacts with": "NaN",
            "Authors": "Alec Radford, Luke Metz, Soumith Chintala",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "WGAN-GP",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "Wasserstein GAN gradient penalty",
            "Description": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable train- ing of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1704.00028",
            "Publication name": "Improved Training of Wasserstein GANs",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LSGAN",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "Least Squares Generative Adversarial Networks",
            "Description": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \u03c72 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1611.04076",
            "Publication name": "Least Squares Generative Adversarial Networks",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RESNED",
            "Using for": "NaN",
            "Based on": "AE, RESN",
            "Method": "ResNet encoder-decoder",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "3D-GAN",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "3D-GAN",
            "Description": "GAN architecture for 3D shape generation.",
            "Publication date": 2016.0,
            "Publication link": "http://3dgan.csail.mit.edu/papers/3dgan_nips.pdf",
            "Publication name": "A 3D-GAN is a GAN architecture for 3D shape generation.",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VON",
            "Using for": "Generation",
            "Based on": "3D-GAN, RESNED ENCDR, DCDR,",
            "Method": "Visual Object Networks",
            "Description": "End-to-end generative model that jointly synthesizes 3D shapes and 2D images via a disentangled object representation. Specifically, authors decompose image generation model into three conditionally independent factors: shape, viewpoint, and texture, borrowing ideas from classic graphics rendering engines. Model first learns to synthesize 3D shapes that are indistinguishable from real shapes. It then computes its 2.5D sketches with a differentiable projection module from a sampled viewpoint. This disentangled 3D representation allows to learn the model from both 3D and 2D visual data collections under an adversarial learning framework.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7297-visual-object-networks-image-generation-with-disentangled-3d-representations",
            "Publication name": "Visual Object Networks: Image Generation with Disentangled 3D Representations",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenenbaum, Bill Freeman",
            "Code link": "None",
            "Data": "ShapeNet"
        },
        {
            "id": "WN-DNN",
            "Using for": "Normalization",
            "Based on": "DNNs",
            "Method": "Weight normalized deep neural network \n",
            "Description": "Authors present a general framework for capacity control on WN-DNNs. In particular, they provide a satisfying answer for the central question: we obtain the generalization bounds for WN-DNNs that grows with depth by a square root term while getting the approximation error controlled. \nWN-DNNs, includes all layer-wise weight normalizations. In addition, these networks have a bias neuron per hidden layer, while prior studies either exclude the bias neuron, or only include the bias neuron in the input layer, which differs from the practical application. Authors establish the upper bound on the Rademacher complexities of this family and study the theoretical properties of WN-DNNs in terms of the approximation error.  This is the first theoretical result for the fully connected DNNs including a bias neuron for each hidden layer in terms of generalization. \n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7298-understanding-weight-normalized-deep-neural-networks-with-rectified-linear-units",
            "Publication name": "Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Yixi Xu, Xiao WangYixi Xu, Xiao Wang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "N&B",
            "Using for": "Pipelines Learning",
            "Based on": "NaN",
            "Method": "Nuts&Bolts\u2028\n",
            "Description": "Is  a framework to learn pipelines with a provided hierarchy of sub-tasks. This framework incorporates multiple function approximators for various sub-tasks, domain knowledge in the form of rules and stage-wise supervision. Nuts&Bolts outperforms prior work on the task of diagram parsing. This also leads to improvements on down stream tasks \u2013  answering Newtonian physics questions. Nuts&Bolts utilizes labelled data as well as unlabelled data to achieve better performance. Nuts&Bolts can incorporate supervision at various stages of the pipeline. It is robust to low amounts of supervision at certain stages in the pipeline. Nuts&Bolts jointly models the various stages of a pipeline which prevents error propagation. Since there is no existing prior work which performs end-to-end parsing of diagrams to formal language. A key benefit of Nuts&Blots is its ability to incorporate unlabelled data. Nuts&Blots allows for varying amount of supervision at each stage in the pipeline. Nuts&Blots learns the entire pipeline jointly, thus preventing error propagation.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7299-learning-pipelines-with-limited-data-and-domain-knowledge-a-study-in-parsing-physics-problems",
            "Publication name": "Learning Pipelines with Limited Data and Domain Knowledge: A Study in Parsing Physics Problems \n",
            "Released at": "NIPS",
            "Interacts with": "Pipelines",
            "Authors": "Mrinmaya Sachan, Kumar Avinava Dubey, Tom M. Mitchell, Dan Roth, Eric P. Xing",
            "Code link": "None",
            "Data": "pre-university physics text books: Resnick Halliday Walker,D. B. Singhand NCERT. \n"
        },
        {
            "id": "RNN",
            "Using for": "Recurrent Networks",
            "Based on": "NaN",
            "Method": "Recurrent Neural Network",
            "Description": "A number of procedures are described for finding delta E/ delta W/sub ij/ where E is an error functional of the temporal trajectory of the states of a continuous recurrent network and w/sub ij/ are the weights of that network. Computing these quantities allows one to perform gradient descent in the weights to minimize E, so these procedures form the kernels of connectionist learning algorithms. Simulations in which networks are taught to move through limit cycles are shown, along with some empirical perturbation sensitivity tests. The author describes a number of elaborations of the basic idea, including mutable time delays and teacher forcing. He includes a complexity analysis of the various learning procedures discussed and analyzed. Temporally continuous recurrent networks seems particularly suited for temporally continuous domains, such as signal processing, control, and speech.",
            "Publication date": 1989.0,
            "Publication link": "https://ieeexplore.ieee.org/document/118724/authors#authors",
            "Publication name": "Learning state space trajectories in recurrent neural networks",
            "Released at": "IJCNN",
            "Interacts with": "NaN",
            "Authors": "Barak A. Pearlmutter",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HGRU",
            "Using for": "Recognition \n",
            "Based on": "CNN, RNN",
            "Method": "Horizontal Gated Recurrent Unit \n",
            "Description": "Is a trainable convolutional recurrent neural network  which performs Euler integration of a dynamical system. Single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7300-learning-long-range-spatial-dependencies-with-horizontal-gated-recurrent-units",
            "Publication name": "Learning long-range spatial dependencies with horizontal gated recurrent units",
            "Released at": "NIPS",
            "Interacts with": "MLP, LSTM, CNN, RNN, GRU",
            "Authors": "Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, Thomas Serre",
            "Code link": "None",
            "Data": "Pathfinder dataset \n"
        },
        {
            "id": "SCON",
            "Using for": "Classification",
            "Based on": "NaN",
            "Method": "Skip Connections",
            "Description": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/pdf/1512.03385.pdf",
            "Publication name": "Deep Residual Learning for Image Recognition",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SRFEN",
            "Using for": "Embedding",
            "Based on": "CNN, SCON",
            "Method": "Feature Embedding Net \n",
            "Description": "First layers tries to increase the number of channels of input, which can be added with the output of the clique block group via the skip connection. Then the propagation of first stage does the same things as dense block, while the second stage distills the feature further. Second, a clique block contains more skip connections compared with a dense block, so the information among layers can be more easily propagated. Authors add a residual connection to the clique block, since the input feature contains plenty of useful information in terms of super resolution problem.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7301-joint-sub-bands-learning-with-clique-structures-for-wavelet-domain-super-resolution",
            "Publication name": "Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution \n",
            "Released at": "NIPS",
            "Interacts with": "CNNs",
            "Authors": "Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin, Chao Zhang",
            "Code link": "None",
            "Data": "DIV2K, Flickr"
        },
        {
            "id": "DWT",
            "Using for": "Image preprocessing",
            "Based on": "NaN",
            "Method": "Discrete wavelet transformation \n",
            "Description": "Any wavelet transform for which the wavelets are discretely sampled. As with other wavelet transforms, a key advantage it has over Fourier transforms is temporal resolution",
            "Publication date": 1990.0,
            "Publication link": "https://web.njit.edu/~akansu/PAPERS/Akansu-BinomialQMF-Wavelet-SPIE-VCIP-Sept1990.pdf",
            "Publication name": "Perfect Reconstruction Binomial QMF-Wavelet Transform",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "A.N. Akansu, R.A. Haddad and H. Caglar",
            "Code link": "NaN",
            "Data": "\n"
        },
        {
            "id": "SRIRN",
            "Using for": "Reconstruction",
            "Based on": "CNN, DWT",
            "Method": "Image Reconstruction Net \n",
            "Description": "Consists of two parts: a clique up-sampling module and a convolutional layer which is used to reduce the number of feature maps to reconstruct the high resolution",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7301-joint-sub-bands-learning-with-clique-structures-for-wavelet-domain-super-resolution",
            "Publication name": "Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution \n",
            "Released at": "NIPS",
            "Interacts with": "SRFEN, CNNs",
            "Authors": "Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin, Chao Zhang",
            "Code link": "None",
            "Data": "DIV2K, Flickr \n"
        },
        {
            "id": "SRCLIN",
            "Using for": "High-resolution \n",
            "Based on": "SRFEN,  SRIRN",
            "Method": "Super-Resolution CliqueNet \n",
            "Description": "Authors propose a novel CNN called Super-Resolution CliqueNet  for single-image super-resolution. Authors design a new up-sampling module called clique up-sampling which uses IDWT to change the size of feature maps and jointly learn all sub-band coefficients depending on the edge feature property. Also they design a res-clique block to extract features for super-resolution. Extensive evaluations on benchmark datasets demonstrate that the proposed network performs better than the state-of-the-art super-resolution algorithms in terms of quantitative metrics.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7301-joint-sub-bands-learning-with-clique-structures-for-wavelet-domain-super-resolution",
            "Publication name": "Joint Sub-bands Learning with Clique Structures for Wavelet Domain Super-Resolution \n",
            "Released at": "NIPS",
            "Interacts with": "CNNs",
            "Authors": "Zhisheng Zhong, Tiancheng Shen, Yibo Yang, Zhouchen Lin, Chao Zhang",
            "Code link": "None",
            "Data": "DIV2K, Flickr \n"
        },
        {
            "id": "LSH",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "locality sensitive hashing",
            "Description": "Is an algorithmic technique that hashes similar input items into the same \"buckets\" with high probability",
            "Publication date": 2014.0,
            "Publication link": "https://www.aaai.org/ocs/index.php/AAAI/AAAI14/paper/view/8357",
            "Publication name": "Locality Preserving Hashing",
            "Released at": "AAAI",
            "Interacts with": "NaN",
            "Authors": "Kang Zhao, Hongtao Lu, Jincheng Mei\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "OPSL",
            "Using for": "Similarity search",
            "Based on": "LSH\n",
            "Method": "Optimal Sparse Lifting \n",
            "Description": "The optimal sparse lifting is a sparse binary vector representation of the input samples in a higher-dimensional space, such that the pairwise similarity between the samples can be roughly preserved. This component can be efficiently obtained by solving optimization problems with the Frank-Wolfe method.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7302-fast-similarity-search-via-optimal-sparse-lifting",
            "Publication name": "Fast Similarity Search via Optimal Sparse Lifting \n",
            "Released at": "NIPS",
            "Interacts with": "FASF",
            "Authors": "Wenye Li, Jingwei Mao, Yin Zhang, Shuguang Cui",
            "Code link": "None",
            "Data": "MNIST, WIKI dataset \n"
        },
        {
            "id": "FASF",
            "Using for": "Similarity search",
            "Based on": "OPSL",
            "Method": "Fast Similarity Search Framework \n",
            "Description": "This work proposes a unified framework for dimension expansion and applies it in similarity search. The framework has two key components. The optimal sparse lifting is a sparse binary vector representation of the input samples in a higher-dimensional space, such that the pairwise similarity between the samples can be roughly preserved. The sparse lifting operator is a sparse binary matrix that best maps the input samples to the optimal sparse lifting. Computationally, both components can be efficiently and effectively obtained by solving optimization problems with the Frank-Wolfe method.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7302-fast-similarity-search-via-optimal-sparse-lifting",
            "Publication name": "Fast Similarity Search via Optimal Sparse Lifting \n",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Wenye Li, Jingwei Mao, Yin Zhang, Shuguang Cui",
            "Code link": "None",
            "Data": "MNIST, WIKI dataset \n"
        },
        {
            "id": "FSTL",
            "Using for": "Classification, few-shot learning\n",
            "Based on": "None",
            "Method": "F-Statistic Loss \n",
            "Description": "The F-statistic loss is motivated by the goal of unifying the deep-embedding and disentangling literatures. F -statistic loss has four desirable properties. First, the gradient rapidly drops to zero once classes become reliably separated on at least d dimensions, leading to a natural stopping criterion; the degree of separation obtained is related to the number of samples per class. Second, in contrast to other losses, the F-statistic loss is not invariant to rotations in the embedding space; this focus on separating along specific dimensions tends to yield disentangled features when the class structure is factorial or compositional. Third, embeddings obtained are relatively insensitive to the one free parameter, d. Fourth, because the loss is expressed in the currency of probability it can readily be combined with additional losses expressed similarly",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7303-learning-deep-disentangled-embeddings-with-the-f-statistic-loss",
            "Publication name": "Learning Deep Disentangled Embeddings With the F-Statistic Loss",
            "Released at": "NIPS",
            "Interacts with": "INCV3, DEML",
            "Authors": "Karl Ridgeway, Michael C. Mozer",
            "Code link": "https://github.com/kridgeway/f-statistic-loss-nips-2018 \n",
            "Data": "CUHK03, Market-1501 \n"
        },
        {
            "id": "MC",
            "Using for": "Sampling",
            "Based on": "NaN",
            "Method": "Monte Carlo Methods\n",
            "Description": "Monte Carlo is a computational technique based on constructing a random process for a problem and carrying out a NUMERICAL EXPERIMENT by N-fold sampling from a random sequence of numbers with a PRESCRIBED probability distribution.",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BYINF",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Bayesian inference \n",
            "Description": "Statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Bayesian_inference",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "QMC",
            "Using for": "Sampling",
            "Based on": "MC",
            "Method": "Quasi Monte Carlo",
            "Description": "quasi-Monte Carlo method is a method for numerical integration and solving some other problems using low-discrepancy sequences",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GCMC",
            "Using for": "Sampling",
            "Based on": "MC, QMC",
            "Method": "Geometrically Coupled Monte Carlo  \n",
            "Description": "Authors improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. They show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies.  Introduced Monte Carlo is coupling with strategies in Euclidean spaces for improving algorithms that typically operate in a high-dimensional, low-sample regime, demonstrating fundamental connections to multi-marginal transport.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7304-geometrically-coupled-monte-carlo-sampling",
            "Publication name": "Geometrically Coupled Monte Carlo Sampling",
            "Released at": "NIPS",
            "Interacts with": "BYINF, RL, KERM",
            "Authors": "Mark Rowland, Krzysztof M. Choromanski, Fran\u00e7ois Chalus, Aldo Pacchiano, Tamas Sarlos, Richard E. Turner, Adrian Weller",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "CHSU",
            "Using for": "Scene Understanding \n",
            "Based on": "RESN, ENCDR, MLP",
            "Method": "Cooperative Holistic Scene Understanding \n",
            "Description": "The existing methods can simultaneously solves all object bounding boxes,  room layout, and  camera pose, in real- time given only a single RGB image. Authors propose an end-to-end model that recovers a 3D indoor scene in real-time, including the 3D room layout, camera pose, and object bounding boxes. A novel parametrization of 3D bounding boxes and a 2D projection loss are introduced to enforce the consistency between 2D and 3D. The proposed model consists of two networks, a global geometric network  that estimates the 3D room layout and camera pose, and a local object network  that infers the attributes of each object. Based on these two networks, authors further formulate differentiable loss functions to train the two networks cooperatively.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7305-cooperative-holistic-scene-understanding-unifying-3d-object-layout-and-camera-pose-estimation",
            "Publication name": "Cooperative Holistic Scene Understanding: Unifying 3D Object, Layout, and Camera Pose Estimation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu",
            "Code link": "None",
            "Data": "SUN RGB-D,"
        },
        {
            "id": "DP",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Dynamic programming",
            "Description": "Is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions using a memory-based data structure",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Dynamic_programming",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NIR",
            "Using for": "Regression",
            "Based on": "NaN",
            "Method": "Nearly-Isotonic Regression \n",
            "Description": "We consider the problem of approximating a sequence of data points with a \u201cnearly-isotonic\u201d, or nearly-monotone function. This is formulated as a convex optimization problem that yields a family of solutions, with one extreme member being the standard isotonic regression fit. We devise a simple algorithm to solve for the path of solutions, which can be viewed as a modified version of the well-known pool adjacent violators algorithm, and computes the entire path in O(nlogn) operations, (n being the number of data points). In practice, the intermediate fits can be used to examine the assumption of monotonicity. Nearly-isotonic regression admits a nice property in terms of its degrees of freedom: at any point along the path, the number of joined pieces in the solution is an unbiased estimate of its degrees of freedom. We also extend the ideas to provide \u201cnearly-convex\u201d approximations.",
            "Publication date": 2011.0,
            "Publication link": "http://www.stat.cmu.edu/~ryantibs/papers/neariso.pdf",
            "Publication name": "Nearly-Isotonic Regression",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "\nRyan J. Tibshirani, Holger Hoefling, Robert Tibshirani",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SPAV",
            "Using for": "Isotonic Regression",
            "Based on": "NaN",
            "Method": "Scaling Pool Adjacent Violators  algorithm \n",
            "Description": "NaN",
            "Publication date": 2001.0,
            "Publication link": "https://pubsonline.informs.org/doi/pdf/10.1287/opre.49.5.784.10601",
            "Publication name": "A Fast Scaling Algorithm for Minimizing Separable Convex Functions Subject to Chain Constraints \n",
            "Released at": "Operations Research \n",
            "Interacts with": "NaN",
            "Authors": "Ravindra K. Ahuja, James B. Orlin",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "EPA",
            "Using for": "Isotonic Regression",
            "Based on": "DP, SPAV",
            "Method": "Efficient Pruning Algorithm \n",
            "Description": "Authors propose a fast pruning algorithm that builds upon the standard Dynamic Programming algorithm for solving the separable nonconvex isotonic regression problem  to any arbitrary accuracy (to the global optimal value). This algorithm works by solving the problem over a finer and finer set of points, choosing not to add points that will not get us closer to the global optimum. It use two ways to exclude points, the first of which only requires the Lipschitz constants of each function, and the second requires to be able to compute a linear underestimate of function over an interval efficiently. This information is readily available for a variety of quasiconvex distance functions, and for Tukey\u2019s biweight function.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7306-an-efficient-pruning-algorithm-for-robust-isotonic-regression",
            "Publication name": "An Efficient Pruning Algorithm for Robust Isotonic Regression",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Cong Han Lim",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "PAC",
            "Using for": "Adversarial Attacks, Adversarial Robustness \n",
            "Based on": "NaN",
            "Method": "Probably Approximately Correct learning framework \n",
            "Description": "Authors provide a useful theoretical understanding of the problem of learning with adversaries, the nature of the 0-1 loss prevents the efficient implementation of Adversarial ERM to obtain robust classifiers.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7307-pac-learning-in-the-presence-of-adversaries",
            "Publication name": "PAC-learning in the presence of adversaries",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Daniel Cullina, Arjun Nitin Bhagoji, Prateek Mittal",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "SARDNN",
            "Using for": "Adversarial Attacks, Adversarial Robustness",
            "Based on": "None",
            "Method": "Sparse Adversarial Robustness Deep Neural Networks",
            "Description": "In this paper, authors study some intrinsic relationships between the adversarial robustness and the sparsity of classifiers, both theoretically and empirically. By introducing plausible metrics, we demonstrate that unlike some linear models which behave differently under different attacks, sparse nonlinear DNNs can be consistently more robust to both of them than their corresponding dense references, until their sparsity reaches certain thresholds and inevitably causes harm to the network capacity.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7308-sparse-dnns-with-improved-adversarial-robustness",
            "Publication name": "Sparse DNNs with Improved Adversarial Robustness",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yiwen Guo, Chao Zhang, Changshui Zhang, Yurong Chen",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SNAPML",
            "Using for": "Software Framework",
            "Based on": "None",
            "Method": "Snap Machine Learning \n",
            "Description": "Is a software framework for fast training of generalized linear models. The framework, combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems.The framework is hierarchical in nature, allowing it to adapt to cloud-based deployments where the cost of communication between nodes may be relatively high. It is also able to effectively leverage modern high-speed interconnects to hide the cost of transferring data between CPU and GPU when training on datasets that are too large to fit into GPU memory.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7309-snap-ml-a-hierarchical-framework-for-machine-learning",
            "Publication name": "Snap ML: A Hierarchical Framework for Machine Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Celestine D\u00fcnner, Thomas Parnell, Dimitrios Sarigiannis, Nikolas Ioannou, Andreea Anghel, Gummadi Ravi, Madhusudanan Kandasamy, Haralampos Pozidis",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SATSEENET",
            "Using for": "Semantic Scene Completion",
            "Based on": "NaN",
            "Method": "See and Think: See Network",
            "Description": "NaN",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7310-see-and-think-disentangling-semantic-scene-completion",
            "Publication name": "See and Think: Disentangling Semantic Scene Completion",
            "Released at": "NIPS",
            "Interacts with": "SATNET, SATTHNET",
            "Authors": "Shice Liu, YU HU, Yiming Zeng, Qiankun Tang, Beibei Jin, Yinhe Han, Xiaowei Li",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SATTHNET",
            "Using for": "Semantic Scene Completion",
            "Based on": "NaN",
            "Method": "See and Think: Think Network",
            "Description": "NaN",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7310-see-and-think-disentangling-semantic-scene-completion",
            "Publication name": "See and Think: Disentangling Semantic Scene Completion",
            "Released at": "NIPS",
            "Interacts with": "SATNET, SATSEENET,",
            "Authors": "Shice Liu, YU HU, Yiming Zeng, Qiankun Tang, Beibei Jin, Yinhe Han, Xiaowei Li",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SSCSDI",
            "Using for": "Semantic Scene Completion \n",
            "Based on": "NaN",
            "Method": "Semantic Scene Completion from a Single Depth Image \u2028\n",
            "Description": "NaN",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1611.08974",
            "Publication name": "Semantic Scene Completion from a Single Depth Image",
            "Released at": "CVPR",
            "Interacts with": "NaN",
            "Authors": "Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SATNET",
            "Using for": "Semantic Scene Completion \n",
            "Based on": "SATSEENET, SATTHNET",
            "Method": "See And Think Network \n",
            "Description": " In this work, authors propose a disentangled framework, it consists of SNet for 2D semantic segmentation, a 2D-3D reprojection layer, and TNet for semantic scene com- pletion. Authors provide single-branch and double-branch implementations within the SATNet framework, which demonstrates the SATNet framework is effective, extensible and evolvable. This three-stage framework has three advantages:  explicit semantic segmentation significantly boosts performance;  flexible fusion ways of sensor data bring good extensibility;  progress in any subtask will promote the holistic performance. Experimental results show SATNet outperforms the state-of-the-art approaches.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7310-see-and-think-disentangling-semantic-scene-completion",
            "Publication name": "See and Think: Disentangling Semantic Scene Completion",
            "Released at": "NIPS",
            "Interacts with": "ENCDR, DCDR, VOLC",
            "Authors": "Shice Liu, YU HU, Yiming Zeng, Qiankun Tang, Beibei Jin, Yinhe Han, Xiaowei Li",
            "Code link": "https://github.com/ShiceLiu/SATNet. \n",
            "Data": "NYUv2 , SUNCG \n"
        },
        {
            "id": "RELN",
            "Using for": "Relational Learning",
            "Based on": "NaN",
            "Method": "Relation Networks \n",
            "Description": "Artificial neural network component with a structure that can reason about relations among objects.Is a few-shot learning model aiming to do classification via learning a deep distance metric between images.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/pdf/1706.01427.pdf",
            "Publication name": "A simple neural network module for relational reasoning",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CBIP",
            "Using for": "Visual question answering",
            "Based on": "NaN",
            "Method": "Compact Bilinear Pooling \u2028\n",
            "Description": "Bilinear models has been shown to achieve impressive performance on a wide range of visual tasks, such as semantic segmentation, fine grained recognition and face recognition. However, bilinear features are high dimensional, typically on the order of hundreds of thousands to a few million, which makes them impractical for subsequent analysis. We propose two compact bilinear representations with the same discriminative power as the full bilinear representation but with only a few thousand dimensions. Our compact representations allow back-propagation of classification errors enabling an end-to-end optimization of the visual recognition system. The compact bilinear representations are derived through a novel kernelized analysis of bilinear pooling which provide insights into the discriminative power of bilinear pooling, and a platform for further research in compact pooling methods. Experimentation illustrate the utility of the proposed representations for image classification and few-shot learning across several datasets.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1511.06062",
            "Publication name": "Multimodal compact bilinear pooling for visual question answering and visual grounding. \u2028\n",
            "Released at": "EMNLP",
            "Interacts with": "NaN",
            "Authors": "Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "COR",
            "Using for": "Reasoning, Visual question answering\n\n",
            "Based on": "MLP, EMBD",
            "Method": "Chain of Reasoning \n",
            "Description": "Authors introduce a new VQA model that performs a chain of reasoning, which generates new relations and compound objects dynamically to infer the answer. Method achieve new state-of-the-art results on four publicly available datasets. Also method conduct a detailed ablation study to show that proposed chain structure is superior to stack structure and parallel structure. Authors visualize the chain of reasoning, which shows the progress that the CoR generates new compound objects dynamically that lead to the answer of the question step by step. Experimental results on four publicly available datasets show that CoR outperforms state-of-the-art approaches",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7311-chain-of-reasoning-for-visual-question-answering",
            "Publication name": "Chain of Reasoning for Visual Question Answering",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Chenfei Wu, Jinlai Liu, Xiaojie Wang, Xuan Dong",
            "Code link": "None",
            "Data": "VQA 1.0 dataset, VQA 2.0 dataset, COCO-QA dataset, TDIUC dataset \n \n"
        },
        {
            "id": "SIGM",
            "Using for": "Activation Function",
            "Based on": "NaN",
            "Method": "Sigmoid",
            "Description": "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function",
            "Publication date": 1995.0,
            "Publication link": "https://link.springer.com/chapter/10.1007/3-540-59497-3_175",
            "Publication name": "The influence of the sigmoid function parameters on the speed of backpropagation learning",
            "Released at": "From Natural to Artificial Neural Computation. Lecture Notes in Computer Science. 930. pp. 195\u2013201.",
            "Interacts with": "DNNs",
            "Authors": "Jun Han, Claudio Moraga",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RELU",
            "Using for": "Activation Function",
            "Based on": "NaN",
            "Method": "Rectified Linear Unit",
            "Description": "Digital circuits such as the flip-flop use feedback to achieve multistability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.",
            "Publication date": 2000.0,
            "Publication link": "https://www.ncbi.nlm.nih.gov/pubmed/10879535",
            "Publication name": "Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit.",
            "Released at": "Nature",
            "Interacts with": "DNNs",
            "Authors": "Hahnloser RH, Sarpeshkar R, Mahowald MA, Douglas RJ, Seung HS.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SMAX",
            "Using for": "Activation Function",
            "Based on": "NaN",
            "Method": "Softmax",
            "Description": "In mathematics, the softmax function, also known as softargmax or normalized exponential function, is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval (0,1), and the components will add up to 1, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities. Softmax is often used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.",
            "Publication date": 1868.0,
            "Publication link": "https://www.cambridge.org/core/books/wissenschaftliche-abhandlungen/studien-uber-das-gleichgewicht-der-lebendigen-kraft-zwischen-bewegten-materiellen-punkten/7C385B6332CC6214CF9418195F84DD84",
            "Publication name": "Studies on the balance of living force between moving material points",
            "Released at": "NaN",
            "Interacts with": "DNNs",
            "Authors": "Boltzmann, Ludwig",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SSMAX",
            "Using for": "Activation Function\n",
            "Based on": "SIGM, SMAX",
            "Method": "Sigsoftmax \n",
            "Description": "Sigsoftmax, is composed of a multiplication of an exponential function and sigmoid function. Sigsoftmax can break the softmax bottleneck,  and has more representational power than softmax without additional parameters. Since sigsoftmax has the desirable properties for output activation functions, it has the potential to replace softmax in many applications. The experiments on language modeling demonstrate that sigsoftmax and mixture of sigsoftmax outperform softmax and mixture of softmax, respectively.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7312-sigsoftmax-reanalysis-of-the-softmax-bottleneck",
            "Publication name": "Sigsoftmax: Reanalysis of the Softmax Bottleneck",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, Shuichi Adachi",
            "Code link": "None",
            "Data": "Penn Treebank dataset,  WikiText-2 dataset,"
        },
        {
            "id": "TGMCMC",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Tempered Geodesic Markov Chain Monte Carlo \n",
            "Description": "We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites global non-convex optimization on the spherical manifold of quaternions with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of solutions. We devise theoretical convergence guarantees and extensively evaluate our method on synthetic and real benchmarks. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7314-bayesian-pose-graph-optimization-via-bingham-distributions-and-tempered-geodesic-mcmc",
            "Publication name": "Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Tolga Birdal, Umut Simsekli, Mustafa Onur Eken, Slobodan Ilic",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RCNN",
            "Using for": "Object Detection",
            "Based on": "NaN",
            "Method": "Regions with CNN\n",
            "Description": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset.",
            "Publication date": 2014.0,
            "Publication link": "https://arxiv.org/pdf/1311.2524.pdf",
            "Publication name": "Rich feature hierarchies for accurate object detection and semantic segmentation \u2028\n",
            "Released at": "Proceedings of the IEEE conference on computer vision and pattern recognition \u2028\n",
            "Interacts with": "NaN",
            "Authors": "R. Girshick, J. Donahue, T. Darrell, and J. Malik \u2028\n",
            "Code link": "http://www.cs.berkeley.edu/rbg/rcnn. \n",
            "Data": "ILSVRC2013 \n"
        },
        {
            "id": "METAAN",
            "Using for": "Object Detection",
            "Based on": "RCNN",
            "Method": "MetaAnchor \n\n",
            "Description": "A novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. In traditional approaches, the predefined anchor box often needs careful design \u2013 too few anchors may be insufficient to cover rare boxes, or result in coarse predictions; however, more anchors usually imply more parameters, which may suffer from overfitting. In addition, many traditional strategies use independent weights to model different anchor functions, so it is very likely for the anchors associated with few ground truth object boxes in training to produce poor results. In contrast, for MetaAnchor anchor boxes of any shape could be randomly sampled during training so as to cover different kinds of object boxes, meanwhile, the number of parameters keeps constant. So MetaAnchor is more robust to anchor settings and bounding box distributions. MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7315-metaanchor-learning-to-detect-objects-with-customized-anchors",
            "Publication name": "MetaAnchor: Learning to Detect Objects with Customized Anchors",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Tong Yang, Xiangyu Zhang, Zeming Li, Wenqiang Zhang, Jian Sun",
            "Code link": "None",
            "Data": "COCO-all,  COCO-mini \n"
        },
        {
            "id": "CE",
            "Using for": "Embedding",
            "Based on": "CNN, ENCDR",
            "Method": "Context encoders \u2028\n",
            "Description": "We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1604.07379",
            "Publication name": "Context encoders: Feature learning by inpainting. \u2028\n",
            "Released at": "CVPR",
            "Interacts with": "NaN",
            "Authors": "D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CA",
            "Using for": "Attention",
            "Based on": "GAN, CE",
            "Method": "Contextual Attention \u2028\n",
            "Description": "Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feed-forward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1801.07892",
            "Publication name": "Generative image inpainting with contextual attention. \u2028\n",
            "Released at": "arXiv preprint arXiv:1801.07892 \u2028\n",
            "Interacts with": "NaN",
            "Authors": "J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang \u2028\n",
            "Code link": "https://github.com/JiahuiYu/generative_inpainting",
            "Data": "NaN"
        },
        {
            "id": "MSNPS",
            "Using for": "Image Inpainting",
            "Based on": "NaN",
            "Method": "Multi-Scale Neural Patch Synthesis",
            "Description": "Recent advances in deep learning have shown exciting promise in filling large holes in natural images with semantically plausible and context aware details, impacting fundamental image manipulation tasks such as object removal. While these learning-based methods are significantly more effective in capturing high-level features than prior techniques, they can only handle very low-resolution inputs due to memory limitations and difficulty in training. Even for slightly larger images, the inpainted regions would appear blurry and unpleasant boundaries become visible. We propose a multi-scale neural patch synthesis approach based on joint optimization of image content and texture constraints, which not only preserves contextual structures but also produces high-frequency details by matching and adapting patches with the most similar mid-layer feature correlations of a deep classification network. We evaluate our method on the ImageNet and Paris Streetview datasets and achieved state-of-the-art inpainting accuracy. We show our approach produces sharper and more coherent results than prior methods, especially for high-resolution images.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1611.09969",
            "Publication name": "High-resolution image inpainting using multi-scale neural patch synthesis \u2028\n",
            "Released at": "CVPR \u2028\n",
            "Interacts with": "NaN",
            "Authors": "C. Yang, X. Lu, Z. Lin, E. Shechtman, O. Wang, and H. Li. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GMCNN",
            "Using for": "Image Inpainting \n",
            "Based on": "ENCDR, DCDR, CNN, WGAN-GP \n",
            "Method": "Generative Multi-column Convolutional Neural Network \n",
            "Description": "In this paper, we propose a generative multi-column network for image inpainting. This network synthesizes different image components in a parallel manner within one stage. To better characterize global structures, we design a confidence-driven reconstruction loss while an implicit diversified MRF regularization is adopted to enhance local details. The multi-column network combined with the reconstruction and MRF loss propagates local and global information derived from context to the target inpainting regions. Extensive experiments on challenging street view, face, natural objects and scenes manifest that our method produces visual compelling results even without previously common post-processing.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7316-image-inpainting-via-generative-multi-column-convolutional-neural-networks",
            "Publication name": "Image Inpainting via Generative Multi-column Convolutional Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yi Wang, Xin Tao, Xiaojuan Qi, Xiaoyong Shen, Jiaya Jia",
            "Code link": "None",
            "Data": "Pairs street view-100 ImageNet-200 Places2-2K CelebA-HQ-2K \n"
        },
        {
            "id": "CASP",
            "Using for": "Misinformation Containmen",
            "Based on": "None",
            "Method": "Cascade priority\n",
            "Description": "In this paper, authors provide a formal model for multi-cascade diffusion and introduce an important concept called as cascade priority, and propose three types of cascade priorities, homogeneous cascade priority, M- dominant cascade priority, and P-dominant cascade priority. Also they provide a formal model supporting multi-cascade influence diffusion in online social networks. Authors study the misinformation containment problem under the general case where there is an arbitrary number of cascades.  The considered scenario is more realistic and it applies to complicated real applications in online social networks. Authors provide a formal model and address the misinformation containment problem from the view of combinatorial optimization, and show the misinformation containment problem is not only NP-hard but also admits strong inapproximability property.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7317-on-misinformation-containment-in-online-social-networks",
            "Publication name": "On Misinformation Containment in Online Social Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Amo Tong, Ding-Zhu Du, Weili Wu",
            "Code link": "None",
            "Data": "Twitter dataset, Higgs-10K, Higgs-100K, HepPh \n\n"
        },
        {
            "id": "A^2NET",
            "Using for": "Image Recognition, Video Recognition \u2028\n",
            "Based on": "RESN",
            "Method": "Attention Attention Neural Network",
            "Description": "Author propose the double attention block for gathering and distributing long-range features, an efficient architecture that captures second-order feature statistics and makes adaptive feature assignment. The block can model long-range interdependencies with a low computational and memory footprint and at the same time boost image/video recognition performance significantly.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7318-a2-nets-double-attention-networks",
            "Publication name": "A^2-Nets: Double Attention Networks",
            "Released at": "NIPS",
            "Interacts with": "CNNs",
            "Authors": "Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi Feng",
            "Code link": "Note",
            "Data": "NaN"
        },
        {
            "id": "SSG360",
            "Using for": "Generation",
            "Based on": "NaN",
            "Method": "Self-Supervised Generation of Spatial Audio for 360\u00b0 Video",
            "Description": "We introduce an approach to convert mono audio recorded by a 360\u00b0 video camera into spatial audio, a representation of the distribution of sound over the full viewing sphere. Spatial audio is an important component of immersive 360\u00b0 video viewing, but spatial audio microphones are still rare in current 360\u00b0 video production. Our system consists of end-to-end trainable neural networks that separate individual sound sources and localize them on the viewing sphere, conditioned on multi-modal analysis from the audio and 360\u00b0 video frames. We introduce several datasets, including one filmed ourselves, and one collected in-the-wild from YouTube, consisting of 360\u00b0 videos uploaded with spatial audio. During training, ground truth spatial audio serves as self-supervision and a mixed down mono track forms the input to our network. Using our approach we show that it is possible to infer the spatial localization of sounds based only on a synchronized 360\u00b0 video and the mono audio track.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7319-self-supervised-generation-of-spatial-audio-for-360-video",
            "Publication name": "Self-Supervised Generation of Spatial Audio for 360\u00b0 Video",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Pedro Morgado, Nuno Nvasconcelos, Timothy Langlois, Oliver Wang",
            "Code link": "https://pedro-morgado.github.io/spatialaudiogen. \n",
            "Data": "NaN"
        },
        {
            "id": "ARINDNM",
            "Using for": "Regularization",
            "Based on": "NaN",
            "Method": "Algorithmic Regularization in Learning Deep Homogeneous Models \n",
            "Description": "We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes \u03b7t=O(t\u2212(1/2+\u03b4))(0<\u03b4\u22641/2) automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7321-algorithmic-regularization-in-learning-deep-homogeneous-models-layers-are-automatically-balanced",
            "Publication name": "Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced\n",
            "Released at": "NIPS",
            "Interacts with": "CNNS",
            "Authors": "Simon S. Du, Wei Hu, Jason D. Lee",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MOASF",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Maximization of approximately submodular functions study",
            "Description": "We consider the problem of maximizing a submodular function when given access to its approximate version. Submodular functions are heavily studied in a wide variety of disciplines, since they are used to model many real world phenomena, and are amenable to optimization. However, there are many cases in which the phenomena we observe is only approximately submodular and the approximation guarantees cease to hold. We describe a technique which we call the sampled mean approximation that yields strong guarantees for maximization of submodular functions from approximate surrogates under cardinality and intersection of matroid constraints. In particular, we show tight guarantees for maximization under a cardinality constraint and 1/(1+P) approximation under intersection of P matroids.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7322-optimization-for-approximate-submodularity",
            "Publication name": "Optimization for Approximate Submodularity \n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Yaron Singer, Avinatan Hassidim",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CONCOV",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Conditionally Concave \n",
            "Description": "We analyze and generalize the idea of concave relaxations. We introduce the concepts of conditionally concave and probably conditionally concave energies on polytopes and show that they encapsulate many instances of the graph matching problem, including matching Euclidean graphs and graphs on surfaces. We further prove that local minima of probably conditionally concave energies on general matching polytopes doubly stochastic are with high probability extreme points of the matching polytope eg, permutations.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7323-probably-concave-graph-matching",
            "Publication name": "(Probably) Concave Graph Matching",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Haggai Maron, Yaron Lipman",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PCONCOV",
            "Using for": "Optimization",
            "Based on": "CONCOV",
            "Method": "Probably Conditionally Concave \n",
            "Description": "We analyze and generalize the idea of concave relaxations. We introduce the concepts of conditionally concave and probably conditionally concave energies on polytopes and show that they encapsulate many instances of the graph matching problem, including matching Euclidean graphs and graphs on surfaces. We further prove that local minima of probably conditionally concave energies on general matching polytopes doubly stochastic are with high probability extreme points of the matching polytope eg, permutations.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7323-probably-concave-graph-matching",
            "Publication name": "(Probably) Concave Graph Matching",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Haggai Maron, Yaron Lipman",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DEDEF",
            "Using for": "Adversarial Robustness",
            "Based on": "NaN",
            "Method": "Deep Defense",
            "Description": "Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named \"deep defense\". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7324-deep-defense-training-dnns-with-improved-adversarial-robustness",
            "Publication name": "Deep Defense: Training DNNs with Improved Adversarial Robustness",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Ziang Yan, Yiwen Guo, Changshui Zhang",
            "Code link": "https://github.com/ZiangYan/deepdefense.pytorch.",
            "Data": "MNIST, CIFAR-10, ImageNet \n"
        },
        {
            "id": "SRS",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Scheduled Restart Schemes \n",
            "Description": "We propose a structure-adaptive variant of a state-of-the-art stochastic variance- reduced gradient algorithm Katyusha for regularized empirical risk minimization. The proposed method is able to exploit the intrinsic low-dimensional structure of the solution, such as sparsity or low rank which is enforced by a non-smooth regularization, to achieve even faster convergence rate. This provable algorithmic improvement is done by restarting the Katyusha algorithm according to restricted strong-convexity (RSC) constants. We also propose an adaptive-restart variant which is able to estimate the RSC on the fly and adjust the restart period automati- cally. We demonstrate the effectiveness of our approach via numerical experiments.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7325-rest-katyusha-exploiting-the-solutions-structure-via-scheduled-restart-schemes",
            "Publication name": "Rest-Katyusha: Exploiting the Solution\u2019s Structure via Scheduled Restart Schemes \n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Junqi Tang, Mohammad Golbabaee, Francis Bach, Mike E. davies",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VAE",
            "Using for": "NaN",
            "Based on": "ELBO",
            "Method": "Variational Autoencoder",
            "Description": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
            "Publication date": 2013.0,
            "Publication link": "https://arxiv.org/abs/1312.6114",
            "Publication name": "Auto-Encoding Variational Bayes",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Diederik P Kingma, Max Welling",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "IRG",
            "Using for": "Reparameterization\n",
            "Based on": "VRT",
            "Method": "Implicit Reparameterization Gradients",
            "Description": "IRI provide unbiased estimators for continuous distributions with numerically tractable CDFs. This allows many other important distributions to be used as easily as the Normal distribution in stochastic computation graphs. By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. Authors introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7326-implicit-reparameterization-gradients",
            "Publication name": "Implicit Reparameterization Gradients",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Mikhail Figurnov, Shakir Mohamed, Andriy Mnih",
            "Code link": "None",
            "Data": "20 Newsgroups dataset \n"
        },
        {
            "id": "HBFP",
            "Using for": "Representations \n",
            "Based on": "\n",
            "Method": "Hybrid Block Floating Point \n",
            "Description": "However, DNN training still depends on floating-point representations for convergence, severely limiting the efficiency of accelerators. In this paper, we authors HBFP. HBFP a DNN training framework that maximizes fixed-point arithmetic and minimizes the mantissa width requirements while preserving convergence and two optimizations to BFP, namely tiling and wide weight storage, to improve BFP\u2019s precision with modest area and memory bandwidth overhead. Also authors propose an exploration of the HBFP design space showing that DNNs trained on BFP with 12- and 8-bit mantissas match FP32 accuracy, serving as a drop-in replacement for this representation and show that HBFP exhibits arithmetic density similar to that of fixed-point hardware with the accuracy of FP32 hardware.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7327-training-dnns-with-hybrid-block-floating-point",
            "Publication name": "Training DNNs with Hybrid Block Floating Point",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Mario Drumond, Tao LIN, Martin Jaggi, Babak Falsafi",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "LBLOS",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Learned Bloom Filters and Optimizing by Sandwiching",
            "Description": "Recent work has suggested enhancing Bloom filters by using a pre-filter, based on applying machine learning to determine a function that models the data set the Bloom filter is meant to represent. Here we model such learned Bloom filters, with the following outcomes: (1) we clarify what guarantees can and cannot be associated with such a structure; (2) we show how to estimate what size the learning function must obtain in order to obtain improved performance; (3) we provide a simple method, sandwiching, for optimizing learned Bloom filters; and (4) we propose a design and analysis approach for a learned Bloomier filter, based on our modeling approach.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7328-a-model-for-learned-bloom-filters-and-optimizing-by-sandwiching",
            "Publication name": "A Model for Learned Bloom Filters and Optimizing by Sandwiching",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Michael Mitzenmacher",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SGWG",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "Soft-Gated Warping-GAN",
            "Description": "Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is light-weight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our Warping-GAN that significantly outperforms all existing methods on two large datasets.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7329-soft-gated-warping-gan-for-pose-guided-person-image-synthesis",
            "Publication name": "Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Haoye Dong, Xiaodan Liang, Ke Gong, Hanjiang Lai, Jia Zhu, Jian Yin",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DFD",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Deep Functional Dictionaries",
            "Description": "Various 3D semantic attributes such as segmentation masks, geometric features, keypoints, and materials can be encoded as per-point probe functions on 3D geometries. Given a collection of related 3D shapes, we consider how to jointly analyze such probe functions over different shapes, and how to discover common latent structures using a neural network \u2014 even in the absence of any correspondence information. Our network is trained on point cloud representations of shape geometry and associated semantic functions on that point cloud. These functions express a shared semantic understanding of the shapes but are not coordinated in any way. For example, in a segmentation task, the functions can be indicator functions of arbitrary sets of shape parts, with the particular combination involved not known to the network. Our network is able to produce a small dictionary of basis functions for each shape, a dictionary whose span includes the semantic functions provided for that shape. Even though our shapes have independent discretizations and no functional correspondences are provided, the network is able to generate latent bases, in a consistent order, that reflect the shared semantic structure among the shapes. We demonstrate the effectiveness of our technique in various segmentation and keypoint selection applications.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7330-deep-functional-dictionaries-learning-consistent-semantic-structures-on-3d-models-from-functions",
            "Publication name": "Deep Functional Dictionaries: Learning Consistent Semantic Structures on 3D Models from Functions",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Minhyuk Sung, Hao Su, Ronald Yu, Leonidas J. Guibas",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NONNN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Nonlocal Neural Networks",
            "Description": "Nonlocal neural networks have been proposed and shown to be effective in several computer vision tasks, where the nonlocal operations can directly capture long-range dependencies in the feature space. In this paper, we study the nature of diffusion and damping effect of nonlocal networks by doing spectrum analysis on the weight matrices of the well-trained networks, and then propose a new formulation of the nonlocal block. The new block not only learns the nonlocal interactions but also has stable dynamics, thus allowing deeper nonlocal structures. Moreover, we interpret our formulation from the general nonlocal modeling perspective, where we make connections between the proposed nonlocal network and other nonlocal models, such as nonlocal diffusion process and Markov jump process.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7331-nonlocal-neural-networks-nonlocal-diffusion-and-nonlocal-modeling",
            "Publication name": "Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Yunzhe Tao, Qi Sun, Qiang Du, Wei Liu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RSNvsLP",
            "Using for": "NaN",
            "Based on": "RESN",
            "Method": "ResNets Provably vs. Linear Predictors",
            "Description": "A residual network (or ResNet) is a standard deep neural net architecture, with state-of-the-art performance across numerous applications. The main premise of ResNets is that they allow the training of each layer to focus on fitting just the residual of the previous layer's output and the target output. Thus, we should expect that the trained network is no worse than what we can obtain if we remove the residual layers and train a shallower network instead. However, due to the non-convexity of the optimization problem, it is not at all clear that ResNets indeed achieve this behavior, rather than getting stuck at some arbitrarily poor local minimum. In this paper, we rigorously prove that arbitrarily deep, nonlinear residual units indeed exhibit this behavior, in the sense that the optimization landscape contains no local minima with value above what can be obtained with a linear predictor (namely a 1-layer network). Notably, we show this under minimal or no assumptions on the precise network architecture, data distribution, or loss function used. We also provide a quantitative analysis of approximate stationary points for this problem. Finally, we show that with a certain tweak to the architecture, training the network with standard stochastic gradient descent achieves an objective value close or better than any linear predictor.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7332-are-resnets-provably-better-than-linear-predictors",
            "Publication name": "Are ResNets Provably Better than Linear Predictors",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Ohad Shamir",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DDPAE",
            "Using for": "Autoencoders",
            "Based on": "AE",
            "Method": "Decompositional Disentangled Predictive Auto-Encoder",
            "Description": "Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we would intuitively do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7333-learning-to-decompose-and-disentangle-representations-for-video-prediction",
            "Publication name": "Learning to Decompose and Disentangle Representations for Video Prediction",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F. Fei-Fei, Juan Carlos Niebles",
            "Code link": "NaN",
            "Data": "MNIST"
        },
        {
            "id": "MLTWL",
            "Using for": "Multi-Task Learning",
            "Based on": "NaN",
            "Method": "Multi-task learning using uncertainty to weigh losses",
            "Description": "Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1705.07115",
            "Publication name": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.\u2028\n",
            "Released at": "CVPR\u2028\n",
            "Interacts with": "NaN",
            "Authors": "A. Kendall, Y. Gal, and R. Cipolla. \n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MLTASMOO",
            "Using for": "Multi-Task Learning",
            "Based on": "RESN",
            "Method": "Multi-Task Learning as Multi-Objective Optimization",
            "Description": "In order to apply multi-objective optimization to MTL, authors described an efficient algorithm as well as specific approximations that yielded a deep MTL algorithm with almost no computational overhead. Authors explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, they use algorithms developed in the gradient-based multi- objective optimization literature.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7334-multi-task-learning-as-multi-objective-optimization",
            "Publication name": "Multi-Task Learning as Multi-Objective Optimization",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ozan Sener, Vladlen Koltun",
            "Code link": "None",
            "Data": "MultiMNIST, CelebA \n\n"
        },
        {
            "id": "GCN",
            "Using for": "NaN",
            "Based on": "CNN",
            "Method": "Graph Convolutional Networks",
            "Description": "Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require that a model learns from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on graph convolutional network (GCN) and gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.",
            "Publication date": 2019.0,
            "Publication link": "https://arxiv.org/abs/1812.08434 ",
            "Publication name": "Graph Neural Networks: A Review of Methods and Applications",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "TS",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Tree Search",
            "Description": "In computer science, a search tree is a tree data structure used for locating specific keys from within a set. In order for a tree to function as a search tree, the key for each node must be greater than any keys in subtrees on the left and less than any keys in subtrees on the right.",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "COGCNTS",
            "Using for": "NaN",
            "Based on": "GCN, TS",
            "Method": "Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search",
            "Description": "We present a learning-based approach to computing solutions for certain NP-hard problems. Our approach combines deep learning techniques with useful algorithmic elements from classic heuristics. The central component is a graph convolutional network that is trained to estimate the likelihood, for each vertex in a graph, of whether this vertex is part of the optimal solution. The network is designed and trained to synthesize a diverse set of solutions, which enables rapid exploration of the solution space via tree search. The presented approach is evaluated on four canonical NP-hard problems and five datasets, which include benchmark satisfiability problems and real social network graphs with up to a hundred thousand nodes. Experimental results demonstrate that the presented approach substantially outperforms recent deep learning work, and performs on par with highly optimized state-of-the-art heuristic solvers for some NP-hard problems. Experiments indicate that our approach generalizes across datasets, and scales to graphs that are orders of magnitude larger than those used during training.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7335-combinatorial-optimization-with-graph-convolutional-networks-and-guided-tree-search",
            "Publication name": "Combinatorial Optimization with Graph Convolutional Networks and Guided Tree Search",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Zhuwen Li, Qifeng Chen, Vladlen Koltun",
            "Code link": "NaN",
            "Data": "BUAA-MC dataset \n"
        },
        {
            "id": "ATTN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Attention",
            "Description": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VGG",
            "Using for": "Classification",
            "Based on": "CNN\n",
            "Method": "VGGNet",
            "Description": "VGGNet consists of N convolutional layers and is very appealing because of its very uniform architecture. ",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SEENET",
            "Using for": "Segmentation",
            "Based on": "CNN, ATTN, VGG,  RESN \n",
            "Method": "Self-Erasing Network \n",
            "Description": "Authors propose to extract background information based on the initial attention maps produced by the initial attention generator by thresholding the maps into three zones, and design two self-erasing strategies, both of which aim at prohibiting the attention regions from spreading to unexpected regions.  Method functionally separate the images into three zones in spatial dimension, the internal \u201cattention zone\u201d, the external \u201cbackground zone\u201d, and the middle \u201cpotential zone\u201d. By introducing the background prior, we aim to drive attention networks into a self-erasing state so that the observable regions can be restricted to non-background areas, avoiding the continuous spread of attention areas that are already near a state of perfection. The segmentation results based on proxy ground-truths greatly outperform existing state-of-the-art results.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7336-self-erasing-network-for-integral-object-attention",
            "Publication name": "Self-Erasing Network for Integral Object Attention",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Qibin Hou, PengTao Jiang, Yunchao Wei, Ming-Ming Cheng",
            "Code link": "http://mmcheng.net/SeeNet/. \n",
            "Data": "Pascal VOC dataset,"
        },
        {
            "id": "GLEM",
            "Using for": "Image Description",
            "Based on": "NaN",
            "Method": "Geometrical Layout Encoding Module \n",
            "Description": "Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a novel method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our novel method significantly benefits two main parts of the scene graph generation task: object classification and relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7337-linknet-relational-embedding-for-scene-graph",
            "Publication name": "LinkNet: Relational Embedding for Scene Graph",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GCEM",
            "Using for": "Image Description",
            "Based on": "NaN",
            "Method": "Global Context Encoding Module \n",
            "Description": "Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a novel method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our novel method significantly benefits two main parts of the scene graph generation task: object classification and relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7337-linknet-relational-embedding-for-scene-graph",
            "Publication name": "LinkNet: Relational Embedding for Scene Graph",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LinkNet",
            "Using for": "Image Description",
            "Based on": "GLEM,  GCEM",
            "Method": "LinkNet",
            "Description": "Objects and their relationships are critical contents for image understanding. A scene graph provides a structured description that captures these properties of an image. However, reasoning about the relationships between objects is very challenging and only a few recent works have attempted to solve the problem of generating a scene graph from an image. In this paper, we present a novel method that improves scene graph generation by explicitly modeling inter-dependency among the entire object instances. We design a simple and effective relational embedding module that enables our model to jointly represent connections among all related objects, rather than focus on an object in isolation. Our novel method significantly benefits two main parts of the scene graph generation task: object classification and relationship classification. Using it on top of a basic Faster R-CNN, our model achieves state-of-the-art results on the Visual Genome benchmark. We further push the performance by introducing global context encoding module and geometrical layout encoding module. We validate our final model, LinkNet, through extensive ablation studies, demonstrating its efficacy in scene graph generation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7337-linknet-relational-embedding-for-scene-graph",
            "Publication name": "LinkNet: Relational Embedding for Scene Graph",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Sanghyun Woo, Dahun Kim, Donghyeon Cho, In So Kweon",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "FM1",
            "Using for": "Initialization \n",
            "Based on": "NaN",
            "Method": "FM1",
            "Description": "In this article, authors give a rigorous analysis of the layerwise length scales in fully connected, convolu- tional, and residual ReLU networks at initialization. \nFM1  is the mean length scale in the final layer increases/decreases exponentially with the depth.  FM1 is dependent on weight initialization. Initializing weights with the correct variance (in fully connected and convolutional networks) and correctly weighting residual modules (in residual networks) prevents the mean size of activations from becoming exponentially large or small as a function of the depth, allowing training to start for deeper architectures. \u2028\n\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture",
            "Publication name": "How to Start Training: The Effect of Initialization and Architecture",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Boris Hanin, David Rolnick",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "FM2",
            "Using for": "Initialization \n",
            "Based on": "NaN",
            "Method": "FM2",
            "Description": "In this article, authors give a rigorous analysis of the layerwise length scales in fully connected, convolu- tional, and residual ReLU networks at initialization. \nFM2 is dependent on architecture.. Is empirical variance of length scales across layers grows exponentially with the depth. For fully connected and convolutional networks, Wider layers prevent FM2, again allowing training to start for deeper architectures. In the case of constant-width networks, the width should grow approximately linearly with the depth to avoid FM2. For residual networks, FM2 is largely independent of the architecture. Provided that residual modules are weighted to avoid FM1, FM2 can never occur. This qualitative difference between fully connected and residual networks can help to explain the empirical success of the latter, allowing deep and relatively narrow networks to be trained more readily. \n \n\u2028\n\n\n\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture",
            "Publication name": "How to Start Training: The Effect of Initialization and Architecture",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Boris Hanin, David Rolnick",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "NNREVG",
            "Using for": "Statistical Behavior \n",
            "Based on": "NaN",
            "Method": "Neural Networks Rise to Exploding and Vanishing Gradients \n",
            "Description": "Author derive new exact formulas for the joint even moments of the entries of the input-output Jacobian in a fully connected ReLU net with random weights and biases. These formulas hold at finite depth and width, and prove that the empirical variance of gradients in a fully connected ReLU net is exponential in the sum of the reciprocals of the hidden layer widths. This suggests that when this sum of reciprocals is too large, early training dynamics are very slow and it may take many epochs to achieve better-than-chance performance. Also author prove that, so long as weights and biases are initialized independently with the correct variance scaling, whether the EVGP occurs in fully connected ReLU nets is a function only of the architecture and not the distributions from which the weights and biases are drawn.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7339-which-neural-net-architectures-give-rise-to-exploding-and-vanishing-gradients",
            "Publication name": "Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Boris Hanin",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "CEM",
            "Using for": "Feature Selection\n\u2028\n",
            "Based on": "NaN",
            "Method": "Contrastive Explanations Method \n",
            "Description": "Authors provides contrastive explanations justifying the classification of an input by a black box classifier such as a deep neural network. Given an input they find what should be minimally and sufficiently present to justify its classification and analogously what should be minimally and necessarily absent. Authors provided a novel explanation method called CEM, which finds not only what should be minimally present in the input to justify its classification by black box classifiers such as neural networks, but also finds contrastive perturbations, in particular, additions, that should be necessarily absent to justify the classification.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives",
            "Publication name": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Amit Dhurandhar, Pin-Yu Chen, Ronny Luss, Chun-Chen Tu, Paishun Ting, Karthikeyan Shanmugam, Payel Das",
            "Code link": "https://github.com/IBM/ Contrastive-Explanation-Method \n",
            "Data": "MNIST"
        },
        {
            "id": "HitNet",
            "Using for": "Speech Recognition, Machine Translation, Language Modeling, Question Answering \n",
            "Based on": "LSTM, GRU\n",
            "Method": "Hybrid Ternary Network \n",
            "Description": "Authors analyze the value distributions of weights and activations to reveal the reason behind the accuracy degradation. In HitNet authors take full advantages of various quantization methods. Specifically, they use threshold ternary quantization method for weight quantization and Bernoulli ternary quantization method for activation quantization. HitNet adopts different quantization methods to quantize weights and activations according to their statistical characteristics.  introduce a sloping factor into the activation functions. HitNet achieves significant accuracy improvement compared with previous work, like the perplexity per word of a ternary LSTM",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7341-hitnet-hybrid-ternary-recurrent-neural-network",
            "Publication name": "HitNet: Hybrid Ternary Recurrent Neural Network",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Peiqi Wang, Xinfeng Xie, Lei Deng, Guoqi Li, Dongsheng Wang, Yuan Xie",
            "Code link": "None",
            "Data": "Penn Tree Bank (PTB)\n"
        },
        {
            "id": "UNIFRAM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Unified Framework",
            "Description": "In this paper authors present a unified framework for analyzing abstractions that can express all types of abstractions and solution concepts used in prior papers with performance guarantees\u2014while maintaining comparable bounds on abstraction quality. Moreover, framework gives an exact decomposition of abstraction error in a much broader class of games, albeit only in an ex-post sense, as results depend on the specific strategy chosen. We also show, via counterexample, that such assumptions are necessary for some games.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7342-a-unified-framework-for-extensive-form-game-abstraction-with-bounds",
            "Publication name": "A Unified Framework for Extensive-Form Game Abstraction with Bounds",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Christian Kroer, Tuomas Sandholm",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DRPT",
            "Using for": "Regularization",
            "Based on": "NaN",
            "Method": "Dropout",
            "Description": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods.",
            "Publication date": 2014.0,
            "Publication link": "http://jmlr.org/papers/v15/srivastava14a.html",
            "Publication name": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
            "Released at": "JMLR",
            "Interacts with": "NaN",
            "Authors": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NCMN",
            "Using for": "Normalization",
            "Based on": "BN",
            "Method": "Non-correlating Multiplicative Noise \n",
            "Description": "This method exploits batch normalization to remove the correlation effect in a simple yet effective way. From an information perspective, authors consider injecting multiplicative noise into a DNN as training the network to solve the task with noisy information pathways, which leads to the observation that multiplicative noise tends to increase the correlation between features, so as to increase the signal-to-noise ratio of information pathways. Specifically, authors first decompose noisy features into the sum of two components, a signal component and a noise component, and then truncate the gradient through the latter, i.e., treat it as a constant. However, naively modifying the gradient would encourage the magnitude of features to grow in order to increase the signal-to-noise ratio, causing optimization difficulties. Authors solve this problem by combining the aforementioned technique with batch normalization , which effectively counteracts the tendency of increasing feature magnitude.  Proposed method outperforms standard multiplicative noise by a large margin, proving it to be a better alternative for batch-normalized networks.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7343-removing-the-feature-correlation-effect-of-multiplicative-noise",
            "Publication name": "Removing the Feature Correlation Effect of Multiplicative Noise",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Zijun Zhang, Yining Zhang, Zongpeng Li",
            "Code link": "https://github.com/zj10/NCMN. \n",
            "Data": "CIFAR-10, CIFAR-100\n \n"
        },
        {
            "id": "MEFGC",
            "Using for": "Classification",
            "Based on": "NaN",
            "Method": "Maximum-Entropy Fine Grained Classification",
            "Description": "Fine-Grained Visual Classification (FGVC) is an important computer vision problem that involves small diversity within the different classes, and often requires expert annotators to collect data. Utilizing this notion of small visual diversity, we revisit Maximum-Entropy learning in the context of fine-grained classification, and provide a training routine that maximizes the entropy of the output probability distribution for training convolutional neural networks on FGVC tasks. We provide a theoretical as well as empirical justification of our approach, and achieve state-of-the-art performance across a variety of classification tasks in FGVC, that can potentially be extended to any fine-tuning task. Our method is robust to different hyperparameter values, amount of training data and amount of training label noise and can hence be a valuable tool in many similar problems.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7344-maximum-entropy-fine-grained-classification",
            "Publication name": "Maximum-Entropy Fine Grained Classification",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, Nikhil Naik",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "OLMC",
            "Using for": "NaN",
            "Based on": "MCMC",
            "Method": "On Learning Markov Chains",
            "Description": "The problem of estimating an unknown discrete distribution from its samples is a fundamental tenet of statistical learning. Over the past decade, it attracted significant research effort and has been solved for a variety of divergence measures. Surprisingly, an equally important problem, estimating an unknown Markov chain from its samples, is still far from understood. We consider two problems related to the min-max risk (expected loss) of estimating an unknown k-state Markov chain from its n sequential samples: predicting the conditional distribution of the next sample with respect to the KL-divergence, and estimating the transition matrix with respect to a natural loss induced by KL or a more general f-divergence measure. For the first measure, we determine the min-max prediction risk to within a linear factor in the alphabet size, showing it is \\Omega(k\\log\\log n/n) and O(k^2\\log\\log n/n). For the second, if the transition probabilities can be arbitrarily small, then only trivial uniform risk upper bounds can be derived. We therefore consider transition probabilities that are bounded away from zero, and resolve the problem for essentially all sufficiently smooth f-divergences, including KL-, L_2-, Chi-squared, Hellinger, and Alpha-divergences.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7345-on-learning-markov-chains",
            "Publication name": "On Learning Markov Chains",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Yi HAO, Alon Orlitsky, Venkatadheeraj Pichapati",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NCPIC",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Neural Compositional Paradigm for Image Captioning",
            "Description": "Mainstream captioning models often follow a sequential structure to generate captions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7346-a-neural-compositional-paradigm-for-image-captioning",
            "Publication name": "A Neural Compositional Paradigm for Image Captioning",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Bo Dai, Sanja Fidler, Dahua Lin",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "QLGCIS",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Quantifying Learning Guarantees for Convex but Inconsistent Surrogates",
            "Description": "We study consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. (2017) for the quantitative analysis of consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in a new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory to two concrete cases: multi-class classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7347-quantifying-learning-guarantees-for-convex-but-inconsistent-surrogates",
            "Publication name": "Quantifying Learning Guarantees for Convex but Inconsistent Surrogates",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Kirill Struminsky, Simon Lacoste-Julien, Anton Osokin",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "KNN",
            "Using for": "Regression, Classification",
            "Based on": "NaN",
            "Method": "k-nearest neighbors algorithm",
            "Description": "Non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space",
            "Publication date": 1967.0,
            "Publication link": "http://ssg.mit.edu/cal/abs/2000_spring/np_dens/classification/cover67.pdf",
            "Publication name": " Nearest neighbor pattern classification\"",
            "Released at": "IEEE TRANSACTIONS ON INFORMATION THEORY,",
            "Interacts with": "NaN",
            "Authors": "Cover T.M, Hart P.E ",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DBIIR",
            "Using for": "Reinforcement Learning, Dialog System, Image Search",
            "Based on": "ENCDR, MLP, CNN, GRU, K-NNs",
            "Method": "Dialog-based Interactive Image Retrieval",
            "Description": "A new approach to interactive visual content retrieval by introducing a novel form of user feedback based on natural language. Authors model the ranking percentile as the environment reward received by the agent and frame the learning process in a reinforcement learning setting with the goal of maximizing the expected sum of discounted rewards. A new vision/NLP task and machine learning problem setting for interactive visual content search, where the dialog agent learns to interact with a human user over the course of several dialog turns, and the user gives feedback in natural language. A novel end-to-end deep dialog manager architecture, which addresses the above problem setting in the context of image retrieval. The network is trained based on an efficient policy optimization strategy, employing triplet loss and model-based policy improvement. The introduction of a computer vision task, relative image captioning, where the generated captions describe the salient visual differences between two images, which is distinct from and complementary to context-aware discriminative image captioning, where the absolute attributes of one image that discriminate it from another are described. The contribution of a new dataset, which supports further research on the task of relative image captioning.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7348-dialog-based-interactive-image-retrieval",
            "Publication name": "Dialog-based Interactive Image Retrieval",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Feris",
            "Code link": "www.spacewu.com/posts/fashion-retrieval/ \n",
            "Data": "Shoes dataset"
        },
        {
            "id": "NGD",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Normalized Gradient Descent \n",
            "Description": "The note considers normalized gradient descent (NGD), a natural modification of classical gradient descent (GD) in optimization problems. A serious shortcoming of GD in non-convex problems is that GD may take arbitrarily long to escape from the neighborhood of a saddle point. This issue can make the convergence of GD arbitrarily slow, particularly in high-dimensional non-convex problems where the relative number of saddle points is often large. The paper focuses on continuous-time descent. It is shown that, contrary to standard GD, NGD escapes saddle points `quickly.' In particular, it is shown that (i) NGD `almost never' converges to saddle points and (ii) the time required for NGD to escape from a ball of radius r about a saddle point x\u2217 is at most 5\u03ba\u221ar, where \u03ba is the condition number of the Hessian of f at x\u2217. As an application of this result, a global convergence-time bound is established for NGD under mild assumptions.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1711.05224",
            "Publication name": "Revisiting Normalized Gradient Descent: Fast Evasion of Saddle Points",
            "Released at": "NaN",
            "Interacts with": "DNNs",
            "Authors": "Ryan Murray, Brian Swenson, Soummya Kar",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SNVRG",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Stochastic nested variance reduction for nonconvex optimization \n",
            "Description": "We study finite-sum nonconvex optimization problems, where the objective function is an average of n nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses K+1 nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration.",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1806.07811",
            "Publication name": "Stochastic Nested Variance Reduction for Nonconvex Optimization",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Dongruo Zhou, Pan Xu, Quanquan Gu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SGD",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Stochastic Gradient Descent",
            "Description": "Is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It is called stochastic because the method uses randomly selected (or shuffled) samples to evaluate the gradients, hence SGD can be regarded as a stochastic approximation of gradient descent optimization. ",
            "Publication date": 1957.0,
            "Publication link": "https://projecteuclid.org/euclid.aoms/1177729586",
            "Publication name": "A Stochastic Approximation Method",
            "Released at": "NaN",
            "Interacts with": "DNNs",
            "Authors": "Herbert Robbins, Sutton Monro",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SPIDER",
            "Using for": "Optimization, Variance Reduction",
            "Based on": "NGD",
            "Method": "Stochastic Path-Integrated Differential EstimatoR \n",
            "Description": "Authors study the optimization problem minimize where the stochastic component, indexed by some random vector, is smooth and possibly non-convex, Proposed method significantly avoids excessive access of stochastic oracles and reduces the time complexity. SPIDER technique is a much more general variance-reduced estimation method for many quantities (not limited to gradients) and can be flexibly applied to numerous problems, e.g. stochastic zeroth-order method.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7349-spider-near-optimal-non-convex-optimization-via-stochastic-path-integrated-differential-estimator",
            "Publication name": "SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator\n",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SPIDER+",
            "Using for": "Optimization, Variance reduction",
            "Based on": "NGD",
            "Method": "Stochastic Path-Integrated Differential EstimatoR \n",
            "Description": "Algorithm for finding an approximate second-order stationary point for non-convex stochastic optimization problem. this is also the first time that the gradient cost achieved with standard assumptions.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7349-spider-near-optimal-non-convex-optimization-via-stochastic-path-integrated-differential-estimator",
            "Publication name": "SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path-Integrated Differential Estimator\n",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "GANES",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "Generative Adversarial Networks Empirical study",
            "Description": "Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. Authors conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. Authors find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, authors also propose several data sets on which precision and recall can be computed. Experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, authors did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN \n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7350-are-gans-created-equal-a-large-scale-study",
            "Publication name": "Are GANs Created Equal? A Large-Scale Study",
            "Released at": "NIPS",
            "Interacts with": "GAN",
            "Authors": "Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier Bousquet",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "RPN",
            "Using for": "Object Detection",
            "Based on": "NaN",
            "Method": "Region Proposal Network \n",
            "Description": "Region Proposal Network (RPN) shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1506.01497",
            "Publication name": "Faster r-cnn: Towards real-time object detection with region proposal networks \u2028\n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "FRCNN",
            "Using for": "Object Detection\n",
            "Based on": "RPN, CNN",
            "Method": "Faster R-CNN \n",
            "Description": "Object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully convolutional network that proposes regions, and the second module is the Fast R-CNN detector that uses the proposed regions. ",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1506.01497",
            "Publication name": "Faster r-cnn: Towards real-time object detection with region proposal networks \u2028\n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CoordConv",
            "Using for": "Classification, Regression, Reinforcement Learning",
            "Based on": "CNN",
            "Method": "Coordinate transform Convolution",
            "Description": "We have shown the curious inability of CNNs to model the coordinate transform task, shown a simple fix in the form of the CoordConv layer, and given results that suggest including these layers can boost performance in a wide range of applications. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST detection showed 24% better IOU when using CoordConv, and in the Reinforcement Learning (RL) domain agents playing Atari games benefit significantly from the use of CoordConv layers.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/8169-an-intriguing-failing-of-convolutional-neural-networks-and-the-coordconv-solution",
            "Publication name": "An intriguing failing of convolutional neural networks and the CoordConv solution",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, Jason Yosinski",
            "Code link": "https://github.com/uber-research/coordconv \n",
            "Data": "Not-so-Clevr dataset, MNIST\n"
        },
        {
            "id": "QSGD",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "SGD with unbiased stochastic quantization functions \n",
            "Description": "Family of compression schemes which allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with standard uncompressed techniques on a variety of real tasks. \nIn particular, experiments show that gradient quantization applied to training of deep neural networks for image classification and automated speech recognition can lead to significant reductions in communication cost, and end-to-end training time. ",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1610.02132",
            "Publication name": "Qsgd: Communication-efficient sgd via gradient quantization and encoding. \u2028\n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PQASGD",
            "Using for": "Optimization",
            "Based on": "SGD, QSGD",
            "Method": "Periodic Quantized Averaging",
            "Description": "The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks. Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost. However, there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. Authors analyze the convergence rate of distributed SGD with two communication reducing techniques: sparse parameter averaging and gradient quantization,and study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. Authors show that O(1/\u221aMK) convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly. Authors also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost while preserving the O(1/\u221aMK) convergence rate.  Evaluation validates theoretical results and shows that PQASGD can converge as fast as full-communication SGD with only 3%\u22125% communication data size.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7519-a-linear-speedup-analysis-of-distributed-deep-learning-with-sparse-and-quantized-communication",
            "Publication name": "A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Peng Jiang, Gagan Agrawal",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "PIP",
            "Using for": "Similarity, Dissimilarity",
            "Based on": "None",
            "Method": "Pairwise Inner Product \n",
            "Description": "A novel metric on the dissimilarity between word embeddings. Using techniques from matrix perturbation theory, we reveal a fundamental bias-variance trade-off in dimensionality selection for word embeddings. \nAuthors develop a mathematical framework that reveals a fundamental bias-variance trade-off in dimensionality selection. They explain the existence of an optimal dimensionality, a phenomenon commonly observed but lacked explanations;\nAuthors quantify the robustness of embedding algorithms using the exponent parameter \u03b1, and establish that many widely used embedding algorithms, including skip-gram and GloVe, are robust to over-fitting",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding",
            "Publication name": "On the Dimensionality of Word Embedding",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Zi Yin, Yuanyuan Shen",
            "Code link": "https://github.com/aaaasssddf/PIP-experiments \n",
            "Data": "WordSim353 MTurk771"
        },
        {
            "id": "HMAD",
            "Using for": "Adversarial Attacks, Adversarial Robustness",
            "Based on": "None",
            "Method": "Human Machine Adversarial Examples",
            "Description": "Machine learning models are vulnerable to adversarial examples, However, it is still an open question whether humans are prone to similar mistakes. In this work, authors construct adversarial examples that transfer from computer vision models to the human visual system. In order to successfully construct these examples and observe their effect, we leverage three key ideas from machine learning, neuroscience, and psychophysics. First, they use the recent black box adversarial example construction techniques that create adversarial examples for a target model without access to the model\u2019s architecture or parameters. Second, authors adapt machine learning models to mimic the initial visual processing of humans, making it more likely that adversarial examples will transfer from the model to a human observer. Third, evaluate classification decisions of human observers in a time-limited setting, so that even subtle effects on human perception are detectable.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7647-adversarial-examples-that-fool-both-computer-vision-and-time-limited-humans",
            "Publication name": "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans \n",
            "Released at": "NIPS",
            "Interacts with": "GANs",
            "Authors": "Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey Kurakin, Ian Goodfellow, Jascha Sohl-Dickstein",
            "Code link": "None",
            "Data": "ImageNet \n"
        },
        {
            "id": "DCMA",
            "Using for": "Classification, Regression\n",
            "Based on": "DNNs",
            "Method": "Dendritic cortical microcircuits approximate \n",
            "Description": "Humans have the ability to perform fast (e.g., one-shot) learning, whereas neural networks trained by backpropagation of error require iterating over many training examples to learn. This is an important open problem that stands in the way of understanding the neuronal basis of intelligence. One possibility where our model naturally fits is to consider multiple subsystems that transfer knowledge to each other and learn at different rates. Overall, this work provides a new view on how the brain may solve the credit assignment problem for time-continuous input streams by approximating the backpropagation algorithm, and bringing together many puzzling features of cortical microcircuits.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/8089-dendritic-cortical-microcircuits-approximate-the-backpropagation-algorithm",
            "Publication name": "Dendritic cortical microcircuits approximate the backpropagation algorithm \n",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Jo\u00e3o Sacramento, Rui Ponte Costa, Yoshua Bengio, Walter Senn",
            "Code link": "None",
            "Data": "MNIST"
        },
        {
            "id": "INFOGAN",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "Information-Theoretic extension Generative Adversarial Network",
            "Description": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.",
            "Publication date": 2016.0,
            "Publication link": "https://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets",
            "Publication name": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "\u03b2-VAE",
            "Using for": "NaN",
            "Based on": "VAE",
            "Method": "\u03b2-VAE",
            "Description": "Learning an interpretable factorised representation of the independent data gen- erative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce \u03b2-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hy- perparameter \u03b2 that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that \u03b2-VAE with appropriately tuned \u03b2 > 1 qualitatively outperforms VAE (\u03b2 = 1), as well as state of the art unsu- pervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, \u03b2-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter \u03b2, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data",
            "Publication date": 2017.0,
            "Publication link": "https://openreview.net/references/pdf?id=Sy2fzU9gl",
            "Publication name": "LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK\n",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "JOINTVAE",
            "Using for": "Generation",
            "Based on": "\u03b2-VAE\n",
            "Method": "Joint Continuous and Discrete Representations Variational Autorncoder\n",
            "Description": "Framework for learning disentangled and interpretable jointly continu- ous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Authors build the encoder to output the parameters of the continuous distribution \u03bc and \u03c3^2 and of each of the discrete distributions \u03b1(i). Then sample zi and ci using the reparametrization trick and concatenate z and c into one latent vector which is passed as input to the decoder. Authors have shown that JointVAE disentangles factors of variation on several datasets while producing realistic samples.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7351-learning-disentangled-joint-continuous-and-discrete-representations",
            "Publication name": "Learning Disentangled Joint Continuous and Discrete Representations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Emilien Dupont",
            "Code link": "https://github.com/ Schlumberger/joint-vae. \n",
            "Data": "MNIST, FashionMNIST ,CelebA, Chairs \n"
        },
        {
            "id": "MATN",
            "Using for": "Few-shot learning",
            "Based on": "NaN",
            "Method": "Matching Networks \n",
            "Description": "Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1606.04080",
            "Publication name": "Matching Networks for One Shot Learning",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, Daan Wierstra",
            "Code link": "NaN",
            "Data": "Omniglot, Penn Treebank"
        },
        {
            "id": "TEN",
            "Using for": "Few-shot learning, Task conditioning \n",
            "Based on": "CNN, BN, RESN",
            "Method": "Task Embedding Network \n",
            "Description": "Simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. TEN introduces additional complexity into the architecture via task conditioning layers inserted after the convolutional and batch norm blocks. Method predicts the values of and parameters for each convolutional layer in the feature extractor from the task representation.\n\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning",
            "Publication name": "TADAM: Task dependent adaptive metric for improved few-shot learning",
            "Released at": "NIPS",
            "Interacts with": "CNN, RNN, ATTN",
            "Authors": "Boris Oreshkin, Pau Rodr\u00edguez L\u00f3pez, Alexandre Lacoste",
            "Code link": "None",
            "Data": "CIFAR100, mini-Imagenet \n\n"
        },
        {
            "id": "TADAM",
            "Using for": "Image classification, Few-shot learning",
            "Based on": "TEN, RESN, CNN, RNN, ATTN",
            "Method": "Task dependent adaptive metric for improved few-shot learning",
            "Description": "In this work, authors identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. Authors propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, they propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. Authors show that a non-trivial interaction between the similarity metric and the cost function can be exploited to improve the performance of a given similarity metric via scaling. Also they extend the very notion of the metric space by making it task dependent via conditioning the feature extractor on the specific task. The experimental results obtained on two independent challenging datasets demonstrated that the proposed approach significantly improves over existing results and achieves state-of-the-art on few-shot image classification task.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7352-tadam-task-dependent-adaptive-metric-for-improved-few-shot-learning",
            "Publication name": "TADAM: Task dependent adaptive metric for improved few-shot learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Boris Oreshkin, Pau Rodr\u00edguez L\u00f3pez, Alexandre Lacoste",
            "Code link": "None",
            "Data": "CIFAR100, mini-Imagenet \n\n"
        },
        {
            "id": "STLCP",
            "Using for": "NaN",
            "Based on": "Spatio-Temporal Low Count Processes",
            "Method": "NaN",
            "Description": "There is significant interest in being able to predict where crimes will happen, for example to aid in the efficient tasking of police and other protective measures. We aim to model both the temporal and spatial dependencies often exhibited by violent crimes in order to make such predictions. The temporal variation of crimes typically follows patterns familiar in time series analysis, but the spatial patterns are irregular and do not vary smoothly across the area. Instead we find that spatially disjoint regions exhibit correlated crime patterns. It is this indeterminate inter-region correlation structure along with the low-count, discrete nature of counts of serious crimes that motivates our proposed forecasting tool. In particular, we propose to model the crime counts in each region using an integer-valued first order autoregressive process. We take a Bayesian nonparametric approach to flexibly discover a clustering of these region-specific time series. We then describe how to account for covariates within this framework. Both approaches adjust for seasonality.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1304.5642",
            "Publication name": "Spatio-temporal low count processes with application to violent crime events. \u2028\n",
            "Released at": "Statistica Sinica \u2028\n",
            "Interacts with": "NaN",
            "Authors": "Sivan Aldor-Noiman, Lawrence D. Brown, Emily B. Fox, and Robert A. Stine. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ARD",
            "Using for": "GP",
            "Based on": "None",
            "Method": "Automatic relevance determination\n",
            "Description": "This method reduces dimensionality by relating the observation vectors to a lower dimensional subspace and encodes basis sparsity through their coefficients to account for the inherent redundancy. In particular, authors characterize the loadings as a sparse combination of unknown basis functions. The basis functions vary over X with Gaussian processes as their convenient priors. ARD method place a shrinkage prior on its size with the loadings increasingly shrunk towards zero as their column index increases. The induced covariance matrices are regularized quadratic functions of these basis elements. Authors validate our model across domains and demonstrate its superior performance over the state-of-the-art methods.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7354-sparse-covariance-modeling-in-high-dimensions-with-gaussian-processes",
            "Publication name": "NIPS",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Rui Li, Kishan KC, Feng Cui, Justin Domke, Anne Haake",
            "Code link": "None",
            "Data": "DREAM5 network inference challenge.\n"
        },
        {
            "id": "WNLL",
            "Using for": "Activation",
            "Based on": "None",
            "Method": "Weighted nonlocal Laplacian \n",
            "Description": "Authors replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. Framework combines both learned decision boundary and nearest neighbor information for classification. During training, WNLL activation plays two roles: on one hand, the alternating between linear and WNLL activations benefits each other which enables the neural nets to learn features that is appropriate for both linear classification and WNLL based manifold interpolation. On the other hand, in the case where we lack sufficient training data, the training of DNNs usually gets stuck at some bad local minima which cannot generalize well on new data.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7355-deep-neural-nets-with-interpolating-function-as-output-activation",
            "Publication name": "Deep Neural Nets with Interpolating Function as Output Activation",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley Osher",
            "Code link": "https://github.com/ BaoWangMath/DNN-DataDependentActivation",
            "Data": "CIFAR10, CIFAR100 \n"
        },
        {
            "id": "FISHNet",
            "Using for": "Pose estimation, Classification, High Resolution",
            "Based on": "CNN, RESN",
            "Method": "Fish-like network",
            "Description": "In this method the information of all resolutions is preserved and refined for the final task. Besides, the observe that existing works still cannot directly propagate the gradient information from deep layers to shallow layers.  The design of feature preservation and refinement not only helps to handle the problem of direct gradient propagation, but also is friendly to pixel-level and region-level tasks. Experimental results have demonstrated and validated the improvement of our network.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction",
            "Publication name": "FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Shuyang Sun, Jiangmiao Pang, Jianping Shi, Shuai Yi, Wanli Ouyang",
            "Code link": "https://github.com/kevin-ssy/FishNet. \n",
            "Data": "ImageNet-1k, COCO Detection 2018 challenge \n \n"
        },
        {
            "id": "RPF",
            "Using for": "Reinforcement Learning",
            "Based on": "RNN, ATTN",
            "Method": "Robust Path Following \n",
            "Description": "In this paper, we studied the task of following paths under noisy actuation and changing environments. We operationalized insights from classical robotics with learning-based models and designed neural architectures that implicitly tackle localization to output actions to convey the robot directly to the target location. As input, model takes a sequence of images and actions at these images. It abstracts this sequence into a sequence of memories. A second, recurrent, network \u03c0 uses this sequence of memories to emit actions that retrace the path. At each time-step, \u03c0 reads in: (a) the sequence, softly attending to the relevant part at past time-step via \u03b7; (b) the current observation O from the world; as well as its recurrent hidden state h. As output, \u03c0 updates the attention location \u03b7 and its hidden state, and emits action a\u02c6 that the agent executes in the world.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7357-visual-memory-for-robust-path-following",
            "Publication name": "Visual Memory for Robust Path Following",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ashish Kumar, Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik",
            "Code link": "https://ashishkumar1993.github.io/rpf/. \n",
            "Data": "Stanford Building Parser Dataset, Matterport 3D Dataset, SUNCG synthetic indoor environments \n\n\n"
        },
        {
            "id": "VISMEM",
            "Using for": "Reinforcement Learning",
            "Based on": "RNN, ATTN",
            "Method": "Visual Memory\n",
            "Description": "In this paper, we studied the task of following paths under noisy actuation and changing environments. We operationalized insights from classical robotics with learning-based models and designed neural architectures that implicitly tackle localization to output actions to convey the robot directly to the target location. As input, model takes a sequence of images and actions at these images. It abstracts this sequence into a sequence of memories. A second, recurrent, network \u03c0 uses this sequence of memories to emit actions that retrace the path. At each time-step, \u03c0 reads in: (a) the sequence, softly attending to the relevant part at past time-step via \u03b7; (b) the current observation O from the world; as well as its recurrent hidden state h. As output, \u03c0 updates the attention location \u03b7 and its hidden state, and emits action a\u02c6 that the agent executes in the world.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7357-visual-memory-for-robust-path-following",
            "Publication name": "Visual Memory for Robust Path Following",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ashish Kumar, Saurabh Gupta, David Fouhey, Sergey Levine, Jitendra Malik",
            "Code link": "https://ashishkumar1993.github.io/rpf/. \n",
            "Data": "Stanford Building Parser Dataset, Matterport 3D Dataset, SUNCG synthetic indoor environments \n\n\n"
        },
        {
            "id": "KD",
            "Using for": "Distillation",
            "Based on": "NaN",
            "Method": "Knowledge Distillation",
            "Description": "Knowledge distillation  aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may be resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. KD consists of a classifier and a teacher . To operate for resource- constrained inference, the classifier does not use privileged provision. On the other hand, the teacher uses privileged provision by, e.g., having a larger model capacity or taking more features as input. Once trained, the teacher outputs a distribution over labels called soft labels for each training instance.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1503.02531",
            "Publication name": "Distilling the Knowledge in a Neural Network\u2028\n",
            "Released at": "ICML",
            "Interacts with": "DNNs",
            "Authors": "Geoffrey Hinton, Oriol Vinyals, Jeff Dean",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "KDGAN",
            "Using for": "Distillation, Adversarial Networks",
            "Based on": "KD, GAN",
            "Method": "Knowledge Distillation with Generative Adversarial Networks",
            "Description": "Is a framework to distill knowledge with generative adversarial networks for multi-label learning with privileged provision. This method consisting of a classifier, a teacher, and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. KDGAN consists of a classifier, a teacher, and a discriminator. In addition to the distillation loss in KD and the adversarial losses in naive GAN, authors define a distillation loss from the classifier to the teacher and an adversarial loss between the teacher and the discriminator. Specifically, the classifier and the teacher, serving as generators, aim to fool the discriminator by generating pseudo labels that resemble the true labels. Meanwhile, the classifier and the teacher try to reach an agreement on what pseudo labels to generate by distilling their knowledge into each other. By formulating the distillation and adversarial losses as a minimax game, we enable the classifier to learn the true data distribution at the equilibrium.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7358-kdgan-knowledge-distillation-with-generative-adversarial-networks",
            "Publication name": "KDGAN: Knowledge Distillation with Generative Adversarial Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Xiaojie Wang, Rui Zhang, Yu Sun, Jianzhong Qi",
            "Code link": "https://github.com/xiaojiew1/KDGAN/ \u2028\n \u2028\n",
            "Data": "YFCC100M, MNIST, Cifar-10,\n"
        },
        {
            "id": "SPIKN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Spiking Neural Network",
            "Description": "Spiking Neural Networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by hardware implementations that have demonstrated significant energy reductions as compared to conventional Artificial Neural Networks (ANNs). SNNs have been traditionally studied in the field of theoretical neuroscience through the lens of biological plausibility.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Spiking_neural_network",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RSNNs",
            "Using for": "NaN",
            "Based on": "RNN, SPIKN",
            "Method": "Recurrent networks of spiking neurons \n",
            "Description": "Recurrent networks of spiking neurons (RSNNs) underlie the astounding comput- ing and learning capabilities of the brain",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1601.07620",
            "Publication name": "Using firing-rate dynamics to train recurrent networks of spiking model neurons. ",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Brian DePasquale, Mark M Churchland, and LF Abbott.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LSNNs",
            "Using for": "Speech Recognition, Machine Translation, Language Modeling, Question Answering",
            "Based on": "RSNNs, LSTM",
            "Method": "Long short-term memory Spiking Neural Networks",
            "Description": "This method acquire abstract knowledge from prior learning in a Learning-to-Learn  scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. Neurons and synapses in common RSNN models are missing many of the dynamic processes found in their biological counterparts, especially those on larger time scales. Authors integrate one of them into RSNN model: neuronal adaptation. It is well known that a substantial fraction of excitatory neurons in the brain are adapting, with diverse time constants, see e.g. the Allen Brain Atlas for data from the neocortex of mouse and humans. LSNNs consist of a population R of integrate-and-fire (LIF) neurons (excitatory and inhibitory), and a second population A of LIF excitatory neurons whose excitability is temporarily reduced through preceding firing activity, i.e., these neurons are adapting. Both populations R and A receive spike trains from a population X of external input neurons.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7359-long-short-term-memory-and-learning-to-learn-in-networks-of-spiking-neurons",
            "Publication name": "Long short-term memory and Learning-to-learn in networks of spiking neurons",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, Wolfgang Maass",
            "Code link": "None",
            "Data": "Sequential MNIST, TIMIT \n\n"
        },
        {
            "id": "DSDH",
            "Using for": "Hashing, Supervised Learning",
            "Based on": "KNN",
            "Method": "Deep Supervised Discrete Hashing \n",
            "Description": "With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefit from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1705.10999",
            "Publication name": "Deep supervised discrete hashing \u2028\n",
            "Released at": "NIPS",
            "Interacts with": "KNN",
            "Authors": "Q. Li, Z. Sun, R. He, and T. Tan \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DTSH",
            "Using for": "Hashing, Supervised Learning",
            "Based on": "KNN",
            "Method": "Deep Triplet Supervised Hashing",
            "Description": "Hashing is one of the most popular and powerful approximate nearest neighbor search techniques for large-scale image retrieval. Most traditional hashing methods first represent images as off-the-shelf visual features and then produce hashing codes in a separate stage. However, off-the-shelf visual features may not be optimally compatible with the hash code learning procedure, which may result in sub-optimal hash codes. Recently, deep hashing methods have been proposed to simultaneously learn image features and hash codes using deep neural networks and have shown superior performance over traditional hashing methods. Most deep hashing methods are given supervised information in the form of pairwise labels or triplet labels. The current state-of-the-art deep hashing method DPSH~\\cite{li2015feature}, which is based on pairwise labels, performs image feature learning and hash code learning simultaneously by maximizing the likelihood of pairwise similarities. Inspired by DPSH~\\cite{li2015feature}, we propose a triplet label based deep hashing method which aims to maximize the likelihood of the given triplet labels. Experimental results show that our method outperforms all the baselines on CIFAR-10 and NUS-WIDE datasets, including the state-of-the-art method DPSH~\\cite{li2015feature} and all the previous triplet label based deep hashing methods",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1612.03900",
            "Publication name": "Deep supervised hashing with triplet labels. \u2028\n",
            "Released at": "Asian Conference on \u2028Computer Vision \u2028\n",
            "Interacts with": "KNN",
            "Authors": "X. Wang, Y. Shi, and K. M. Kitani. \u2028\n",
            "Code link": "NaN",
            "Data": "Cifar-10,MNIST"
        },
        {
            "id": "GREH",
            "Using for": "Hashing",
            "Based on": "DSDH, DTSH \n",
            "Method": "Greedy Hash",
            "Description": "A major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this paper authors propose to adopt greedy algorithm to tackle the discrete hashing optimization, and design a new neural layer to implement this approach, which fixedly uses the sign function in forward propagation without any relaxation to avoid the quantization error, while in backward propagation the gradients are transmitted intactly to the front layer, preventing the vanishing gradients and helping to achieve fast convergence.  A hash coding layer is designed to implement approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. The superiority of this method is proved from a novel visual perspective as well as abundant experiments on different retrieval tasks.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7360-greedy-hash-towards-fast-optimization-for-accurate-hash-coding-in-cnn",
            "Publication name": "Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN",
            "Released at": "NIPS",
            "Interacts with": "CNNs",
            "Authors": "Shupeng Su, Chao Zhang, Kai Han, Yonghong Tian",
            "Code link": "https://github.com/ssppp/GreedyHash. \n",
            "Data": "Cifar-10, ImageNet,"
        },
        {
            "id": "Rel-MMD",
            "Using for": "Model Comparison ",
            "Based on": "NaN",
            "Method": "A Test of Relative Similarity For Model Selection in Generative Models",
            "Description": "Probabilistic generative models provide a powerful framework for representing data that avoids the expense of manual annotation typically needed by discriminative approaches. Model selection in this generative setting can be challenging, however, particularly when likelihoods are not easily accessible. To address this issue, we introduce a statistical test of relative similarity, which is used to determine which of two models generates samples that are significantly closer to a real-world reference dataset of interest. We use as our test statistic the difference in maximum mean discrepancies (MMDs) between the reference dataset and each model dataset, and derive a powerful, low-variance test based on the joint asymptotic distribution of the MMDs between each reference-model pair. In experiments on deep generative models, including the variational auto-encoder and generative moment matching network, the tests provide a meaningful ranking of model performance as a function of parameter and training settings.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1511.04581",
            "Publication name": "A test of relative similarity for model selection in generative models \n",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "Wacha Bounliphone, Eugene Belilovsky, Matthew B. Blaschko, Ioannis Antonoglou, Arthur Gretton",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "IDFMTP",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Interpretable distribution features with maximum testing power.",
            "Description": "Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distin- guishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. We show that the empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high- dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.",
            "Publication date": 2016.0,
            "Publication link": "https://papers.nips.cc/paper/6148-interpretable-distribution-features-with-maximum-testing-power.pdf",
            "Publication name": "Interpretable distribution features with maximum testing power. \n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "W. Jitkrittum, Z. Szab\u00f3, K. P. Chwialkowski, and A. Gretton. \n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LTKG",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Linear-time kernel goodness-of-fit",
            "Description": "A adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1705.07673",
            "Publication name": "A linear-time kernel goodness-of-fit test \n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "W. Jitkrittum, W. Xu, Z. Szabo, K. Fukumizu, and A. Gretton. \n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "INFF",
            "Using for": "Model Comparison \n",
            "Based on": "Rel-MMD, IDFMTP, LTKG",
            "Method": "informative features \n",
            "Description": "This method indicating the regions in the data domain where one model fits significantly better than the other.  In this work, authors propose two new linear-time tests for relative goodness-of-fit. In the first test, the two models P, Q are represented by their two respective samples Xn and Yn, and the test generalises, In the second, the test has access to the probability density functions p, q of the two respective candidate models P, Q (which need only be known up to normalisation)\nIn a real-world problem of comparing GAN models, the test power of new test matches that of the state-of-the-art test of relative goodness of fit, while being one order of magnitude faster.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7361-informative-features-for-model-comparison",
            "Publication name": "Informative Features for Model Comparison",
            "Released at": "NIPS",
            "Interacts with": "GANs",
            "Authors": "Wittawat Jitkrittum, Heishiro Kanagawa, Patsorn Sangkloy, James Hays, Bernhard Sch\u00f6lkopf, Arthur Gretton",
            "Code link": "https://github.com/wittawatj/kernel-mod. \n",
            "Data": "CIFAR-10 \n"
        },
        {
            "id": "X-CONV",
            "Using for": "Classification, Segmentation, Learning from point clouds \n",
            "Based on": "CNN, MLP",
            "Method": "X-Convolution",
            "Description": "Is capable of taking the point shapes into consideration, while being invariant to ordering. In practice, we find that the learned X -transformations are far from ideal, especially in terms of the permutation equivalence aspect.  X -Conv operates in local regions. Since the output features are supposed to be associated with the representative points, X-Conv takes their neighborhood points, as well as the associated features, as input to convolve with. it is the basic building block for PointCNN.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points",
            "Publication name": "PointCNN: Convolution On X-Transformed Points",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen",
            "Code link": "https://github.com/yangyanli/PointCNN \n",
            "Data": "Net40, ScanNet, TU-Berlin, Quick Draw, MNIST, CIFAR-10, ShapeNet Parts, S3DIS, and ScanNet."
        },
        {
            "id": "POINTCNN",
            "Using for": "Classification, Segmentation, Learning from point clouds \n",
            "Based on": "X-CONV, MLP",
            "Method": "Convolution On X-Transformed Points",
            "Description": "A simple and general framework for feature learning from point clouds. Authors propose to learn an X -transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order.  PointCNN shares the same design and generalizes it to point clouds. First, we introduce hierarchical convolutions in PointCNN, in analogy to that of image CNNs, then, we explain the core X -Conv operator in detail, and finally, present PointCNN architectures geared toward various tasks. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7362-pointcnn-convolution-on-x-transformed-points",
            "Publication name": "PointCNN: Convolution On X-Transformed Points",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen",
            "Code link": "https://github.com/yangyanli/PointCNN \n",
            "Data": "Net40, ScanNet, TU-Berlin, Quick Draw, MNIST, CIFAR-10, ShapeNet Parts, S3DIS, and ScanNet. \n\n"
        },
        {
            "id": "INCVCV",
            "Using for": "Classification",
            "Based on": "CNN",
            "Method": "inception computer vision",
            "Description": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1512.00567",
            "Publication name": "Rethinking the inception architecture for computer vision",
            "Released at": "CVPR",
            "Interacts with": "NaN",
            "Authors": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. \u2028",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ENCTC",
            "Using for": "Connectionist Temporal Classification",
            "Based on": "RNN, INCVCV",
            "Method": "Entropy based regularization for Connectionist Temporal Classification \n",
            "Description": "Authors propose a maximum conditional entropy regularization for CTC, which encourages exploration for CTC training and prevents peaky output distributions. Authors derive from equal spacing prior a pruning algorithm (EsCTC) to effectively limit the size of CTC feasible set and give theoretical explanations from the perspective of maximum entropy, and provide polynomial-time dynamic programming algorithms for calculating EnCTC, EsCTC and their combination (EnEsCTC). Authors validate the proposed methods on scene text recognition tasks and show that these methods are able to improve the baseline model without changing training settings. Proposed method achieves comparable performance to the state-of-the-art methods with a much simpler architecture.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7363-connectionist-temporal-classification-with-maximum-entropy-regularization",
            "Publication name": "Connectionist Temporal Classification with Maximum Entropy Regularization\n",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Hu Liu, Sheng Jin, Changshui Zhang",
            "Code link": "https://github.com/liuhu-bigeye/enctc.crnn. \n",
            "Data": "Synth90K, ICDAR-2003, IIIT5k-word, Street View Text \n \n"
        },
        {
            "id": "LMDN",
            "Using for": "Classification",
            "Based on": "None",
            "Method": "Large Margin Deep Networks \n",
            "Description": "Authors have presented a new loss function inspired by the theory of large margin that is amenable to deep network training. This new loss is flexible and can establish a large margin that can be defined on input, hidden or output layers, and using l\u221e, l1, and l2 distance definitions. Authors demonstrate that the decision boundary obtained by loss has nice properties compared to standard classification loss functions. Proposed method is computationally practical: for Imagenet, training was about 1.6 times more expensive than cross-entropy. Finally, empirical results show the benefit of margin at the hidden layers of a network.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7364-large-margin-deep-networks-for-classification",
            "Publication name": "Large Margin Deep Networks for Classification",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Gamaleldin Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, Samy Bengio",
            "Code link": "https://github.com/google-research/ google-research/tree/master/large_margin \n",
            "Data": "MNIST, CIFAR-10, ImageNet \n"
        },
        {
            "id": "GGM",
            "Using for": "Quadratic Assignment Problem, Graph matching \n\n",
            "Based on": "None",
            "Method": "Generalized Graph Matching\n\n",
            "Description": "Graph matching has received persistent attention over several decades, which can be formulated as a quadratic assignment problem (QAP). Authors show that a large family of functions, defined as Separable Functions, can asymptotically approximate the discrete matching problem by varying the approximation controlling parameters. With this function family, there exist infinite modelings of graph matching problem, thereby providing the feasibility of adapting different practical problems with different models. This provides a new perspective of considering graph matching. Authors also study the properties of global optimality and devise convex/concave-preserving extensions to the widely used Lawler\u2019s QAP form. Based on the theoretical anslysis, authors propose a novel solver, which achieves remarkable performance in both synthetic and real-world image tests. This gives rise to the possibility of solving graph matching with many alternative approximations with different solution paths.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7365-generalizing-graph-matching-beyond-quadratic-assignment-model",
            "Publication name": "Generalizing Graph Matching beyond Quadratic Assignment Model",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Tianshu Yu, Junchi Yan, Yilin Wang, Wei Liu, baoxin Li",
            "Code link": "None",
            "Data": "Image Matching, Graph Matching \n"
        },
        {
            "id": "SGEGT",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Sequential Games Excessive Gap Technique",
            "Description": "NaN",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7366-solving-large-sequential-games-with-the-excessive-gap-technique",
            "Publication name": "Solving Large Sequential Games with the Excessive Gap Technique",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Christian Kroer, Gabriele Farina, Tuomas Sandholm",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DAL",
            "Using for": "Classification",
            "Based on": "BN, RELU,",
            "Method": "Discrimination-aware loss",
            "Description": "losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Each loss uses the output of layer as the input feature maps. To make the computation of the loss feasible, we impose an average pooling operation over the feature maps.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7367-discrimination-aware-channel-pruning-for-deep-neural-networks",
            "Publication name": "Discrimination-aware Channel Pruning for Deep Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "DCP",
            "Authors": "Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, Jinhui Zhu",
            "Code link": "https://github.com/SCUT-AILab/Discrimination-aware-Channel-Pruning-for-Deep-Neural-Networks",
            "Data": "CIFAR-10 \n"
        },
        {
            "id": "DCP",
            "Using for": "Classification",
            "Based on": "CNN, DAL",
            "Method": "Discrimination-aware channel pruning \n",
            "Description": "A simple-yet-effective method called discrimination-aware channel pruning  to choose those channels that really contribute to discriminative power. By introducing P DAL to intermediate layers, starting from a pre-trained model, DCP updates the model and performs channel pruning with (P + 1) stages. Algorithm  is called discrimination-aware in the sense that an additional loss and the final loss are considered to fine-tune the model.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7367-discrimination-aware-channel-pruning-for-deep-neural-networks",
            "Publication name": "Discrimination-aware Channel Pruning for Deep Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "VGG, RESN",
            "Authors": "Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, Jinhui Zhu",
            "Code link": "https://github.com/SCUT-AILab/Discrimination-aware-Channel-Pruning-for-Deep-Neural-Networks",
            "Data": "CIFAR-10 \n"
        },
        {
            "id": "RCL",
            "Using for": "Continual Learning, Classification",
            "Based on": "RNN",
            "Method": "Reinforced Continual Learning \n",
            "Description": "In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. Faced with a new task, deciding optimal number of nodes/filters to add for each layer is posed as a combinatorial optimization problem. Authors provide a sophisticatedly designed reinforcement learning method to solve this problem. In RCL, a controller implemented as a recurrent neural network is adopted to determine the best architectural hyper-parameters of neural networks for each task. Controller trains by an actor-critic strategy guided by a reward signal deriving from both validation accuracy and network complexity. This can maintain the prediction accuracy on older tasks as much as possible while reducing the overall model complexity. Is the first attempt that employs the reinforcement learning for solving the continual learning problems.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7369-reinforced-continual-learning",
            "Publication name": "Reinforced Continual Learning",
            "Released at": "NIPS",
            "Interacts with": "CNN, MLP",
            "Authors": "Ju Xu, Zhanxing Zhu",
            "Code link": "https://github.com/xujinfan/Reinforced-Continual-Learning",
            "Data": "MNIST, CIFAR-100 \n"
        },
        {
            "id": "VARINF",
            "Using for": "Inference",
            "Based on": "None",
            "Method": "Variational Inference",
            "Description": "NaN",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1601.00670",
            "Publication name": "Variational Inference: A Review for Statisticians.",
            "Released at": "Journal of the American Statistical Association,",
            "Interacts with": "None",
            "Authors": "David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "UA",
            "Using for": "Classification, Health\n",
            "Based on": "ATTN, RNN, VARINF",
            "Method": "Uncertainty-aware Attention \n",
            "Description": "Authors propose a novel variational attention model with instance-dependent modeling of variance, that captures input-level uncertainty and use it to attenuate attention strengths. and show that our uncertainty-aware attention yields accurate calibration of model uncertainty as well as attentions that aligns well with human interpretations. Authors validate model on six real-world risk prediction problems in healthcare domains, for both the original binary classification task and classification with \u201cI don\u2019t know\" decision, and show that our model obtains significant improvements over existing attention models.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7370-uncertainty-aware-attention-for-reliable-interpretation-and-prediction",
            "Publication name": "Uncertainty-Aware Attention for Reliable Interpretation and Prediction",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Jay Heo, Hae Beom Lee, Saehoon Kim, Juho Lee, Kwang Joon Kim, Eunho Yang, Sung Ju Hwang",
            "Code link": "https://github.com/jayheo/UA. \n",
            "Data": "PhysioNet, Pancreatic Cancer, MIMIC-Sepsis \n"
        },
        {
            "id": "DropMax",
            "Using for": "Classification, Activation, Regularization",
            "Based on": "DRPT, SMAX, VARINF",
            "Method": "Adaptive Variational Softmax",
            "Description": "n this paper, we propose a novel variant of softmax classifier that achieves improved accuracy over the regular softmax function by leveraging the popular dropout regularization. DropMax is  a stochastic version of softmax classifier which at each iteration drops non-target classes according to dropout probabilities adaptively decided for each instance. Overlay binary masking variables over class output probabilities, which are input-adaptively learned via variational inference. Variational inference framework adaptively learn the dropout probability of non-target classes for each input, and DropMax, randomly drops non-target classes when computing the class probability for each input instance. This stochastic regularization has an effect of building an ensemble classifier out of exponentially many classifiers with different decision boundaries.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7371-dropmax-adaptive-variational-softmax",
            "Publication name": "DropMax: Adaptive Variational Softmax",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Hae Beom Lee, Juho Lee, Saehoon Kim, Eunho Yang, Sung Ju Hwang",
            "Code link": "https://github.com/haebeom-lee/dropmax",
            "Data": "AWA, CUB dataset \n"
        },
        {
            "id": "BAYESM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Bayesian methods",
            "Description": "Methods based on Bayesian inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Bayesian_inference",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "None",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SS-DL",
            "Using for": "Activation, Regularization",
            "Based on": "RELU, BAYESM",
            "Method": "Spike-and-Slab Deep Learning \n",
            "Description": "Fully Bayesian alternative to dropout for improving generalizability of deep ReLU networks. This new type of regularization enables provable recovery of smooth input-output maps with unknown levels of smoothness. Bayesian deep ReLU networks can be near-minimax tuning-free. In other words, they can adapt to unknown smoothness, giving rise posteriors that concentrate around smooth surfaces at near-minimax rates. Results provide new theoretical justifications for deep ReLU networks from a Bayesian point of view.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7372-posterior-concentration-for-sparse-deep-learning",
            "Publication name": "Posterior Concentration for Sparse Deep Learning",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Veronika Rockova, nicholas polson",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "VLS",
            "Using for": "Detection",
            "Based on": "None",
            "Method": "Varying levels of supervision \n",
            "Description": "Is based on discriminative clustering and integrates different types of supervision as constraints on the optimization. Authors investigate applications of such a model to training setups with alternative supervisory signals ranging from video-level class labels to the full per-frame annotation of action bounding boxes. A unifying framework that can handle and combine varying types of less-demanding weak supervision. The key observations are that  dense spatial annotation is not always needed due to the recent advances of human detection and tracking,  the performance of \u2018temporal point\u2019 supervision indicates that only annotating an action with a \u2018click\u2019 is very promising to decrease the annotation cost at a moderate performance drop, and  mixing levels of supervision is a powerful approach for reducing annotation efforts.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7373-a-flexible-model-for-training-action-localization-with-varying-levels-of-supervision",
            "Publication name": "A flexible model for training action localization with varying levels of supervision",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Guilhem Ch\u00e9ron, Jean-Baptiste Alayrac, Ivan Laptev, Cordelia Schmid",
            "Code link": "https://www.di.ens.fr/willow/research/weakactionloc/. \n",
            "Data": "UCF101, DALY \n"
        },
        {
            "id": "BPR+",
            "Using for": "NaN",
            "Based on": "BAYESM",
            "Method": "Bayesian Policy Reuse",
            "Description": "A long-lived autonomous agent should be able to respond online to novel instances of tasks from a familiar domain. Acting online requires 'fast' responses, in terms of rapid convergence, especially when the task instance has a short duration, such as in applications involving interactions with humans. These requirements can be problematic for many established methods for learning to act. In domains where the agent knows that the task instance is drawn from a family of related tasks, albeit without access to the label of any given instance, it can choose to act through a process of policy reuse from a library, rather than policy learning from scratch. In policy reuse, the agent has prior knowledge of the class of tasks in the form of a library of policies that were learnt from sample task instances during an offline training phase. We formalise the problem of policy reuse, and present an algorithm for efficiently responding to a novel task instance by reusing a policy from the library of existing policies, where the choice is based on observed 'signals' which correlate to policy performance. We achieve this by posing the problem as a Bayesian choice problem with a corresponding notion of an optimal response, but the computation of that response is in many cases intractable. Therefore, to reduce the computation cost of the posterior, we follow a Bayesian optimisation approach and define a set of policy selection functions, which balance exploration in the policy library against exploitation of previously tried policies, together with a model of expected performance of the policy library on their corresponding task instances. We validate our method in several simulated domains of interactive, short-duration episodic tasks, showing rapid convergence in unknown task variations.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1505.00284",
            "Publication name": "Bayesian Policy Reuse",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Benjamin Rosman, Majd Hawasly, Subramanian Ramamoorthy",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DBPR+",
            "Using for": "Multiagent scenarios \n",
            "Based on": "BPR+, DNN,",
            "Method": "Deep Bayesian Policy Reuse+\n",
            "Description": "New deep BPR+ algorithm by extending the recent BPR+ algorithm with a neural network as the value-function approximator. To detect policy accurately, we propose the rectified belief model taking advantage of the opponent model to infer the other agent\u2019s policy from reward signals and its behaviors. Deep BPR+ inherits all the advantages of BPR+ and empirically shows better performance in terms of detection accuracy, cumulative rewards and speed of convergence compared to existing algorithms in complex Markov games with raw visual inputs. Deep BPR+ inherits the ability of online learning against a new opponent. However vanilla BPR+\nlearns from scratch each time when encountering a new opponent, which can be extremely inefficient. There may exist certain similarities among opponent\u2019s different policies, as well as their associated response policies. Thus it is reasonable to reuse the learned response policy as a starting policy for the subsequent policy optimization when its associate opponent model shares high similarity to the\nnew opponent model. Empirical evaluations demonstrate that the deep BPR+ algorithm indeed achieves promising performance than other existing algorithms on three complex Markov games. As a future work, we would like to investigate how to act optimally in face of the adaptive agents whose behaviors are continuously changing over time.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7374-a-deep-bayesian-policy-reuse-approach-against-non-stationary-agents",
            "Publication name": "A Deep Bayesian Policy Reuse Approach Against Non-Stationary Agents",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "YAN ZHENG, Zhaopeng Meng, Jianye Hao, Zongzhang Zhang, Tianpei Yang, Changjie Fan",
            "Code link": "None",
            "Data": "Markov Games"
        },
        {
            "id": "Eps-LDP",
            "Using for": "Empirical Risk Minimization \n",
            "Based on": "NaN",
            "Method": "Eps-LDP ",
            "Description": "Empirical Risk Minimization problem in the non-interactive local model of differential privacy.  Eps-LDP is the polynomial approximation techniques.  Non-interactive Eps-LDP algorithms answering the class of k-way marginals queries and the class of smooth queries, by using different type of polynomials approximations ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7375-empirical-risk-minimization-in-non-interactive-local-differential-privacy-revisited",
            "Publication name": "Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Di Wang, Marco Gaboardi, Jinhui Xu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ERMNLD",
            "Using for": "Empirical Risk Minimization \n",
            "Based on": "Eps-LDP",
            "Method": "Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited",
            "Description": "Empirical Risk Minimization problem in the non-interactive local model of differential privacy. In the case of constant or low dimensions (p\u226an), we first show that if the loss function is (\u221e,T)-smooth, we can avoid a dependence of the sample complexity, to achieve error \u03b1, on the exponential of the dimensionality p with base 1/\u03b1 ({\\em i.e.,} \u03b1\u2212p). Our approach is based on polynomial approximation. Then, we propose player-efficient algorithms with 1-bit communication complexity and O(1) computation cost for each player. The error bound is asymptotically the same as the original one. With some additional assumptions, we also give an efficient algorithm for the server.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7375-empirical-risk-minimization-in-non-interactive-local-differential-privacy-revisited",
            "Publication name": "Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Di Wang, Marco Gaboardi, Jinhui Xu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MITIT",
            "Using for": "Image-To-Image Translation",
            "Based on": "NaN",
            "Method": "Multimodal image-to-image translation.",
            "Description": "Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a distribution of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1711.11586",
            "Publication name": "Toward multimodal image-to-image translation. \u2028\n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NBS",
            "Using for": "Base-novel class pairs extracting\u2028\n",
            "Based on": "PROTONET",
            "Method": "Neighborhood Batch Sampling",
            "Description": "Extract all related base-novel class pairs, Therefore, to define which base classes are translatable to a novel class, method rank them by their distance in a semantic space. For simplicity, authors formulate approach on top of Prototypical Networks, learned by a nearest neighbor classifier on the semantic space measured by the Euclidean distance.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks",
            "Publication name": "Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks",
            "Released at": "NIPS",
            "Interacts with": "CP-AAN",
            "Authors": "Hang Gao, Zheng Shou, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang",
            "Code link": "None",
            "Data": "ImageNet classification dataset"
        },
        {
            "id": "CP-AAN",
            "Using for": "Few-shot learning, Image-To-Image Translation\n",
            "Based on": "GAN, MITIT, NBS",
            "Method": "Covariance-Preserving Adversarial Augmentation Networks \n",
            "Description": "In this work, proposed Covariance-Preserving Adversarial Augmentation Networks to overcome existing limits of low-shot learning. This approaches are novel in two aspects: modeling and training strategy. Authors propose a new class of Generative Adversarial Networks for feature augmentation and formulate feature augmentation problem as an imbalanced set-to-set translation problem where the conditional distribution of examples of each novel class can be conceptually expressed as a mixture of related base classes. Model first extract all related base-novel class pairs by an intuitive yet effective approach called Neighborhood Batch Sampling. Then, model aims to learn the latent distribution of each novel class given its base counterparts. Empirical results show that method can generate realistic yet diverse examples, leading to substantial improvements on the ImageNet benchmark over the state of the art.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7376-low-shot-learning-via-covariance-preserving-adversarial-augmentation-networks",
            "Publication name": "Low-shot Learning via Covariance-Preserving Adversarial Augmentation Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Hang Gao, Zheng Shou, Alireza Zareian, Hanwang Zhang, Shih-Fu Chang",
            "Code link": "None",
            "Data": "ImageNet classification dataset \n"
        },
        {
            "id": "WASSD",
            "Using for": "Metric",
            "Based on": "NaN",
            "Method": "Wasserstein distance",
            "Description": "Distance function defined between probability distributions on a given metric space. Intuitively, if each distribution is viewed as a unit amount of \"dirt\" piled on \nmetric space, the metric is the minimum \"cost\" of turning one pile into the other, which is assumed to be the amount of dirt that needs to be moved times the mean distance it has to be moved. Because of this analogy, the metric is known in computer science as the earth mover's distance.",
            "Publication date": 2015.0,
            "Publication link": "http://proceedings.mlr.press/v37/kusnerb15.pdf",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "WMD",
            "Using for": "Metric",
            "Based on": "WASSD",
            "Method": "Word Mover\u2019s Distance \n",
            "Description": "WMD is a method that allows us to assess the \"distance\" between two documents in a meaningful way, even when they have no words in common.",
            "Publication date": "NaN",
            "Publication link": "https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html",
            "Publication name": "From Word Embeedings To Document Distance.",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Kusner Matt J., Sun Yu, Kolkin Nicholas I., Weinberger Kilian Q.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VSF",
            "Using for": "Dissimilarity \n",
            "Based on": "WMD, VAE",
            "Method": "Variational siamese framework \n",
            "Description": "This is model which hidden representations consist of two Gaussian distributions instead of two vectors.  This allows model to smoothly measure semantic similarity with different distance metrics or divergences from Information Theory. To learn and measure semantic similarity with variational siamese network, authors express the previous metrics \"element-wise\" and feed the resulting tensor as input to a Multi-Layer Perceptron \u03c8 that predicts the degree of similarity of the corresponding pair. The Wasserstein distance (scalar) is obtained by summing the components of this tensor. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7377-learning-semantic-similarity-in-a-continuous-space",
            "Publication name": "Learning semantic similarity in a continuous space",
            "Released at": "NIPS",
            "Interacts with": "MLP",
            "Authors": " Michel Deudon\n",
            "Code link": "https://github.com/MichelDeudon/variational-siamese-network \n",
            "Data": "Quora question pairs dataset"
        },
        {
            "id": "LRR",
            "Using for": "Dissimilarity \n",
            "Based on": "None\n",
            "Method": "Learning to repeat, reformulate \n",
            "Description": "Intuition is that behind semantically equivalent sentences, the intent is identical. Authors consider that two duplicate questions or paraphrases were generated from a same latent intent (hidden variable). Authors thus take the VAE approach to learn and infer semantic representation of sentences, but instead of only recovering sentence s from s (repeat), they also let model learn to recover semantically equivalent sentences s\u2019 from s (reformulate). This first task (repeat, reformulate) provides a sound (meaningful) initialization for encoder\u2019s parameters. Then model learn semantic similarity discriminatively as an optimal transport metric in the intent\u2019s latent space.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7377-learning-semantic-similarity-in-a-continuous-space",
            "Publication name": "Learning semantic similarity in a continuous space",
            "Released at": "NIPS",
            "Interacts with": "VSF",
            "Authors": " Michel Deudon\n",
            "Code link": "https://github.com/MichelDeudon/variational-siamese-network \n",
            "Data": "Quora question pairs dataset"
        },
        {
            "id": "VSFRR",
            "Using for": "Dissimilarity \n",
            "Based on": "VSF, LRR\n",
            "Method": "Variational siamese framework repeat/reformulate",
            "Description": "In this work, authors replace the discrete combinatorial problem of Word Mover\u2019s Distance (Wasserstein 1) by a continuous one between normal distributions (intents). Firstly model learn to repeat, reformulate with a deep generative model (variational auto encoder) to learn and infer intents. Then model learn and measure semantic similarity between question pairs with novel variational siamese framework(VSF). This approach performs strongly on Quora question pairs dataset and experiments show its effectiveness for question retrieval in knowledge databases with a million entries.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7377-learning-semantic-similarity-in-a-continuous-space",
            "Publication name": "Learning semantic similarity in a continuous space",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": " Michel Deudon\n",
            "Code link": "https://github.com/MichelDeudon/variational-siamese-network \n",
            "Data": "Quora question pairs dataset"
        },
        {
            "id": "METAREG",
            "Using for": "Image Recognition, Regularization, Domain Adaptation, Meta-Learning",
            "Based on": "None",
            "Method": "Meta-Regularization",
            "Description": "Training models that generalize to new domains at test time is a problem of fundamental importance in machine learning. In this work, author encode notion of domain generalization using a novel regularization function. They pose the problem of finding such a regularization function in a Learning to Learn (or) meta- learning framework. Authors show how the notion of domain generalization can be explicitly encoded in a regularization function, which can then be used to train models that are more robust to domain shifts. This framework is also scalable as the same regularizer can later be used to fine-tune on a larger dataset. Experimental validations on computer vision and natural language datasets indicate that method can learn regularizers that achieve good cross-domain generalization.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7378-metareg-towards-domain-generalization-using-meta-regularization",
            "Publication name": "MetaReg: Towards Domain Generalization using Meta-Regularization",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Yogesh Balaji, Swami Sankaranarayanan, Rama Chellappa",
            "Code link": "None",
            "Data": "PACS dataset \n"
        },
        {
            "id": "SURF",
            "Using for": "Regression",
            "Based on": "None",
            "Method": "Fast Stagewise Unit-Rank Tensor Factorization \n",
            "Description": "Authors propose sparse and low-rank tensor regression model to relate a univariate outcome to a feature tensor, in which each unit-rank tensor from the CP decomposition of the coefficient tensor is assumed to be sparse. This structure is both parsimonious and highly interpretable, as it implies that the outcome is related to the features through a few distinct pathways, each of which may only involve subsets of feature dimensions.  In this case, if we want to predict a response variable for each tensor, a naive approach is to perform linear regression on the vectorized data (e.g., by stretching the tensor element by element). However, it completely ignores the multidimensional structure of the tensor data, such as the spatial coherence of the voxels. Authors take a divide-and-conquer strategy to simplify the task into a set of sparse unit-rank tensor regression problems. To make the computation efficient and scalable, for the unit-rank tensor regression, authors propose a stagewise estimation procedure to efficiently trace out its entire solution path. Also they show that as the step size goes to zero, the stagewise solution paths converge exactly to those of the corresponding regularized regression. The superior performance of our approach is demonstrated on various real-world and synthetic examples.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7379-boosted-sparse-and-low-rank-tensor-regression",
            "Publication name": "Boosted Sparse and Low-Rank Tensor Regression",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Lifang He, Kun Chen, Wanwan Xu, Jiayu Zhou, Fei Wang",
            "Code link": "https://github.com/LifangHe/SURF. \n",
            "Data": "PPMI database \n"
        },
        {
            "id": "SCR",
            "Using for": "Zero-shot Learning, Recognition",
            "Based on": "NaN",
            "Method": "Semantically consistent regularization",
            "Description": "The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1704.03039",
            "Publication name": "Semantically consistent regularization for zero-shot recognition \u2028\n",
            "Released at": "ICPR",
            "Interacts with": "NaN",
            "Authors": "P.Morgado,N.Vasconcelos. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "USE",
            "Using for": "Embedding",
            "Based on": "NaN",
            "Method": "Unified semantic embedding \u2028\n",
            "Description": "We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercat- egories and attributes. Contrary to prior work, which only utilized them as side in- formation, we explicitly embed these semantic entities into the same space where we embed categories, which enables us to represent a category as their linear com- bination. By exploiting such a unified model for semantics, we enforce each cate- gory to be generated as a supercategory + a sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a model with better generalization. This model also generates compact se- mantic description of each category, which enhances interoperability and enables humans to analyze what has been learned.",
            "Publication date": 2014.0,
            "Publication link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.905.8295&rep=rep1&type=pdf",
            "Publication name": "A unified semantic embedding: Relating taxonomies and attributes. \u2028\n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "S. J. Hwang and L. Sigal. \u2028\n",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DIPL",
            "Using for": "Zero-shot Learning",
            "Based on": "USE, SCR",
            "Method": "Domain-invariant projection learning \n",
            "Description": "NaN",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7380-domain-invariant-projection-learning-for-zero-shot-recognition",
            "Publication name": "Domain-Invariant Projection Learning for Zero-Shot Recognition",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "An Zhao, Mingyu Ding, Jiechao Guan, Zhiwu Lu, Tao Xiang, Ji-Rong Wen",
            "Code link": "None",
            "Data": "Animals with Attributes, CUB-200-2011, aPascal&Yahoo,ILSVRC-2012 dateset \n\n"
        },
        {
            "id": "NS-VQA",
            "Using for": "Question-Answering, Visual Question Answering, Language Modelling",
            "Based on": "R-CNN, RESN, S2S, BILSTM,LSTM",
            "Method": "Neural-symbolic visual question answering \n",
            "Description": "This model has three components: a scene parser (de-renderer), a question parser (program generator), and a program executor. Given an image-question pair, the scene parser de-renders the image to obtain a structural scene representation, the question parser generates a hierarchical program from the question, and the executor runs the program on the structural representation to obtain an answer.  Scene parser recovers a structural and disentangled representation of the scene in the image, based on which we can perform fully interpretable symbolic reasoning. The parser takes a two-step, segment-based approach for de-rendering: it first generates a number of segment proposals, and for each segment, classifies the object and its attributes. The program has a hierarchy of functional modules, each fulfilling an independent operation on the scene representation. Using a hierarchical program, reasoning backbone naturally supplies compositionality and generalization power. The program executor takes the output sequence from the question parser, applies these functional modules on the abstract scene representation of the input image, and generates the final answer. The executable program performs purely symbolic operations on its input throughout the entire execution process, and is fully deterministic, disentangled, and interpretable with respect to the program sequence.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7381-neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding",
            "Publication name": "Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum",
            "Code link": "http://nsvqa.csail.mit.edu",
            "Data": "CLEVR dataset. Minecraft VQA\n \n"
        },
        {
            "id": "BA-FDNP",
            "Using for": "Pruning",
            "Based on": "None",
            "Method": "Band Adaptive Frequency-Domain Dynamic Network Pruning \n",
            "Description": "Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Authors firstly give an implementation of CNN in frequency domain. The coefficients can be efficiently pruned since they are sparser after 2-D DCT transformation and many spatial-domain pruning methods can be applied. What\u2019s more, authors set different compression rates for different frequency bands due to the variant importance. BA-FDNP scheme achieves a 8.4\u00d7 of compression and a 9.2\u00d7 of acceleration for ResNet-110 respectively without any loss of accuracy, while the accuracy is even better than the reference model on CIFAR-10.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7382-frequency-domain-dynamic-pruning-for-convolutional-neural-networks",
            "Publication name": "Frequency-Domain Dynamic Pruning for Convolutional Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "CNNs, RESN",
            "Authors": "Zhenhua Liu, Jizheng Xu, Xiulian Peng, Ruiqin Xiong",
            "Code link": "None",
            "Data": "CIFAR-10, ILSVRC-2012\n\n"
        },
        {
            "id": "QDSFM",
            "Using for": "Semi-supervised Learning, PageRank \n",
            "Based on": "NaN",
            "Method": "Quadratic Decomposable Submodular Function Minimization \n",
            "Description": "Authors approach the problem via a new dual strategy and describe an objective that may be optimized via random coordinate descent (RCD) methods and projections onto cones. Authors also establish the linear convergence rate of the RCD algorithm and develop efficient projection algorithms with provable performance guarantees. Numerical experiments in semi-supervised learning on hypergraphs confirm the efficiency of the proposed algorithm and demonstrate the significant improvements in prediction accuracy with respect to state-of-the-art methods.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7383-quadratic-decomposable-submodular-function-minimization",
            "Publication name": "Quadratic Decomposable Submodular Function Minimization \n",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Pan Li, Niao He, Olgica Milenkovic",
            "Code link": "https://github.com/lipan00123/QDSDM\u2028\n",
            "Data": "Synthetic data \n"
        },
        {
            "id": "BSG",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "Block stochastic gradient coordinate descent \n",
            "Description": "Block stochastic gradient coordinate descent cyclically updates all block variables. BSG generalizes SG by updating all the blocks of variables in the Gauss-Seidel type (updating the current block depends on the previously updated block), in either a fixed or randomly shuffled order. Although BSG has slightly more work at each iteration, it typically outperforms SG be- cause of BSG\u2019s Gauss-Seidel updates and larger stepsizes, the latter of which are determined by the smaller per-block Lipschitz constants.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/pdf/1408.2597.pdf",
            "Publication name": "Block Stochastic Gradient Iteration for Convex and Nonconvex Optimization",
            "Released at": "SIAM Journal on Optimization, \n",
            "Interacts with": "DNNs",
            "Authors": "Yangyang Xu, Wotao Yin\n",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "MVP",
            "Using for": "Reinforcement Learning, Risk-sensitive Tasks \n",
            "Based on": "BSD",
            "Method": "Mean-Variance Policy Gradient \n \n",
            "Description": "In this paper, authors develop a model-free policy search framework for mean-variance optimization with finite-sample error bound analysis (to local optima). This paper is motivated to provide a risk-sensitive policy search algorithm with provable sample complexity analysis to maximize the mean-variance objective function. To this end, the objective function is reformulated based on the Legendre-Fenchel duality, from which proposed a stochastic block coordinate ascent policy search algorithm.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7384-a-block-coordinate-ascent-algorithm-for-mean-variance-optimization",
            "Publication name": "A Block Coordinate Ascent Algorithm for Mean-Variance Optimization",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Tengyang Xie, Bo Liu, Yangyang Xu, Mohammad Ghavamzadeh, Yinlam Chow, Daoming Lyu, Daesub Yoon",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "HTDL1R",
            "Using for": "Regression\n",
            "Based on": "NaN",
            "Method": "Heavy-tailed Distributions\\L_1-regression",
            "Description": "In this paper, authors consider the problem of linear regression with heavy-tailed distributions. Different from previous studies that use the squared loss to measure the performance, they choose the absolute loss, which is capable of estimating the conditional median. To address the challenge that both the input and output could be heavy-tailed, they  propose a truncated minimization problem for l1-regression, which is more simple than the truncated min-max formulation for l2-regression. This is the first time an O\udbff\udc19(sqrt(\udbff\udc1cd/n))excess risk is established for l1-regression under the condition that both the input and output could be heavy-tailed. Compared with traditional work on L1-regression, the main advantage of this result is that authors achieve a high-probability risk bound without exponential moment conditions on the input and output.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7385-ell_1-regression-with-heavy-tailed-distributions",
            "Publication name": "\\ell_1-regression with Heavy-tailed Distributions",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Lijun Zhang, Zhi-Hua Zhou",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "NNNN",
            "Using for": "Cassification",
            "Based on": "EMBD, CNN, KNN",
            "Method": "Neural Nearest Neighbors Networks \n",
            "Description": "Existing approaches, however, rely on k-nearest neighbors (KNN) matching in a fixed feature space. The main hurdle in optimizing this feature space w. r. t. application performance is the non-differentiability of the KNN selection rule. To overcome this, we propose a continuous deterministic relaxation of KNN selection that maintains differentiability w. r. t. pairwise distances, but retains the original KNN as the limit of a temperature parameter approaching zero. To exploit our relaxation, we propose the neural nearest neighbors block (N3 block), a novel non-local processing layer that leverages the principle of self-similarity and can be used as building block in modern neural network architectures.1 We show its effectiveness for the set reasoning task of correspondence classification as well as for image restoration, including image denoising and single image super-resolution, where we outperform strong convolutional neural network (CNN)",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7386-neural-nearest-neighbors-networks",
            "Publication name": "Neural Nearest Neighbors Networks",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Tobias Pl\u00f6tz, Stefan Roth",
            "Code link": "https://github.com/visinf/n3net/. \n",
            "Data": "NaN"
        },
        {
            "id": "BAS",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Batch Active Search \n",
            "Description": "Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In these settings, specialized instruments can often evaluate multiple points simultaneously; however, all existing work on active search focuses on sequential acquisition. We bridge this gap, addressing batch active search from both the theoretical and practical perspective. We first derive the Bayesian optimal policy for this problem, then prove a lower bound on the performance gap between sequential and batch optimal policies: the cost of parallelization.'' We also propose novel, efficient batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can dramatically speed up computation. We conduct thorough experiments on data from three application domains: a citation network, material science, and drug discovery, testing all proposed policies (14 total) with a wide range of batch sizes. Our results demonstrate that the empirical performance gap matches our theoretical bound, that nonmyopic policies usually significantly outperform myopic alternatives, and that diversity is an important consideration for batch policy design.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7387-efficient-nonmyopic-batch-active-search",
            "Publication name": "Efficient nonmyopic batch active search",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin Moseley, Roman Garnett",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SCP",
            "Using for": "Recommendation Systems \n",
            "Based on": "NaN",
            "Method": "Strategic Content Providers \n",
            "Description": "Game-theoretic approach to the study of recommendation systems with strategic content providers. Such systems should be fair and stable. Showing that traditional approaches fail to satisfy these requirements, we propose the Shapley mediator. We show that the Shapley mediator satisfies the fairness and stability requirements, runs in linear time, and is the only economically efficient mechanism satisfying these properties.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7388-a-game-theoretic-approach-to-recommendation-systems-with-strategic-content-providers",
            "Publication name": "A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Omer Ben-Porat, Moshe Tennenholtz",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "QBC",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Query by committee",
            "Description": "This algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law.",
            "Publication date": 1992.0,
            "Publication link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30.3233&rep=rep1&type=pdf",
            "Publication name": "Query by committee",
            "Released at": "In Proceedings of the 5th\nAnnual Workshop on Computational Learning Theory",
            "Interacts with": "NaN",
            "Authors": "H.S. Seung, M. Opper, and H. Sompolinsky.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ISL",
            "Using for": "Active Learning, Question-Answering, Query by Commite",
            "Based on": "QBC",
            "Method": "interactive structure learning",
            "Description": "In this work, authors introduce interactive structure learning, a framework that unifies many different interactive learning tasks.This framework can be applied to any structure learning problem in which structures are in one-to-one correspondence with their answers to atomic questions. Thus, interactive structure learning may be viewed as a generalization of active learning.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7389-interactive-structure-learning-with-structural-query-by-committee",
            "Publication name": "Interactive Structure Learning with Structural Query-by-Committee",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Christopher Tosh, Sanjoy Dasgupta",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "MDP",
            "Using for": "Reinforcement Learning ",
            "Based on": "NaN",
            "Method": "Markov Decision Process",
            "Description": "Discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. ",
            "Publication date": 1957.0,
            "Publication link": "http://www.iumj.indiana.edu/IUMJ/FULLTEXT/1957/6/56038",
            "Publication name": "Markov Decision Process",
            "Released at": "Journal of Mathematics and Mechanics",
            "Interacts with": "NaN",
            "Authors": "Richard Bellman",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VIT",
            "Using for": "Reinforcement Learning ",
            "Based on": "MDP",
            "Method": "Value Iteration",
            "Description": "Is the process of reasoning backwards in time, from the end of a problem or situation, to determine a sequence of optimal actions. It proceeds by first considering the last time a decision might be made and choosing what to do in any situation at that time. Using this information, one can then determine what to do at the second-to-last time of decision.",
            "Publication date": 1957.0,
            "Publication link": "http://www.iumj.indiana.edu/IUMJ/FULLTEXT/1957/6/56038",
            "Publication name": "Markov Decision Process",
            "Released at": "Journal of Mathematics and Mechanics",
            "Interacts with": "NaN",
            "Authors": "Richard Bellman",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "Q-L",
            "Using for": "Reinforcement Learning ",
            "Based on": "MDP",
            "Method": "Q-Learning",
            "Description": "Model-free reinforcement learning algorithm. The goal of Q-learning is to learn a policy, which tells an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.",
            "Publication date": 1989.0,
            "Publication link": "http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf",
            "Publication name": "Learning from Delayed Rewards",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Watkins, C.J.C.H.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "INFOS",
            "Using for": "Reinforcement Learning ",
            "Based on": "None",
            "Method": "Information set",
            "Description": "Identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets\u200a\u2014\u200a-sets that record constraints on policies consistent with backed-up Q-values; suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration",
            "Publication name": "Non-delusional Q-learning and value-iteration",
            "Released at": "NIPS",
            "Interacts with": "PCQL, PCVI",
            "Authors": "Tyler Lu, Dale Schuurmans, Craig Boutilier\n",
            "Code link": "None",
            "Data": "Grid World"
        },
        {
            "id": "PCQL",
            "Using for": "Reinforcement Learning ",
            "Based on": "Q-L, INFOS",
            "Method": "Policy-class Q-learning",
            "Description": "Identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets\u200a\u2014\u200a-sets that record constraints on policies consistent with backed-up Q-values; suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration",
            "Publication name": "Non-delusional Q-learning and value-iteration",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Tyler Lu, Dale Schuurmans, Craig Boutilier\n",
            "Code link": "None",
            "Data": "Grid World"
        },
        {
            "id": "PCVI",
            "Using for": "Reinforcement Learning ",
            "Based on": "VIT, INFOS",
            "Method": "Policy-class Value Iteration",
            "Description": "Identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets\u200a\u2014\u200a-sets that record constraints on policies consistent with backed-up Q-values; suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration",
            "Publication name": "Non-delusional Q-learning and value-iteration",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Tyler Lu, Dale Schuurmans, Craig Boutilier\n",
            "Code link": "None",
            "Data": "Grid World"
        },
        {
            "id": "I3D",
            "Using for": "NaN",
            "Based on": "CNN",
            "Method": "Inflated 3D ConvNet ",
            "Description": "The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. \nWe also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1705.07750",
            "Publication name": "Quo vadis, action recognition? a new model and the kinetics dataset",
            "Released at": "CVPR",
            "Interacts with": "NaN",
            "Authors": "J. Carreira and A. Zisserman.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PROGL",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Progressive Learning",
            "Description": "Learning to solve complex sequences of tasks\u2014while both leveraging transfer and avoiding catastrophic forgetting\u2014remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/pdf/1606.04671.pdf",
            "Publication name": "Progressive Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, Raia Hadsell",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VTV",
            "Using for": "NaN",
            "Based on": "GAN, PROGL",
            "Method": "Video-to-Video",
            "Description": "Authors present a general video-to-video synthesis framework based on conditional GANs. Through carefully-designed generators and discriminators as well as a spatio-temporal adversarial objective, we can synthesize high-resolution, photorealistic, and temporally consistent videos. Extensive experiments demonstrate that our results are significantly better than the results by state-of-the-art methods. This method also compares favorably against the competing video prediction methods.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7391-video-to-video-synthesis",
            "Publication name": "Video-to-Video Synthesis",
            "Released at": "NIPS",
            "Interacts with": "RESN,  I3D",
            "Authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Nikolai Yakovenko, Andrew Tao, Jan Kautz, Bryan Catanzaro",
            "Code link": "https://github.com/NVIDIA/vid2vid",
            "Data": "Apolloscape, Face video dataset, Dance video dataset, Cityscapes."
        },
        {
            "id": "NCSGD",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "Non-convex SGD",
            "Description": "Authors obtain better rates for standard non-convex optimization tasks. Without strong assumptions, non-convex optimization theory is always in terms of finding points with small gradients. Designing algorithms to find points with small gradients can help us understand non-convex optimization better and design faster non-convex machine learning algorithms.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7391-video-to-video-synthesis.pdf",
            "Publication name": "How To Make the Gradients Small Stochastically: Even Faster Convex and Nonconvex SGD",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Zeyuan Allen-Zhu",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SYNPO",
            "Using for": "Transfer Learning, Reinforcement Learning ",
            "Based on": "EMBD",
            "Method": "Synthesizing Policies",
            "Description": "In this paper authors present a novel approach that learns to synthesize policies from the disentangled embeddings of environments and tasks. Authors evaluate our approach for the challenging transfer scenarios in two simulators. Main idea is to learn a meta rule to synthesize policies whenever the agent encounters new environments or tasks. Concretely, the meta rule uses the embeddings of the environment and the task to compose a policy, which is parameterized as the linear combination of the policy basis. On the training data from seen pairs of environments and tasks, this algorithm learns the embeddings as well as the policy basis. For new environments or tasks, the agent learns the corresponding embeddings only while it holds the policy basis fixed. Since the embeddings are low-dimensional, a limited amount of training data in the new environment or task is often adequate to learn well so as to compose the desired policy. Authors propose a disentanglement objective such that the embeddings for the tasks and the environments can be extracted to maximize the efficacy of the synthesized policy. Empirical studies demonstrate the importance of disentangling the representations.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7393-synthesize-policies-for-transfer-and-adaptation-across-tasks-and-environments",
            "Publication name": "Synthesize Policies for Transfer and Adaptation across Tasks and Environments",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Hexiang Hu, Liyu Chen, Boqing Gong, Fei Sha",
            "Code link": "None",
            "Data": "GRIDWORLD, THOR"
        },
        {
            "id": "ADVPER",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Adversarial Perturbations",
            "Description": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
            "Publication date": 2014.0,
            "Publication link": "https://arxiv.org/abs/1312.6199",
            "Publication name": "Intriguing properties of neural networks",
            "Released at": "ICLR",
            "Interacts with": "NaN",
            "Authors": "C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ADVVUL",
            "Using for": "Adversarial Examples",
            "Based on": "ADVPER",
            "Method": "Adversarial Vul",
            "Description": "In this paper, authors study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. They derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7394-adversarial-vulnerability-for-any-classifier",
            "Publication name": "Adversarial vulnerability for any classifier",
            "Released at": "NIPS",
            "Interacts with": "DNNs, Classifiers",
            "Authors": "Alhussein Fawzi, Hamza Fawzi, Omar Fawzi",
            "Code link": "None",
            "Data": "CIFAR-10, SVHN"
        },
        {
            "id": "EA",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Evolutionary Algorithm",
            "Description": "An evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Evolutionary_algorithm",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GENA",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Genetic Algorithm",
            "Description": "A genetic algorithm is a search heuristic that is inspired by Charles Darwin's theory of natural evolution. This algorithm reflects the process of natural selection where the fittest individuals are selected for reproduction in order to produce offspring of the next generation.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Genetic_algorithm",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DDPG",
            "Using for": "Reinforcement Learning ",
            "Based on": "NaN",
            "Method": "Deep Deterministic Policy Gradient",
            "Description": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1509.02971",
            "Publication name": "Continuous control with deep reinforcement learning",
            "Released at": "arxiv",
            "Interacts with": "NaN",
            "Authors": "T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ERL",
            "Using for": "Reinforcement Learning ",
            "Based on": "DDPG, EA",
            "Method": "Evolutionary Reinforcement Learning",
            "Description": "Hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. The key insight here is that an EA can be used to address the core challenges within Deep RL without losing out on the ability to leverage gradients for higher sample efficiency. ERL inherits EA\u2019s ability to address temporal credit assignment by its use of a fitness metric that consolidates the return of an entire episode. ERL\u2019s selection operator which operates based on this fitness exerts a selection pressure towards regions of the policy space that lead to higher episode-wide return.This process biases the state distribution towards regions that have higher long term returns. This is a form of implicit prioritization that is effective for domains with long time horizons and sparse rewards.Additionally, ERL inherits EA\u2019s population-based approach leading to redundancies that serve to stabilize the convergence properties and make the learning process more robust. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DeppRL and EA methods.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7395-evolution-guided-policy-gradient-in-reinforcement-learning",
            "Publication name": "Evolution-Guided Policy Gradient in Reinforcement Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": " Shauharda Khadka, Kagan Tumer",
            "Code link": "https://github.com/ShawK91/erl_paper_nips18",
            "Data": "Mujoco"
        },
        {
            "id": "TODLEAR",
            "Using for": "Object recognition",
            "Based on": "VGG",
            "Method": "Toddler learning",
            "Description": "This is the first study that has applied deep learning models as formal models to understand visual object recognition in young children. Inspired by toddler learning, the present paper focused on quality of data to understand which fundamental properties in visual training data can lead to successful learning in visual object recognition \u2014 one of the most challenging tasks in computer vision and machine learning. Towards this goal, authors have conducted a series of simulations which systematically examined how different properties of training data lead to different learning outcomes. They found that image data from toddlers\u2019 egocentric views contains unique properties and distributions that are critical for successful learning.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7396-toddler-inspired-visual-object-learning",
            "Publication name": "Toddler-Inspired Visual Object Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sven Bambach, David Crandall, Linda Smith, Chen Yu",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "DT",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Decision Tree",
            "Description": "A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Decision_tree",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "TAO",
            "Using for": "Classification",
            "Based on": "DT",
            "Method": "Tree Alternating Optimization",
            "Description": "Scalable algorithm that can find a local optimum of oblique trees given a fixed structure, in the sense of repeatedly decreasing the misclassification loss until no more progress can be done. Algorithm produces a new tree with the same or smaller structure but new parameter values that provably lower or leave unchanged the misclassification error. Further, the same algorithm can handle a sparsity penalty, so it can learn sparse oblique trees, having a structure that is a subset of the original tree and few nonzero parameters.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7397-alternating-optimization-of-decision-trees-with-application-to-learning-sparse-oblique-trees",
            "Publication name": "Alternating optimization of decision trees, with application to learning sparse oblique trees",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Miguel A. Carreira-Perpinan, Pooya Tavallali\n",
            "Code link": "None",
            "Data": "MNIST"
        },
        {
            "id": "SIREID",
            "Using for": "Person Re-identification",
            "Based on": "CNN",
            "Method": "Siames re-indetification",
            "Description": "In this paper, proposed a person re-identification method based on a two stream convolutional neural network where each stream is a Siamese network. This architecture can learn spatial and temporal information separately in a re-identification setting.",
            "Publication date": 2017.0,
            "Publication link": "http://openaccess.thecvf.com/content_ICCV_2017/papers/Chung_A_Two_Stream_ICCV_2017_paper.pdf",
            "Publication name": "A two stream siamese convolutional neural network for person re-identification",
            "Released at": "ICCV",
            "Interacts with": "None",
            "Authors": "Chung, D., Tahboub, K., Delp, E.J",
            "Code link": "None",
            "Data": "PRID2011"
        },
        {
            "id": "FD-GAN",
            "Using for": "Person Re-identification, Adversarial Networks",
            "Based on": "GAN, SIREID, RESN",
            "Method": "Feature Distilling Generative Adversarial Network",
            "Description": "It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person\u2019s generated images to be similar. This model achieves state-of-the-art performance on three person reID datasets",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018",
            "Publication name": "FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Shuai Yi, Xiaogang Wang, hongsheng Li",
            "Code link": "https://github.com/yxgeee/FD-GAN",
            "Data": "reID"
        },
        {
            "id": "HSGD",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "Hybrid Stochastic Gradient Descent",
            "Description": "The core idea of HSGD is to iteratively sample an evolving mini-batch of terms in the finite-sum for gradient estimation. This style of incremental gradient method has been shown, both in theory and practice, to bridge smoothly the gap between deterministic and stochastic gradient methods",
            "Publication date": 2012.0,
            "Publication link": "https://arxiv.org/abs/1104.2373",
            "Publication name": "Hybrid deterministic-stochastic methods for data fitting.",
            "Released at": "SIAM Journal on Scientific Computing",
            "Interacts with": "DNNs",
            "Authors": "M. Friedlander and M. Schmidt.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "WORSHSGD",
            "Using for": "Optimization",
            "Based on": "HSGD",
            "Method": "Without-Replacement Sampling Hybrid Stochastic Gradient Descent",
            "Description": "In this work, authors address the aforementioned three limitations in the existing analysis of HSGD. They analyze the rate-of-convergence of HSGD under WoRS in a wide problem spectrum including strongly convex, non-strongly convex and non-convex problems. Though HSGD has been shown, both in theory and practice, to bridge smoothly the gap between full and stochastic gradient descent methods, its rate-of-convergence analysis remains restrictive in several aspects.  Authors proved under WoRS, HSGD with constant step-size can match FG descent in convergence rate, while maintaining comparable sample-size-independent IFO complexity to SGD.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7399-new-insight-into-hybrid-stochastic-gradient-descent-beyond-with-replacement-sampling-and-convexity",
            "Publication name": "New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Pan Zhou, Xiaotong Yuan, Jiashi Feng",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "LINOG",
            "Using for": "Optimization",
            "Based on": "None",
            "Method": "Lingering of Gradients",
            "Description": "In this paper, authors study convex problems where the stochastic gradients can be reused when we move away from x. In theoretical result, they model the number of stochastic gradients that can be changed (and thus cannot be reused) as a function of how much distance we travel away from x, and show faster convergence for gradient descent (in terms of the number of gradient computations). On the empirical side, they show how to modify the SVRG method to use reuse stochastic gradients efficiently.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7400-the-lingering-of-gradients-how-to-reuse-gradients-over-time",
            "Publication name": "The Lingering of Gradients: How to Reuse Gradients Over Time",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Zeyuan Allen-Zhu, David Simchi-Levi, Xinshang Wang",
            "Code link": "None",
            "Data": "Dataset of Yahoo! Front Page Today Module"
        },
        {
            "id": "ULVIAR",
            "Using for": "Cross-view Action Recognition",
            "Based on": "ENCDR, DCDR, CNN, RNN",
            "Method": "Unsupervised Learning of View-invariant Action Representations",
            "Description": "In this work authors propose an unsupervised framework to effectively learn view-invariant video representation that can predict motion sequences for multiple views. The learned representation is extracted from a CNN+RNN based encoder, and decoded into multiple sequences of 3D flows by CNN decoders. The framework is trained by jointly minimizing several losses.\nWe propose a view-adversarial training to encourage view-invariant feature learning. Videos from different views are mapped to a shared subspace where a view classifier cannot discriminate them. The shared representation is enforced to contain meaningful motion information by the use of flow decoders. Proposed method outperforms state-of-the-art unsupervised methods across multiple datasets.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7401-unsupervised-learning-of-view-invariant-action-representations",
            "Publication name": "Unsupervised Learning of View-invariant Action Representations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "WAADM",
            "Using for": "Measure",
            "Based on": "None",
            "Method": "Welfare Analysis for Automated Decision Making",
            "Description": "This work makes an important connection between the growing literature on fairness for machine learning, and the long-established formulations of cardinal social welfare in economics. Thanks to their convexity, this measures can be bounded as part of any convex loss minimization program. Authors provided evidence suggesting that constraining measures often leads to bounded inequality in algorithmic outcomes. Focus in this work was on a normative theory of how rational individuals should compare different algorithmic alternatives. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7402-fairness-behind-a-veil-of-ignorance-a-welfare-analysis-for-automated-decision-making",
            "Publication name": "Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated Decision Making",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Hoda Heidari, Claudio Ferrari, Krishna Gummadi, Andreas Krause",
            "Code link": "None",
            "Data": "Propublica\u2019s COMPAS dataset, Crime and Communities data set "
        },
        {
            "id": "GM-SOP",
            "Using for": "Classification",
            "Based on": "CNN",
            "Method": "Global Gated Mixture of Second-order Pooling",
            "Description": "This method improve deep CNNs, whose core is a trainable gated mixture of parametric second-order pooling model for summarizing the outputs of the last convolution layer as image representation. Given a bank of second-order pooling candidates, this method can adaptively choose Top-K(K > 1) candidates for each input sample through the sparsity-constrained gating module, and performs weighted sum of outputs of K selected candidates as representation of the sample. The GM-SOP can be flexibly integrated into deep CNNs in an end-to-end manner.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7403-global-gated-mixture-of-second-order-pooling-for-improving-deep-convolutional-neural-networks",
            "Publication name": "Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Qilong Wang, Zilin Gao, Jiangtao Xie, Wangmeng Zuo, Peihua Li",
            "Code link": "http://www.peihuali.org/GM-SOP.",
            "Data": "CIFAR10, CIFAR100"
        },
        {
            "id": "CDA",
            "Using for": "Image-To-Image Translation",
            "Based on": "AE",
            "Method": "Cross Domain Autencoder",
            "Description": "A classic autoencoder would take the full representation encoded for input image x, Ge(x) = (SX , EX ) and input it in the decoder of module F , which outputs images in X , with the goal of reconstructing x. Since the shared representations in this model must be indistinguishable, we could use the shared representation SY from the other domain instead of SX. This provides an extra incentive for the encoder to place useful information about domain X in EX, as SY does not contain any domain-exclusive information. Cross-domain autoencoders use this combination to generate the reconstructed input x\u2032 =Fd(SY,EX). This model trains with the standard L1 reconstruction loss.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7404-image-to-image-translation-for-cross-domain-disentanglement",
            "Publication name": "Image-to-image translation for cross-domain disentanglement",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio",
            "Code link": "https://github.com/agonzgarc/cross- domain- disen.",
            "Data": "MNIST, Seeing 3d chairs"
        },
        {
            "id": "CDD",
            "Using for": "Image-To-Image Translation",
            "Based on": "GAN, CDA",
            "Method": "Cross Domain Disen.",
            "Description": "Authors have presented the concept of cross-domain disentanglement and proposed a model to solve it. They model effectively disentangles the representation into a part shared across domains and two parts exclusive to each domain. Authors achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Authors applied this to multiple tasks such as diverse sample generation, cross-domain retrieval, domain-specific image transfer and interpolation. Authors have tested on several datasets of different complexity, both synthetic and of real images.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7404-image-to-image-translation-for-cross-domain-disentanglement",
            "Publication name": "Image-to-image translation for cross-domain disentanglement",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Abel Gonzalez-Garcia, Joost van de Weijer, Yoshua Bengio",
            "Code link": "https://github.com/agonzgarc/cross- domain- disen.",
            "Data": "MNIST, Seeing 3d chairs"
        },
        {
            "id": "GSCEDO",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "Gradient Sparsification for Communication-Efficient Distributed Optimization",
            "Description": "The key idea is to randomly drop out coordinates of the stochastic gradient vectors and amplify the remaining coordinates appropriately to ensure the sparsified gradient to be unbiased. To solve the optimal sparsification efficiently, a simple and fast algorithm is proposed for an approximate solution, with a theoretical guarantee for sparseness.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7405-gradient-sparsification-for-communication-efficient-distributed-optimization",
            "Publication name": "Gradient Sparsification for Communication-Efficient Distributed Optimization",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Jianqiao Wangni, Jialei Wang, Ji Liu, Tong Zhang",
            "Code link": "None",
            "Data": "CIFAR10,"
        },
        {
            "id": "ROCK",
            "Using for": "Object Detection",
            "Based on": "ENCDR, DCDR, RESN",
            "Method": "Residual auxiliary block",
            "Description": "a new generic multi-modal fusion block for deep learning tailored to the primary Multi-Task Learning context. ROCK architecture is based on a residual connection, which makes forward prediction explicitly impacted by the intermediate auxiliary repre- sentations. The auxiliary predictor\u2019s architecture is also specifically designed to our primary Multi-Task Learning context, by incorporating intensive pooling operators for maxi- mizing complementarity of intermediate representations. This method outperforms state-of-the-art object detection models ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7406-revisiting-multi-task-learning-with-rock-a-deep-residual-auxiliary-block-for-visual-detection",
            "Publication name": "Revisiting Multi-Task Learning with ROCK: a Deep Residual Auxiliary Block for Visual Detection",
            "Released at": "NIPS",
            "Interacts with": "RESN",
            "Authors": "Taylor Mordan, Nicolas THOME, Gilles Henaff, Matthieu Cord",
            "Code link": "None",
            "Data": "NYUv2"
        },
        {
            "id": "ADER",
            "Using for": "Online Learning, Convex Optimization",
            "Based on": "None",
            "Method": "Adaptive Learning for Dynamic Environment",
            "Description": "In this paper, authors study the general form of dynamic regret, which compares the cumulative loss of the online learner against an arbitrary sequence of comparators.  Theoretical analysis shows that this method achieves an optimal dynamic regret.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7407-adaptive-online-learning-in-dynamic-environments",
            "Publication name": "Adaptive Online Learning in Dynamic Environments",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Lijun Zhang, Shiyin Lu, Zhi-Hua Zhou",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "TRNSFRMR",
            "Using for": "Machine Translation",
            "Based on": "ATTN",
            "Method": "Transformer",
            "Description": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1706.03762",
            "Publication name": "Attention Is All You Need",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",
            "Code link": "NaN",
            "Data": "WMT 2014"
        },
        {
            "id": "FRAGE",
            "Using for": "Embedding, Word Embedding",
            "Based on": "EMBD, DIS",
            "Method": "Frequency-Agnostic Word Representation",
            "Description": "Simple and effective way to learn word representation using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation, and text classification. In this paper, Authors find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7408-frage-frequency-agnostic-word-representation",
            "Publication name": "FRAGE: Frequency-Agnostic Word Representation",
            "Released at": "NIPS",
            "Interacts with": "TRNSFRMR",
            "Authors": "Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",
            "Code link": "https://github.com/ChengyueGongR/Frequency-Agnostic",
            "Data": "IMDB"
        },
        {
            "id": "VNMT",
            "Using for": "Machine Translation",
            "Based on": "ENCDR, DCDR, VAE",
            "Method": "Variational Neural Machine Translation",
            "Description": "Variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. Authors approximate the posterior distribution with neural networks and reparameterize the variational lower bound. This enables this model to be an end-to-end neural network that can be optimized through the stochastic gradi- ent algorithms.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/abs/1605.07869",
            "Publication name": "Variational Neural Machine Translation",
            "Released at": "Conference on Empirical Methods in Natural Language Processing,",
            "Interacts with": "None",
            "Authors": "B. Zhang, D. Xiong, J. Su, H. Duan, and M. Zhang.",
            "Code link": "https://github.com/DeepLearnXMU/VNMT",
            "Data": "NIST MT05"
        },
        {
            "id": "GNMT",
            "Using for": "Machine Translation",
            "Based on": "ENCDR, DCDR, VNMT",
            "Method": "Generative Neural Machine Translation",
            "Description": "Latent variable architecture which is designed to model the semantics of the source and target sentences. This architecture models the joint distribution of the target sentence and the source sentence. To do this, it uses the latent variable as a language agnostic representation of the sentence, which generates text in both the source and target languages.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7409-generative-neural-machine-translation",
            "Publication name": "Generative Neural Machine Translation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Harshil Shah, David Barber",
            "Code link": "None",
            "Data": "Multi UN corpus"
        },
        {
            "id": "SBM",
            "Using for": "NaN",
            "Based on": "None",
            "Method": "Stochastic Block Model",
            "Description": "The model provides a stochastrc generalization of the blockmodel. Estimation techniques are developed for the special case of a single relation social network, with blocks specified prior. ",
            "Publication date": 1983.0,
            "Publication link": "http://www.stat.cmu.edu/~brian/780/bibliography/04%20Blockmodels/Holland%20-%201983%20-%20Stochastic%20blockmodels,%20first%20steps.pdf",
            "Publication name": "New Insight into Hybrid Stochastic Gradient Descent: Beyond With-Replacement Sampling and Convexity",
            "Released at": "Social Networks,\n5(2):109\u2013137",
            "Interacts with": "None",
            "Authors": "P. W. Holland, K. B. Laskey, and S. Leinhardt",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "UMVC",
            "Using for": "Planted Vertex Cover Problem",
            "Based on": "SBM",
            "Method": "Union of Minimal Vertex Covers",
            "Description": "Framework for analyzing this planted vertex cover problem, based on the theory of fixed-parameter tractability, together with algorithms for recovering the core.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7410-found-graph-data-and-planted-vertex-covers",
            "Publication name": "Found Graph Data and Planted Vertex Covers",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Austin R. Benson, Jon Kleinberg",
            "Code link": "https://github.com/arbenson/FGDnPVC.",
            "Data": "email-W3C, email-Enron, email-Eu, call-Reality, text-Reality"
        },
        {
            "id": "JCVE",
            "Using for": "NaN",
            "Based on": "Q-L",
            "Method": "Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding\n",
            "Description": "We propose a novel method to encode the subset of features by appropriately modifying  that can naturally handle missing entries and is shared by the classifier and the agent. In addition, a synchronous variant of n-step Q-learning to handle real-valued feature space in feature acquisition problems.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7411-joint-active-feature-acquisition-and-classification-with-variable-size-set-encoding",
            "Publication name": "Joint Active Feature Acquisition and Classification with Variable-Size Set Encoding",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Hajin Shim, Sung Ju Hwang, Eunho Yang",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "COUL",
            "Using for": "Classification",
            "Based on": "NaN",
            "Method": "Regularization Learning Networks",
            "Description": "Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7412-regularization-learning-networks-deep-learning-for-tabular-datasets",
            "Publication name": "Regularization Learning Networks: Deep Learning for Tabular Datasets",
            "Released at": "NIPS",
            "Interacts with": "RLNs",
            "Authors": "Ira Shavitt, Eran Segal\n",
            "Code link": "https://github.com/irashavitt/regularization_ learning_networks.",
            "Data": "MNIST"
        },
        {
            "id": "RLNs",
            "Using for": "Classification",
            "Based on": "COUL",
            "Method": "Regularization Learning Networks",
            "Description": "Despite their impressive performance, Deep Neural Networks (DNNs) typically underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning tasks. We propose that applying a different regularization coefficient to each weight might boost the performance of DNNs by allowing them to make more use of the more relevant inputs. However, this will lead to an intractable number of hyperparameters. Here, we introduce Regularization Learning Networks (RLNs), which overcome this challenge by introducing an efficient hyperparameter tuning scheme which minimizes a new Counterfactual Loss. Our results show that RLNs significantly improve DNNs on tabular datasets, and achieve comparable results to GBTs, with the best performance achieved with an ensemble that combines GBTs and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of the network edges and 82% of the input features, thus providing more interpretable models and reveal the importance that the network assigns to different inputs. RLNs could efficiently learn a single network in datasets that comprise both tabular and unstructured data, such as in the setting of medical imaging accompanied by electronic health records.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7412-regularization-learning-networks-deep-learning-for-tabular-datasets",
            "Publication name": "Regularization Learning Networks: Deep Learning for Tabular Datasets",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ira Shavitt, Eran Segal\n",
            "Code link": "https://github.com/irashavitt/regularization_ learning_networks.",
            "Data": "MNIST"
        },
        {
            "id": "BOOST",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Application to Boosting",
            "Description": "In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight- update Littlestone\udbff\udc08Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a con- siderably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algo- rithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm.",
            "Publication date": 1997.0,
            "Publication link": "http://www.face-rec.org/algorithms/Boosting-Ensemble/decision-theoretic_generalization.pdf",
            "Publication name": "A decision-theoretic generalization of on-line learning\nand an application to boosting.",
            "Released at": "Journal of computer and system sciences",
            "Interacts with": "NaN",
            "Authors": "Yoav Freund and Robert E Schapire",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "SMTBoost",
            "Using for": "Classification",
            "Based on": "BOOST",
            "Method": "Survival Multitask Boosting",
            "Description": "A nonparametric boosting algorithm for jointly estimating survival distributions for multiple tasks. Boosting algorithms iteratively train simple predictive models on weighted samples of the data such as to encourage improvement on those data points that are mis-predicted in previous iterations. The procedure is sequential: outcome-specific survival distributions form the components of nonparametric multivariate estimators which we combine into an ensemble in such a way as to ensure accurate predictions on all outcome types simultaneously.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7413-multitask-boosting-for-survival-analysis-with-competing-risks",
            "Publication name": "Multitask Boosting for Survival Analysis with Competing Risks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Alexis Bellot, Mihaela van der Schaar",
            "Code link": "None",
            "Data": "SEER"
        },
        {
            "id": "DM",
            "Using for": "Dimensionality Reduction",
            "Based on": "None",
            "Method": "Diffusion maps",
            "Description": "Nonlinear dimensionality reduction framework. This method robustly captures an intrinsic manifold geometry using a row-stochastic Markov matrix associated with a graph of the data.",
            "Publication date": 2006.0,
            "Publication link": "https://pdfs.semanticscholar.org/012e/b8da8885060d22c2598e287e61b25cec2121.pdf",
            "Publication name": "Diffusion maps",
            "Released at": "Applied and Computational Harmonic Analysis",
            "Interacts with": "None",
            "Authors": "Coifman, Ronald R. and Lafon, St\u00e9phane",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SUGAR",
            "Using for": "Generation",
            "Based on": "DM",
            "Method": "Synthesis Using Geometrically Aligned Random-walks",
            "Description": "New type of generative model for high-dimensional data that learns a manifold geometry of the data, rather than density, and can generate points evenly along this manifold. This enables us to compensate for sparsity and heavily biased sampling in many data types of interest, especially biomedical data.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7414-geometry-based-data-generation",
            "Publication name": "Geometry Based Data Generation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ofir Lindenbaum, Jay Stanley, Guy Wolf, Smita Krishnaswamy\n",
            "Code link": "github.com/KrishnaswamyLab/SUGAR",
            "Data": "MNIST"
        },
        {
            "id": "SLAYER",
            "Using for": "Optimization",
            "Based on": "SPIKN",
            "Method": "Spike Layer Error Reassignment in Time",
            "Description": "Error backpropagation for SNNs which properly considers the temporal dependency between input and output signals of a spiking neuron, handles the non-differentiable nature of the spike function, and is not prone to the dead neuron problem. During training, we require both true and false neurons to fire, but specify a much higher spike count target for the true class neuron. This approach prevents neurons from going dormant and they easily learn to fire more frequently again when required. The desired spike count was chosen to be roughly proportional to the simulation interval.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time",
            "Publication name": "SLAYER: Spike Layer Error Reassignment in Time",
            "Released at": "NIPS",
            "Interacts with": "SNN",
            "Authors": "Sumit Bam Shrestha, Garrick Orchard",
            "Code link": "None",
            "Data": "MNIST, NMNIST, DVS Gesture, TIDIGITS datasets"
        },
        {
            "id": "LSVEE",
            "Using for": "Reinforcement Learning ",
            "Based on": "None",
            "Method": "Least Squares Value Elimination by Exploration",
            "Description": "An agent to take actions based on observations (features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/pdf/1602.02722.pdf",
            "Publication name": "PAC reinforcement learning with rich observations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Akshay Krishnamurthy, Alekh Agarwal, and John Langford",
            "Code link": "None",
            "Data": "ATARI"
        },
        {
            "id": "VALOR",
            "Using for": "Reinforcement Learning ",
            "Based on": "LSVEE",
            "Method": "Values stored Locally for RL",
            "Description": "These algorithms are com- putationally efficient in an oracle model, and we emphasize that the oracle-based approach has led to practical algorithms for many other settings",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7416-on-oracle-efficient-pac-rl-with-rich-observations",
            "Publication name": "On Oracle-Efficient PAC RL with Rich Observations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert E. Schapire",
            "Code link": "None",
            "Data": "MINECRAFT ENV"
        },
        {
            "id": "GDSNN",
            "Using for": "Optimization",
            "Based on": "None",
            "Method": "Gradient Descent for Spiking Neural Networks",
            "Description": "This method optimizes the spiking network dynamics for general supervised tasks on the time scale of individual spikes as well as the behavioral time scales.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7417-gradient-descent-for-spiking-neural-networks",
            "Publication name": "Gradient Descent for Spiking Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "SNN",
            "Authors": "Dongsung Huh, Terrence J. Sejnowski",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "BNN",
            "Using for": "NaN",
            "Based on": "BAYESM, DNN",
            "Method": "Bayesian Neural Networks",
            "Description": "A Bayesian neural network is a neural network with a prior distribution on its weights",
            "Publication date": 2014.0,
            "Publication link": "https://arxiv.org/abs/1505.05424",
            "Publication name": "Weight Uncertainty in Neural Networks",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "SBN",
            "Using for": "Optimization",
            "Based on": "MCMC, BNN",
            "Method": "Subsplit Bayesian Networks",
            "Description": "In this paper, we derive a general Bayesian network formulation for probability estimation on leaf-labeled trees that enables flexible approximations which can generalize beyond observations.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7418-generalizing-tree-probability-estimation-via-bayesian-networks",
            "Publication name": "Generalizing Tree Probability Estimation via Bayesian Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Cheng Zhang, Frederick A Matsen IV",
            "Code link": "https://github.com/zcrabbit/sbn.",
            "Data": "None"
        },
        {
            "id": "IDME",
            "Using for": "Reinforcement Learning ",
            "Based on": "Q-L",
            "Method": "Internal Dynamics Model Estimation",
            "Description": "Authors contribute an algorithm that learns a user\u2019s implicit beliefs about the dynamics of the environment from demonstrations of their suboptimal behavior in the real environment. Simulation experiments and a small-scale user study demonstrate the effectiveness of our method at recovering a dynamics model that explains human actions, as well as its utility for applications in shared autonomy and inverse reinforcement learning.\n",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7419-where-do-you-think-youre-going-inferring-beliefs-about-dynamics-from-behavior",
            "Publication name": "Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sid Reddy, Anca Dragan, Sergey Levine",
            "Code link": "None",
            "Data": "Open AI Gym"
        },
        {
            "id": "SBS",
            "Using for": "Classification",
            "Based on": "None",
            "Method": "Splatting Blurring Slicing",
            "Description": "In SBS, pixels are \u201csplatted\u201d(downsampled) onto the grid to reduce the data size, then those vertexes are blurred, finally the filtered values for each pixel are produced via \u201cslicing\u201d(upsampling).",
            "Publication date": 2006.0,
            "Publication link": "https://people.csail.mit.edu/sparis/publi/2006/eccv/Paris_06_Fast_Approximation.pdf",
            "Publication name": "A fast approximation of the bilateral filter using a signal processing approach",
            "Released at": "European Conference on Computer Vision",
            "Interacts with": "AccNet",
            "Authors": "Sylvain Paris,  Fr\u00e9do Durand",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "AccNet",
            "Using for": "Denoising",
            "Based on": "CNN,  SBS",
            "Method": "Acceleration Network",
            "Description": "In this work proposed the first neural network producing fast high-dimensional convolution algorithms. Authors take AccNet to express the approximation function of Splatting Blurring Slicing and generalize Splatting Blurring Slicing by changing the architecture of AccNet. This is done by: 1, interpreting splatting, blurring, slicing operations as convolutions; 2, turning these convolutions to gCP layers to build AccNet.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7420-designing-by-training-acceleration-neural-network-for-fast-high-dimensional-convolution",
            "Publication name": "Designing by Training: Acceleration Neural Network for Fast High-Dimensional Convolution",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Longquan Dai, Liang Tang, Yuan Xie, Jinhui Tang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "AMT",
            "Using for": "Training",
            "Based on": "None",
            "Method": "Algorithmic Machine Teaching",
            "Description": "This menthod studies the interaction between a teacher and a student/learner where the teacher\u2019s objective is to find an optimal training sequence to steer the learner towards a desired goal",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1801.05927",
            "Publication name": "An Overview of Machine Teaching",
            "Released at": "arXiv",
            "Interacts with": "DNNs",
            "Authors": "Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Rafferty.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "AIMT",
            "Using for": "Training",
            "Based on": "AMT",
            "Method": "Adaptivity in Machine Teaching",
            "Description": "In this paper, authors study the case of teaching consistent, version space learners in an interactive setting. At any time step, the teacher provides an example, the learner performs an update, and the teacher observes the learner\u2019s new state.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7421-understanding-the-role-of-adaptivity-in-machine-teaching-the-case-of-version-space-learners",
            "Publication name": "Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Yuxin Chen, Adish Singla, Oisin Mac Aodha, Pietro Perona, Yisong Yue",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SVM",
            "Using for": "Classification, Regression",
            "Based on": "NaN",
            "Method": "Support Vector Machine",
            "Description": "Supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.",
            "Publication date": 1963.0,
            "Publication link": "https://en.wikipedia.org/wiki/Support-vector_machine",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": " Vladimir N. Vapnik, Alexey Ya. Chervonenkis ",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CFFAD",
            "Using for": "Anomaly Detection",
            "Based on": "SVM",
            "Method": "Classification Framework for Anomaly Detection",
            "Description": "Experimental results for anomaly detection problems",
            "Publication date": 2005.0,
            "Publication link": "http://www.jmlr.org/papers/volume6/steinwart05a/steinwart05a.pdf",
            "Publication name": "A classification framework for anomaly detection.",
            "Released at": "Journal of Machine Learning Research",
            "Interacts with": "None",
            "Authors": "Ingo Steinwart, Don R. Hush, and Clint Scovel",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "CADL",
            "Using for": "Anomaly Detection",
            "Based on": "CFFAD, SVM",
            "Method": "Calibrated Anomaly Detection Loss",
            "Description": "A loss function framework for calibrated anomaly detection. The framework also produced a close relative of the one-class SVM as a special case",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7422-a-loss-framework-for-calibrated-anomaly-detection",
            "Publication name": "A loss framework for calibrated anomaly detection",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Aditya Krishna Menon, Robert C. Williamson",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "PacGAN",
            "Using for": "Generation, Adversarial Networks",
            "Based on": "DCGAN",
            "Method": "The power of two samples in generative adversarial networks\n ",
            "Description": "A new GAN framework to mitigate mode collapse. The main idea of model is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. This allows the discriminator to do binary hypothesis testing based on the product distributions, which naturally penalizes mode collapse.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7423-pacgan-the-power-of-two-samples-in-generative-adversarial-networks",
            "Publication name": "PacGAN: The power of two samples in generative adversarial networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Zinan Lin, Ashish Khetan, Giulia Fanti, Sewoong Oh",
            "Code link": "https://github.com/fjxmlzn/PacGAN",
            "Data": "MNIST, CelebA"
        },
        {
            "id": "MANNs",
            "Using for": "Prediction",
            "Based on": "NaN",
            "Method": "Memory Augmented Neural Networks",
            "Description": "Such models have larger memory capacity and thus \u201cremember\u201d temporally distant information in the input sequence and provide a RAM-like mechanism to support model execution.",
            "Publication date": 2014.0,
            "Publication link": "https://arxiv.org/abs/1410.5401",
            "Publication name": "Neural Turing Machines",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "VMED",
            "Using for": "Generation",
            "Based on": "VAE, LSTM, MANNs",
            "Method": "Variational Memory Encoder-Decoder",
            "Description": "Introduces variability into encoder-decoder architecture via the use of external memory as mixture model. By modeling the latent temporal dependencies across timesteps. VMED produces a Mixture of Gaussians representing the latent distribution. Each mode of the Mixture of Gaussians associates with some memory slot and thus captures some aspect of context supporting generation process. To accommodate the MoG, AUTHORS employ a KL approximation and we demonstrate that minimizing this approximation is equivalent to minimizing the KL divergence.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7424-variational-memory-encoder-decoder",
            "Publication name": "Variational Memory Encoder-Decoder",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Hung Le, Truyen Tran, Thin Nguyen, Svetha Venkatesh",
            "Code link": "https://github.com/thaihungle/VMED",
            "Data": "Cornell Moviem,  OpenSubtitle"
        },
        {
            "id": "SCMD",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "Stochastic composite mirror descent",
            "Description": "is a powerful extension of SGD based on two motivations. Firstly, it relaxes the Hilbert space structure of SGD by using a mirror map to capture geometric properties of data from a Banach space. Secondly, it exploits the problem structure by separating, at every iteration, a data-fitting term and a regularization term in structured optimization problems to obtain a desired regularization effect, which arise naturally since a regularizer is often introduced to either avoid overfitting or impose a priori information",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7425-stochastic-composite-mirror-descent-optimal-bounds-with-high-probabilities",
            "Publication name": "Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Yunwen Lei, Ke Tang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "REINFORCE",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility.",
            "Description": "Algorithm for policy gradient can be estimating using the likelihood ratio",
            "Publication date": 1992.0,
            "Publication link": "http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf",
            "Publication name": "Simple Statistical Gradien Following Algorithms Connectionist Reinforcement Learning",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HRGR-A",
            "Using for": "Reinforcement Learning , Generation, Image Report",
            "Based on": "CNN, DCDR, RNN, REINFORCE",
            "Method": "Hybrid Retrieval-Generation Reinforced Agent",
            "Description": "This method reconciles traditional retrieval-based approaches populated with human prior knowledge, with modern learning-based approaches to achieve structured, robust, and diverse report generation. For each sentence, a high-level retrieval policy module chooses to either retrieve a template sentence from an off-the-shelf template database, or invoke a low-level generation module to generate a new sentence.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7426-hybrid-retrieval-generation-reinforced-agent-for-medical-image-report-generation",
            "Publication name": "Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yuan Li, Xiaodan Liang, Zhiting Hu, Eric P. Xing",
            "Code link": "None",
            "Data": "Indiana University Chest X-Ray Collection"
        },
        {
            "id": "VQAAR",
            "Using for": "Visual Question Answering",
            "Based on": "ENCDR, ADVELT",
            "Method": "Visual Question Answering with Adversarial Regularization\n",
            "Description": "A novel adversarial regularization scheme for reducing the memorization of dataset biases in VQA based on a question-only adversary and the difference of model confidences after processing the image",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7427-overcoming-language-priors-in-visual-question-answering-with-adversarial-regularization",
            "Publication name": "Overcoming Language Priors in Visual Question Answering with Adversarial Regularization",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sainandan Ramakrishnan, Aishwarya Agrawal, Stefan Lee",
            "Code link": "None",
            "Data": "VQA-CPv2, VQAv2 "
        },
        {
            "id": "HKRM",
            "Using for": "Object Detection",
            "Based on": "None",
            "Method": "Hybrid Knowledge Routed Modules",
            "Description": "This module incorporates the reasoning routed by two kinds of knowledge forms: an explicit knowledge module for structured constraints that are summarized with linguistic knowledge (e.g. shared attributes, relationships) about concepts; and an implicit knowledge module that depicts some implicit constraints (e.g. common spatial layouts)",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7428-hybrid-knowledge-routed-modules-for-large-scale-object-detection",
            "Publication name": "Hybrid Knowledge Routed Modules for Large-scale Object Detection",
            "Released at": "NIPS",
            "Interacts with": "RESN",
            "Authors": "ChenHan Jiang, Hang Xu, Xiaodan Liang, Liang Lin",
            "Code link": "https://github.com/chanyn/HKRM.",
            "Data": "PASCAL VOC 2007\n, COCO 2017."
        },
        {
            "id": "Bottom-Up",
            "Using for": "VQA",
            "Based on": "FRCNN",
            "Method": "Bottom-Up",
            "Description": "Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1707.07998",
            "Publication name": "Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering",
            "Released at": "arXiv",
            "Interacts with": "NaN",
            "Authors": "Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BAN",
            "Using for": "VQA",
            "Based on": "ATTN",
            "Method": "Bilinear Attention Networks",
            "Description": "This model extends unitary attention networks exploiting bilinear attention maps, where the joint representations of multimodal multi-channel inputs are extracted using low-rank bilinear pooling. BAN find bilinear attention distributions to utilize given vision-language information seamlessly. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7429-bilinear-attention-networks",
            "Publication name": "Bilinear Attention Networks",
            "Released at": "NIPS",
            "Interacts with": "Bottom-Up",
            "Authors": "Jin-Hwa Kim, Jaehyun Jun, Byoung-Tak Zhang",
            "Code link": "None",
            "Data": "Visual Genome"
        },
        {
            "id": "HTQF",
            "Using for": "Sequential Learning",
            "Based on": "None",
            "Method": "Heavy-tailed Quantile Function",
            "Description": "Models a distribution with asymmetric left and right heavy tails",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7430-parsimonious-quantile-regression-of-financial-asset-tail-dynamics-via-sequential-learning",
            "Publication name": "Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning",
            "Released at": "NIPS",
            "Interacts with": "LSTM",
            "Authors": "Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, Qi Wu",
            "Code link": "None",
            "Data": "Real-world Market Data"
        },
        {
            "id": "Conv-MKL",
            "Using for": "Multi-Class Learning, Classification",
            "Based on": "CNN",
            "Method": "Conv-Multi-Class Learning",
            "Description": "Using precomputed kernel matrices regularized by local Rademacher complexity, this method can be implemented by any lp-norm multi-class Multi-Class Learning solvers;",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7431-multi-class-learning-from-theory-to-algorithm",
            "Publication name": "Multi-Class Learning: From Theory to Algorithm",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang",
            "Code link": "None",
            "Data": "LIBSVM"
        },
        {
            "id": "SMSD-MKL",
            "Using for": "Multi-Class Learning, Classification",
            "Based on": "SGD",
            "Method": "Stochastic mirror and sub-gradient descent algorithm-Multi-Class Learning",
            "Description": "This method puts local Rademacher complexity in penalized ERM with l2,p-norm regularizer, implemented by stochastic sub-gradient descent with updating dual weights.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7431-multi-class-learning-from-theory-to-algorithm",
            "Publication name": "Multi-Class Learning: From Theory to Algorithm",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, Weiping Wang",
            "Code link": "None",
            "Data": "LIBSVM"
        },
        {
            "id": "GRUI",
            "Using for": "Time Series Imputation",
            "Based on": "GRU",
            "Method": "Gated Recurrent Units I",
            "Description": "In order to learn the unfixed time lags of two observed values, a modified GRU cell called GRUI is proposed for processing the incomplete time series. This method can take into account the non-fixed time lags and fade the influence of the past observations determined by the time lags.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7432-multivariate-time-series-imputation-with-generative-adversarial-networks",
            "Publication name": "Multivariate Time Series Imputation with Generative Adversarial Networks",
            "Released at": "NIPS",
            "Interacts with": "GAN",
            "Authors": "Yonghong Luo, Xiangrui Cai, Ying ZHANG, Jun Xu, Yuan xiaojie",
            "Code link": "None",
            "Data": "Physionet dataset."
        },
        {
            "id": "SVCO",
            "Using for": "Classification",
            "Based on": "CNN",
            "Method": "Spatial Versatile Convolution Operation",
            "Description": "This method generate multiple feature maps using a fixed number of convolution filters.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7433-learning-versatile-filters-for-efficient-convolutional-neural-networks",
            "Publication name": "Learning Versatile Filters for Efficient Convolutional Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yunhe Wang, Chang Xu, Chunjing XU, Chao Xu, Dacheng Tao",
            "Code link": "None",
            "Data": "ILSVRC 2012"
        },
        {
            "id": "CVF",
            "Using for": "Classification",
            "Based on": "SVCO",
            "Method": "Channel Versatile Filters",
            "Description": "Thos filters generates a series of secondary convolution filters by adjusting the height and width of a given convolution filter. CVF reduced more than 30% weights of convolution filters and achieved the smallest model size with a comparable accuracy, which is a more portable convolutional neural network.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7433-learning-versatile-filters-for-efficient-convolutional-neural-networks",
            "Publication name": "Learning Versatile Filters for Efficient Convolutional Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yunhe Wang, Chang Xu, Chunjing XU, Chao Xu, Dacheng Tao",
            "Code link": "None",
            "Data": "ILSVRC 2012"
        },
        {
            "id": "BFGS",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm",
            "Description": "Is an iterative method for solving unconstrained nonlinear optimization problems.The BFGS method belongs to quasi-Newton methods, a class of hill-climbing optimization techniques that seek a stationary point of a (preferably twice continuously differentiable) function. For such problems, a necessary condition for optimality is that the gradient be zero. Newton's method and the BFGS methods are not guaranteed to converge unless the function has a quadratic Taylor expansion near an optimum. However, BFGS can have acceptable performance even for non-smooth optimization instances.",
            "Publication date": 1967.0,
            "Publication link": "https://www.jstor.org/stable/2003239?seq=1#page_scan_tab_contents",
            "Publication name": "Quasi-Newton methods and their application to function minimisation",
            "Released at": "Mathematics of Computation",
            "Interacts with": "NaN",
            "Authors": "Charles G Broyden.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ASMI",
            "Using for": "Optimization, Second-Order Optimization",
            "Based on": "BFGS",
            "Method": "Accelerated Stochastic Matrix Inversion",
            "Description": "Algorithm for inverting positive definite matrices. This algorithm can be seen as a special case of the accelerated sketch-and-project in Euclidean space, thus its convergence follows from the main theorem. However, we also provide a different formulation of the proof that is specialized to this setting.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7434-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization",
            "Publication name": "Accelerated Stochastic Matrix Inversion: General Theory and Speeding up BFGS Rules for Faster Second-Order Optimization",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Robert Gower, Filip Hanzely, Peter Richtarik, Sebastian U. Stich",
            "Code link": "None",
            "Data": "SVHN dataset, Epsilon dataset "
        },
        {
            "id": "RANDW",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Random Walk",
            "Description": "Random Walk is an algorithm that provides random paths in a graph. A random walk means that we start at one node, choose a neighbor to navigate to at random or based on a provided probability distribution, and then do the same from that node, keeping the resulting path in a list.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Random_walk",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BRS",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Basic Random Search",
            "Description": "Simply computes a finite difference approximation along the random direction and then takes a step along this direction without using a line search.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Random_search",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DifNet",
            "Using for": "Segmentation",
            "Based on": "RANDW",
            "Method": "Diffusion Networks",
            "Description": "This model applies the cascaded random walks to approximate a complex diffusion process.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7435-difnet-semantic-segmentation-by-diffusion-networks",
            "Publication name": "DifNet: Semantic Segmentation by Diffusion Networks",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Peng Jiang, Fanglin Gu, Yunhai Wang, Changhe Tu, Baoquan Chen",
            "Code link": "None",
            "Data": "Pascal VOC, Pascal Context "
        },
        {
            "id": "CDANs",
            "Using for": "DOMAP",
            "Based on": "DOMAP, ADVELT",
            "Method": "Conditional domain adversarial networks",
            "Description": "A novel approach to domain adaptation with multimodal distributions. Unlike previous adversarial adaptation methods that solely match the feature representation across domains which is prone to under-matching, the proposed approach further conditions the adversarial domain adaptation on discriminative information to enable alignment of multimodal distributions.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7436-conditional-adversarial-domain-adaptation",
            "Publication name": "Conditional Adversarial Domain Adaptation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, Michael I. Jordan",
            "Code link": "github.com/thuml.",
            "Data": "ImageNet, ILSVRC 2012, MNIST, SVHN"
        },
        {
            "id": "CHRISF",
            "Using for": "NaN",
            "Based on": "None",
            "Method": "Christoffel function",
            "Description": "A classical tool in polynomial algebra which provides a bound for the evaluation at a given point of a given degree polynomial P in terms of an average value of P2",
            "Publication date": 1974.0,
            "Publication link": "https://people.math.osu.edu/nevai.1/SZEGO/szego=szego1975=ops=OCR.pdf",
            "Publication name": "Orthogonal polynomials",
            "Released at": "In Colloquium publications, AMS, (23), fourth edition,",
            "Interacts with": "None",
            "Authors": "G. Szeg\u00f6.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "RECHRISF",
            "Using for": "Kernel Methods",
            "Based on": "CHRISF",
            "Method": "Regularized Christoffel function",
            "Description": "This method uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7438-relating-leverage-scores-and-density-using-regularized-christoffel-functions",
            "Publication name": "Relating Leverage Scores and Density using Regularized Christoffel Functions",
            "Released at": "NIPS",
            "Interacts with": "KERM",
            "Authors": "Edouard Pauwels, Francis Bach, Jean-Philippe Vert",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "NLRN",
            "Using for": "Image Restoration",
            "Based on": "RNN",
            "Method": "Non-Local Recurrent Network",
            "Description": "Unlike existing methods that measure self-similarity in an isolated manner, the proposed non-local module can be flexibly integrated into existing deep networks for end-to-end training to capture deep feature correlation between each location and its neighborhood. Authors fully employ the RNN structure for its parameter efficiency and allow deep feature correlation to be propagated along adjacent recurrent states.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7439-non-local-recurrent-network-for-image-restoration",
            "Publication name": "Non-Local Recurrent Network for Image Restoration",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, Thomas S. Huang",
            "Code link": "https://github.com/Ding-Liu/NLRN.",
            "Data": "Set12, BSD68, Urban100"
        },
        {
            "id": "GPP",
            "Using for": "NaN",
            "Based on": "BAYESM, GP",
            "Method": "Graph Gaussian process",
            "Description": "Gaussian process model that is data-efficient for semi-supervised learning problems on graphs. In the experiments, we show that the proposed model is competitive with the state-of- the-art deep learning models, and outperforms when the number of labels is small.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7440-bayesian-semi-supervised-learning-with-graph-gaussian-processes",
            "Publication name": "Bayesian Semi-supervised Learning with Graph Gaussian Processes",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yin Cheng Ng, Nicol\u00f2 Colombo, Ricardo Silva\n",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "FCJS",
            "Using for": "Segmentation",
            "Based on": "None",
            "Method": "Foreground Clustering for Joint Segmentation",
            "Description": "A novel framework in which video/image segmentation and localization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. The proposed framework leverages two representations at different levels, exploits the spatial relationship between bounding boxes and superpixels as linear constraints and simultaneously discriminates between foreground and background at bounding box and superpixel level.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7441-foreground-clustering-for-joint-segmentation-and-localization-in-videos-and-images",
            "Publication name": "Foreground Clustering for Joint Segmentation and Localization in Videos and Images",
            "Released at": "NIPS",
            "Interacts with": "CNN",
            "Authors": "Abhishek Sharma",
            "Code link": "https://github.com/Not-IITian/Foreground-Clustering-for-Joint-segmentation-and-Localization",
            "Data": "YouTube Object video dataset, Internet Object Discovery dataset, Pascal VOC 2007.\n"
        },
        {
            "id": "VPSS",
            "Using for": "Video Prediction",
            "Based on": "LSTM, AE, CNN,",
            "Method": "Video Prediction via Selective Sampling",
            "Description": "Specifically a Sampling module produces a collection of high quality proposals, facilitated by a multiple choice adversarial learning scheme, yielding diverse frame proposal set. Subsequently a Selection module selects high possibility candidates from proposals and combines them to produce final prediction. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7442-video-prediction-via-selective-sampling",
            "Publication name": "Video Prediction via Selective Sampling",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Jingwei Xu, Bingbing Ni, Xiaokang Yang",
            "Code link": "None",
            "Data": "MovingMnist, Robot-Push, Human3.6M "
        },
        {
            "id": "DWL",
            "Using for": "NaN",
            "Based on": "WASSD",
            "Method": "Distilled Wasserstein Learning",
            "Description": "A novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a unified framework.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7443-distilled-wasserstein-learning-for-word-embedding-and-topic-modeling",
            "Publication name": "Distilled Wasserstein Learning for Word Embedding and Topic Modeling",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Hongteng Xu, Wenlin Wang, Wei Liu, Lawrence Carin",
            "Code link": "None",
            "Data": "MIMIC-III"
        },
        {
            "id": "PRIM",
            "Using for": "3D Scene Parsing",
            "Based on": "None",
            "Method": "Primitive Prediction Module",
            "Description": "A physical stability model that combines primitive and layout prediction modules with a physics simulation engine.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7444-learning-to-exploit-stability-for-3d-scene-parsing",
            "Publication name": "Learning to Exploit Stability for 3D Scene Parsing",
            "Released at": "NIPS",
            "Interacts with": "RCNN",
            "Authors": "Yilun Du, Zhijian Liu, Hector Basevi, Ales Leonardis, Bill Freeman, Josh Tenenbaum, Jiajun Wu",
            "Code link": "None",
            "Data": "SceneNet RGB-D"
        },
        {
            "id": "TRPO",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Trust Region Policy Optimization",
            "Description": "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1502.05477",
            "Publication name": "Trust Region Policy Optimization",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PPO",
            "Using for": "Reinforcement Learning",
            "Based on": "TRPO",
            "Method": "Proximal Policy Optimization",
            "Description": "Family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a \"surrogate\" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1707.06347",
            "Publication name": "Proximal Policy Optimizations",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "G2Ns",
            "Using for": "Reinforcement Learning",
            "Based on": "MLP, GENA",
            "Method": "Genetic-Gated Networks",
            "Description": "Simple neural networks that combine a gate vector composed of binary genetic genes in the hidden layer(s) of networks. This method can take both advantages of gradient-free optimization and gradient-based optimization methods, of which the former is effective for problems with multiple local minima, while the latter can quickly find local minima. In this model hidden units are partially gated (opened or closed) by a combination of genes (a chromosome vector) and the optimal combination of genes is learned through a genetic algorithm.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7446-genetic-gated-networks-for-deep-reinforcement-learning",
            "Publication name": "Genetic-Gated Networks for Deep Reinforcement Learning",
            "Released at": "NIPS",
            "Interacts with": "A2C, PPO",
            "Authors": "Simyung Chang, John Yang, Jaeseok Choi, Nojun Kwak",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "UCRL",
            "Using for": "Reinforcement Learning",
            "Based on": "MDP",
            "Method": "Near-optimal regret bounds for reinforcement learning",
            "Description": "For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s1,s2 there is a policy which moves from s1 to s2 in at most D steps (on average). We present a reinforcement learning algorithm with total regret O(DSAT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. This bound holds with high probability. We also present a corresponding lower bound of Omega(DSAT) on the total regret of any learning algorithm. Both bounds demonstrate the utility of the diameter as structural parameter of the MDP.",
            "Publication date": 2010.0,
            "Publication link": "http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf",
            "Publication name": "Near-optimal regret bounds for reinforcement learning",
            "Released at": "JMLR J. Mach. Learn. Res., 11:1563\u20131600,",
            "Interacts with": "NaN",
            "Authors": "T. Jaksch, R. Ortner, and P. Auer.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LINUCRL",
            "Using for": "Recommender systems",
            "Based on": "UCRL",
            "Method": "Linear Upper-Confidence bound for Reinforcement Learning",
            "Description": "LINUCRL exploits the linear structure of the reward function and the deterministic and known transition function. The core idea of LINUCRL is to construct confidence intervals on the reward function and apply the optimism-in-face-of-uncertainty principle to compute an optimistic policy.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018",
            "Publication name": "Fighting Boredom in Recommender Systems with Linear Reinforcement Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Romain WARLOP, Alessandro Lazaric, J\u00e9r\u00e9mie Mary",
            "Code link": "None",
            "Data": "Movielens-100k"
        },
        {
            "id": "EAFHDM",
            "Using for": "Sequential decision making",
            "Based on": "None",
            "Method": "Enhancing the Accuracy and Fairness of Human Decision Making",
            "Description": "Authors proposed a set of practical algorithms to improve the utility and fairness of a sequential decision making process, where each decision is taken by a human expert, who is selected from a pool experts. Experiments on synthetic data and real jail-or-release decisions by judges show that our algorithms are able to mitigate imperfect human decisions due to limited experience.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7448-enhancing-the-accuracy-and-fairness-of-human-decision-making",
            "Publication name": "Enhancing the Accuracy and Fairness of Human Decision Making",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Isabel Valera, Adish Singla, Manuel Gomez Rodriguez",
            "Code link": "https://github.com/Networks-Learning/FairHumanDecisions.",
            "Data": "COMPAS data"
        },
        {
            "id": "TEMPR",
            "Using for": "Reinforcement Learning",
            "Based on": "None",
            "Method": "Temporal regularization",
            "Description": "Effectively, temporal regularization considers smoothing over the trajectory, whereby the estimate of the value function at one state is assumed to be related to the value function at the state that typically occur before it in the trajectory. This structure arises naturally out of the fact that the value at each state is estimated using the Bellman equation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7449-temporal-regularization-for-markov-decision-process",
            "Publication name": "Temporal Regularization for Markov Decision Process",
            "Released at": "NIPS",
            "Interacts with": "MDP",
            "Authors": "Pierre Thodoroff, Audrey Durand, Joelle Pineau, Doina Precup",
            "Code link": "https://github.com/pierthodo/temporal_regularization.",
            "Data": "ATARI Games"
        },
        {
            "id": "MARBL",
            "Using for": "Classification",
            "Based on": "NaN",
            "Method": "Margin-based Losses",
            "Description": "In many classification procedures, the classification function is obtained by minimizing a certain empirical risk on the training sample. The classification is then based on the sign of the classification function. In recent years, there have been a host of classification methods proposed that use different margin-based loss functions. The margin-based loss functions are often motivated as upper bounds of the misclassification loss, but this can not explain the statistical properties of the classification procedures. We show that a large family of margin- based loss functions are Fisher consistent for classification. That is, the population minimizer of the loss function leads to the Bayes optimal rule of classification. Our result covers almost all margin-based loss functions that have been proposed in the literature. We give an inequality that links the Fisher consistency of margin-based loss functions with the consistency of methods based on these loss functions. We use this inequality to obtain the rate of convergence for the method of regularization based on a class of margin based loss functions.",
            "Publication date": 2004.0,
            "Publication link": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.6888&rep=rep1&type=pdf",
            "Publication name": "A Note on Margin-based Loss Functions in Classification",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Yi Lin",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PLPMBL",
            "Using for": "Semi-supervised Learning",
            "Based on": "MARBL",
            "Method": "The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning",
            "Description": "Authors shown that for the class of convex margin-based losses, the fact whether they are decreasing or not plays a key role in whether they admit safe semi-supervised procedures. In particular, they have shown that, without making additional assumptions, it is impossible to construct safe semi-supervised procedures for decreasing losses by deriving what partial assignment of the unlabeled objects leads to the recovery of the supervised classifier from a semi-supervised objective",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7450-the-pessimistic-limits-and-possibilities-of-margin-based-losses-in-semi-supervised-learning",
            "Publication name": "The Pessimistic Limits and Possibilities of Margin-based Losses in Semi-supervised Learning",
            "Released at": "NIPS",
            "Interacts with": "SEMISL",
            "Authors": "Jesse Krijthe, Marco Loog",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "ARS",
            "Using for": "Reinforcement Learning",
            "Based on": "BRS",
            "Method": "Augmented Random Search",
            "Description": "Model-free random search algorithm for training static, linear policies for continuous control problems.  This method, which relies on three augmentations of BRS that build on successful heuristics employed in deep reinforcement learning.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7451-simple-random-search-of-static-linear-policies-is-competitive-for-reinforcement-learning",
            "Publication name": "Simple random search of static linear policies is competitive for reinforcement learning",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Horia Mania, Aurelia Guy, Benjamin Recht",
            "Code link": "None",
            "Data": "MuJoCo"
        },
        {
            "id": "VIMO",
            "Using for": "Information maximization",
            "Based on": "None",
            "Method": "Variational Information Maximization Objective",
            "Description": "The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. Authors approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. ",
            "Publication date": 2003.0,
            "Publication link": "https://www.semanticscholar.org/paper/The-IM-algorithm%3A-a-variational-approach-to-Barber-Agakov/aae4efb3d412d585ea0dec03f933397c93caf989",
            "Publication name": "The im algorithm: a variational approach to information maximization.",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "David Barber and Felix Agakov.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "AIM",
            "Using for": "Text Generation, Adversarial Networks",
            "Based on": "GAN, VIMO",
            "Method": "Adversarial Information Maximization",
            "Description": "A new adversarial learning method for training end-to-end neural response generation models that produce informative and diverse conversational responses. Our approach exploits adversarial training to encourage diversity, and explicitly maximizes a Variational Information Maximization Objective to produce informative responses.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7452-generating-informative-and-diverse-conversational-responses-via-adversarial-information-maximization",
            "Publication name": "Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, Bill Dolan",
            "Code link": "None",
            "Data": "Reddit dataset"
        },
        {
            "id": "LLL",
            "Using for": "Regression",
            "Based on": "None",
            "Method": "Lenstra-Lenstra-Lovasz",
            "Description": "Algorithm to solve regression problem",
            "Publication date": 1982.0,
            "Publication link": "https://www.math.leidenuniv.nl/~hwl/PUBLICATIONS/1982f/art.pdf",
            "Publication name": "Factoring polynomials with rational coefficients.\n",
            "Released at": "Mathematische Annalen",
            "Interacts with": "None",
            "Authors": "Lenstra, A. K., Lenstra, H. W., and Lov\u00e1sz, L",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "LBR",
            "Using for": "Regression",
            "Based on": "LLL",
            "Method": "Lattice-Based Regression",
            "Description": "A new polynomial-time algorithm to solve high dimensional regression problem",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7455-high-dimensional-linear-regression-using-lattice-basis-reduction",
            "Publication name": "High Dimensional Linear Regression using Lattice Basis Reduction",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ilias Zadik, David Gamarnik",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SGR",
            "Using for": "Semantic Segmentation, Cassification",
            "Based on": "None",
            "Method": "Symbolic Graph Reasoning",
            "Description": "Layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7456-symbolic-graph-reasoning-meets-convolutions",
            "Publication name": "Symbolic Graph Reasoning Meets Convolutions",
            "Released at": "NIPS",
            "Interacts with": "CNN",
            "Authors": "Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, Eric P. Xing",
            "Code link": "None",
            "Data": "COCO-Stuff, ADE20K, PASCAL-Context, CIFAR 100"
        },
        {
            "id": "DVAE++",
            "Using for": "Generation",
            "Based on": "CNN, VAE",
            "Method": "Discrete variational autoencoders with overlapping transformations",
            "Description": "A generative model with a global discrete prior and a hierarchy of convolutional continuous variables.",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1802.04920",
            "Publication name": "This paper introduces the overlapping smoothing transformations and shows that these transformations can be used for training discrete variational autoencoder with a directed prior as well as an undirected prior.",
            "Released at": "ICML",
            "Interacts with": "None",
            "Authors": "ArashVahdat, WilliamG.Macready, ZhengbingBian, AmirKhoshaman, EvgenyAndriyash.",
            "Code link": "None",
            "Data": "MNIST, OMNIGLOT, CIFAR 10, Caltech-101"
        },
        {
            "id": "DVAE#",
            "Using for": "Generation",
            "Based on": "DVAE++",
            "Method": "Discrete Variational Autoencoders with Relaxed Boltzmann Priors",
            "Description": "Authors introduce two continuous relaxations of Boltzmann machines and use these relaxations to train a discrete VAE with a Boltzmann prior using the IW bound. They generalize the overlapping transformations of DVAE++ to any pair of distributions with computable probability density function  and cumulative density function. Using these more general overlapping transformations, we propose new smoothing transformations using mixtures of Gaussian and power-function  distributions.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7457-dvae-discrete-variational-autoencoders-with-relaxed-boltzmann-priors",
            "Publication name": "DVAE#: Discrete Variational Autoencoders with Relaxed Boltzmann Priors",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Arash Vahdat, Evgeny Andriyash, William Macready",
            "Code link": "https://github.com/QuadrantAI/dvae.",
            "Data": "MNIST, OMNIGLOT"
        },
        {
            "id": "PS3",
            "Using for": "Image Captioning",
            "Based on": "None",
            "Method": "Partially-Specified Sequence Supervision",
            "Description": "A novel algorithm for training sequence models, such as recurrent neural networks, on partially-specified sequences which we represent using finite state automata. In the context of image captioning, this method lifts the restriction that previously required image captioning models to be trained on paired image-sentence corpora only, or otherwise required specialized model architectures to take advantage of alternative data modalities.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7458-partially-supervised-image-captioning",
            "Publication name": "Partially-Supervised Image Captioning",
            "Released at": "NIPS",
            "Interacts with": "RNN",
            "Authors": "Peter Anderson, Stephen Gould, Mark Johnson",
            "Code link": "None",
            "Data": "COCO dataset"
        },
        {
            "id": "3D-SDN",
            "Using for": "Scene Manipulation",
            "Based on": "ENCDR, DCDR",
            "Method": "3D scene de-rendering networks",
            "Description": "Though this work authors mainly focuses on 3D-aware scene manipulation, the learned representations could be potentially useful for various tasks such as image reasoning, captioning, and analogy-making. Is an encoder-decoder framework, first de-render (encode) an image into disentangled representations for semantic, textural, and geometric information. Then, a renderer (decoder) reconstructs the image from the representation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7459-3d-aware-scene-manipulation-via-inverse-graphics",
            "Publication name": "3D-Aware Scene Manipulation via Inverse Graphics",
            "Released at": "NIPS",
            "Interacts with": "REINFORCE",
            "Authors": "Shunyu Yao, Tzu Ming Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, Bill Freeman, Josh Tenenbaum",
            "Code link": "None",
            "Data": "Virtual KITTI, Cityscapes"
        },
        {
            "id": "F",
            "Using for": "Quality measures",
            "Based on": "MCMC",
            "Method": "feature Stein discrepancies",
            "Description": "A new family of quality measures that can be cheaply approximated using importance sampling. This method combine the computational benefits of linear-time discrepancy measures with the convergence-determining properties of quadratic-time Stein discrepancies.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7460-random-feature-stein-discrepancies",
            "Publication name": "Random Feature Stein Discrepancies",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Jonathan Huggins, Lester Mackey",
            "Code link": "None",
            "Data": "Syntetic data"
        },
        {
            "id": "SVRG",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "Stochastic Variance-Reduced Gradient",
            "Description": "Variance-reduction technique for improving the sample complexity of SGD given access to a stream samples, as well as the ability to compute exact gradients in a potentially expensive operation.",
            "Publication date": 2013.0,
            "Publication link": "https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf",
            "Publication name": "Accelerating stochastic gradient descent using predictive variance reduction.",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Rie Johnson, Tong Zhang.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SVRGOL",
            "Using for": "Optimization",
            "Based on": "SGD, SVRG",
            "Method": "Stochastic Variance-Reduced Gradient with Online Learning",
            "Description": "A generic stochastic optimization framework which combines adaptive online learning algorithms with variance reduction to obtain communication efficiency in parallel architectures.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7461-distributed-stochastic-optimization-via-adaptive-sgd",
            "Publication name": "Distributed Stochastic Optimization via Adaptive SGD",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ashok Cutkosky, R\u00f3bert Busa-Fekete",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "TSPRE",
            "Using for": "Metric",
            "Based on": "None",
            "Method": "Time Series Precision",
            "Description": "A new mathematical model to evaluate the accuracy of time series classification algorithms.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7462-precision-and-recall-for-time-series",
            "Publication name": "Precision and Recall for Time Series",
            "Released at": "NIPS",
            "Interacts with": "RNNs",
            "Authors": "Nesime Tatbul, Tae Jun Lee, Stan Zdonik, Mejbah Alam, Justin Gottschlich",
            "Code link": "None",
            "Data": "NYC-Taxi, Twitter-AAPL"
        },
        {
            "id": "TSRE",
            "Using for": "Metric",
            "Based on": "None",
            "Method": "Time Series Recall",
            "Description": "A new mathematical model to evaluate the accuracy of time series classification algorithms.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7462-precision-and-recall-for-time-series",
            "Publication name": "Precision and Recall for Time Series",
            "Released at": "NIPS",
            "Interacts with": "RNNs",
            "Authors": "Nesime Tatbul, Tae Jun Lee, Stan Zdonik, Mejbah Alam, Justin Gottschlich",
            "Code link": "None",
            "Data": "NYC-Taxi, Twitter-AAPL"
        },
        {
            "id": "RECIL",
            "Using for": "Attention",
            "Based on": "None",
            "Method": "Reciprocative Learning",
            "Description": "A reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the classification loss to train deep classifiers, which in themselves learn to attend to temporal robust features. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7463-deep-attentive-tracking-via-reciprocative-learning",
            "Publication name": "Deep Attentive Tracking via Reciprocative Learning",
            "Released at": "NIPS",
            "Interacts with": "ATTN",
            "Authors": "Shi Pu, Yibing Song, Chao Ma, Honggang Zhang, Ming-Hsuan Yang",
            "Code link": "None",
            "Data": "OTB-2013, OTB-2015, VOT-2016 "
        },
        {
            "id": "VIRSMAX",
            "Using for": "Classification",
            "Based on": "EMBD, SMAX",
            "Method": "Virtual Softmax",
            "Description": "A novel yet extremely simple method to enhance the discriminative property of learned features by injecting a dynamic virtual negative class into the original softmax. Injecting virtual class aims to enlarge inter-class margin and compress intra-class distribution by strengthening the decision boundary constraint.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7464-virtual-class-enhanced-discriminative-embedding-learning",
            "Publication name": "Virtual Class Enhanced Discriminative Embedding Learning",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Binghui Chen, Weihong Deng, Haifeng Shen",
            "Code link": "None",
            "Data": "MNIST, SVHN, CIFAR10, CIFAR10100], CUB200, ImageNet32, LFW, SLLFW "
        },
        {
            "id": "ConvLSTM",
            "Using for": "Regression, Classification",
            "Based on": "CNN, LSTM",
            "Method": "Convolutional LSTM ",
            "Description": "The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1506.04214",
            "Publication name": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, Wang-chun Woo",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ATTNConvLSTM",
            "Using for": "Recognition",
            "Based on": "ATTN, ConvLSTM",
            "Method": "Attention Convolution Long Short Term Memory",
            "Description": "The effects of attention in Convolutional LSTM are explored in this paper. Authors evaluation results and previous published results show that the convolutional structures in the gates of ConvLSTM do not play the role of spatial attention, even if the gates have independent weight values for each element of the feature maps in the spatial domain. The reduction of the convolutional structures in the three gates results in a better accuracy, a lower parameter size and a lower computational consumption. This leads to a new variant of LSTM, in which the convolutional structures are only added to the input-to-state transition, and the gates still stick to their own responsibility and superiority for long-term temporal fusion. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7465-attention-in-convolutional-lstm-for-gesture-recognition",
            "Publication name": "Attention in Convolutional LSTM for Gesture Recognition",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": " Liang Zhang, Guangming Zhu, Lin Mei, Peiyi Shen, Syed Afaq Ali Shah, Mohammed Bennamoun",
            "Code link": "https://github.com/GuangmingZhu/AttentionConvLSTM",
            "Data": "Jester, IsoGD"
        },
        {
            "id": "DENSENET",
            "Using for": "Recognition, Classification",
            "Based on": "CNN, MLP",
            "Method": "Densely connected convolutional networks.",
            "Description": "Connects each layer\nto every other layer in a feed-forward fashion.",
            "Publication date": 2016.0,
            "Publication link": "https://arxiv.org/pdf/1608.06993.pdf",
            "Publication name": "Densely Connected Convolutional Networks",
            "Released at": "arXiv",
            "Interacts with": "NaN",
            "Authors": "Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten.",
            "Code link": "https://github.com/liuzhuang13/DenseNet.",
            "Data": "CIFAR-10, CIFAR-100, SVHN, ImageNet"
        },
        {
            "id": "PELEENET",
            "Using for": "Recognition, Classification",
            "Based on": "DENSENET",
            "Method": "Pelee Nentwork",
            "Description": "By combining efficient architecture design with mobile GPU and hardware-specified optimized runtime libraries, this model can to perform real-time prediction for image classification and object detection tasks on mobile devices. For example, Pelee, can run 23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2 with high accuracy.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7466-pelee-a-real-time-object-detection-system-on-mobile-devices",
            "Publication name": "Pelee: A Real-Time Object Detection System on Mobile Devices",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jun Wang, Tanner Bohn, Charles Ling",
            "Code link": "https://github.com/Robert-JunWang/Pelee",
            "Data": " ILSVRC 2012, MS COCO, PASCAL VOC2007"
        },
        {
            "id": "BMS",
            "Using for": "Boundary Detection",
            "Based on": "BAYESM",
            "Method": "Bayesian model selection",
            "Description": "This method can consistently identify multiple mean changes in a data sequence, which effectively removes the flat points without sacrificing the detection accuracy. BMS method is particularly useful when the data sequence contains spike points that are not of interest as they are not real change points. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7468-bayesian-model-selection-approach-to-boundary-detection-with-non-local-priors",
            "Publication name": "Bayesian Model Selection Approach to Boundary Detection with Non-Local Priors",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Fei Jiang, Guosheng Yin, Francesca Dominici",
            "Code link": "None",
            "Data": "MRgRT data"
        },
        {
            "id": "HSG-HT",
            "Using for": "Optimization",
            "Based on": "HSGD",
            "Method": "Hybrid Stochastic Gradient Hard Thresholding",
            "Description": "This method that can be provably shown to have sample-size-independent gradient evaluation and hard thresholding complexity bounds. Specifically, authors prove that the stochastic gradient evaluation complexity of HSG-HT scales linearly with inverse of sub-optimality and its hard thresholding complexity scales logarith- mically.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7469-efficient-stochastic-gradient-hard-thresholding",
            "Publication name": "Efficient Stochastic Gradient Hard Thresholding",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Pan Zhou, Xiaotong Yuan, Jiashi Feng",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "PCA",
            "Using for": "Dimensionality reduction",
            "Based on": "SVD",
            "Method": "Principal component analysis",
            "Description": "Statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.",
            "Publication date": 1901.0,
            "Publication link": "http://pca.narod.ru/pearson1901.pdf",
            "Publication name": "On Lines and Planes of Closest Fit to Systems of Points in Space\"",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Pearson K",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ACA",
            "Using for": "Unsupervised learning",
            "Based on": "PCA",
            "Method": "Additive Component Analysis",
            "Description": "This technique fits a smooth, low dimensional manifold to data and learns a mapping from it to the input space.",
            "Publication date": 2017.0,
            "Publication link": "http://www.humansensing.cs.cmu.edu/sites/default/files/cvpr2017aca.pdf",
            "Publication name": "Additive component analysis.",
            "Released at": "CVPR",
            "Interacts with": "None",
            "Authors": "C. Murdock and F. D. l. Torre.",
            "Code link": "None",
            "Data": "MNIST"
        },
        {
            "id": "SplineNets",
            "Using for": "Classification, recognition, detection",
            "Based on": "CNN, ACA",
            "Method": "Spline Netsworks",
            "Description": "A practical and novel approach for using conditioning in convolutional neural networks. SplineNets are continuous generalizations of neural decision graphs, and they can dramatically reduce runtime complexity and computation costs of convolutional neural networks, while maintaining or even increasing accuracy.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7470-splinenets-continuous-neural-decision-graphs",
            "Publication name": "SplineNets: Continuous Neural Decision Graphs",
            "Released at": "NIPS",
            "Interacts with": "RESN",
            "Authors": "Cem Keskin, Shahram Izadi",
            "Code link": "None",
            "Data": "CIFAR-10"
        },
        {
            "id": "GZSL",
            "Using for": "Zero-shot learning",
            "Based on": "GAN, VAE",
            "Method": "Generalized zero-shot learning",
            "Description": "Using a conditional VAE based architecture and augmenting it with discriminator-driven feedback mechanism enables this model to generate high-quality, class-specific exemplars from the unseen classes (and, if desired, also from seen classes). These exemplars can then be used in any classification model.",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/pdf/1712.03878.pdf",
            "Publication name": "Generalized zero-shot learning via synthesized examples.",
            "Released at": "CVPR",
            "Interacts with": "Classificators",
            "Authors": "V. Kumar Verma, G. Arora, A. Mishra, P. Rai.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "DCN",
            "Using for": "Zero-shot learning",
            "Based on": "GZSL",
            "Method": "Deep Calibration Network",
            "Description": "This approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7471-generalized-zero-shot-learning-with-deep-calibration-network",
            "Publication name": "Generalized Zero-Shot Learning with Deep Calibration Network",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Shichen Liu, Mingsheng Long, Jianmin Wang, Michael I. Jordan",
            "Code link": "None",
            "Data": "AwA, CUB, SUN, aPY."
        },
        {
            "id": "GP",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Gaussian Process",
            "Description": "Stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\n\nA machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the kernel function) to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information\u2014it is a one-dimensional Gaussian distribution (which is the marginal distribution at that point).\nFor some kernel functions, matrix algebra can be used to calculate the predictions using the technique of kriging. When a parameterised kernel is used, optimisation software is typically used to fit a Gaussian process model.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Gaussian_process",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "OTMANN",
            "Using for": "Neural Architecture Search",
            "Based on": "None",
            "Method": "Optimal Transport Metrics for Architectures of Neural Networks",
            "Description": "(pseudo-)distance for neural network architectures that can be computed efficiently via an optimal transport program. To motivate this distance, note that the performance of a neural network is determined by the amount of computation at each layer, the types of these operations, and how the layers are connected. A meaningful distance should account for these factors. To that end, OTMANN is defined as the minimum of a matching scheme which attempts to match the computation at the layers of one network to the layers of the other. Authors incur penalties for matching layers with different types of operations or those at structurally different positions. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7472-neural-architecture-search-with-bayesian-optimisation-and-optimal-transport",
            "Publication name": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport",
            "Released at": "NIPS",
            "Interacts with": "NASBOT",
            "Authors": "Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, Eric P. Xing",
            "Code link": "github.com/kirthevasank/nasbot.",
            "Data": "None"
        },
        {
            "id": "NASBOT",
            "Using for": "Neural Architecture Search",
            "Based on": "GP, OTMANN",
            "Method": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport",
            "Description": "Gaussian process based Bayesian Optimisation framework for neural architecture search. To accomplish this, authors develop a distance metric in the space of neural network architectures which can be computed efficiently via an optimal transport program. This distance might be of independent interest to the deep learning community as it may find applications outside of Bayesian Optimisation. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7472-neural-architecture-search-with-bayesian-optimisation-and-optimal-transport",
            "Publication name": "Neural Architecture Search with Bayesian Optimisation and Optimal Transport",
            "Released at": "NIPS",
            "Interacts with": "MLP, CNN, (DNNs)",
            "Authors": "Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, Eric P. Xing",
            "Code link": "github.com/kirthevasank/nasbot.",
            "Data": "None"
        },
        {
            "id": "GQEs",
            "Using for": "Embedding",
            "Based on": "EMBD",
            "Method": "Graph Query Embeddings",
            "Description": "An embedding-based framework that can efficiently make predictions about conjunctive queries on incomplete knowledge graphs. The key idea behind GQEs is that we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. After training, we can use the model to predict which nodes are likely to satisfy any valid conjunctive query, even if the query involves unobserved edges.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs",
            "Publication name": "Embedding Logical Queries on Knowledge Graphs",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure Leskovec",
            "Code link": "https://github.com/williamleif/graphqembed.",
            "Data": "Bio data, Reddit data"
        },
        {
            "id": "LORPNB",
            "Using for": "Learning Optimal Reserve Price",
            "Based on": "None",
            "Method": "Learning Optimal Reserve Price against Non-myopic Bidders",
            "Description": "Authors consider the problem of learning optimal reserve price in repeated auctions against non-myopic bidders, who may bid strategically in order to gain in future rounds even if the single-round auctions are truthful.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7474-learning-optimal-reserve-price-against-non-myopic-bidders",
            "Publication name": "Learning Optimal Reserve Price against Non-myopic Bidders",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Jinyan Liu, Zhiyi Huang, Xiangning Wang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "TS2CE",
            "Using for": "Obect detection",
            "Based on": "RNN, ENCDR, DCDR",
            "Method": "Two-Stage Sequential Context Encoding",
            "Description": "A new approach for duplicate removal that is important in object detection. Authors applied RNN with global attention and context gate structure to sequentially encode context information existing in all object proposals. The decoder selects appropriate proposals as final output.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7475-sequential-context-encoding-for-duplicate-removal",
            "Publication name": "Sequential Context Encoding for Duplicate Removal",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Lu Qi, Shu Liu, Jianping Shi, Jiaya Jia",
            "Code link": "None",
            "Data": "COCO"
        },
        {
            "id": "MVCL",
            "Using for": "Latent 3D Keypoints",
            "Based on": "None",
            "Method": "Multi-View Consistency Loss",
            "Description": "Loss that measures the discrepancy between the two sets of points under the ground truth transformation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7476-discovery-of-latent-3d-keypoints-via-end-to-end-geometric-reasoning",
            "Publication name": "Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning",
            "Released at": "NIPS",
            "Interacts with": "KeypointNet",
            "Authors": "Supasorn Suwajanakorn, Noah Snavely, Jonathan J. Tompson, Mohammad Norouzi",
            "Code link": "https://keypointnet.github.io",
            "Data": "Pascal3D+, ShapeNet"
        },
        {
            "id": "RPEL",
            "Using for": "Latent 3D Keypoints",
            "Based on": "None",
            "Method": "Relative Pose Estimation Loss",
            "Description": "This loss penalizes the angular difference between the ground truth rotation R vs. the rotation R\u02c6 recovered from P1 and P2 using orthogonal procrustes.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7476-discovery-of-latent-3d-keypoints-via-end-to-end-geometric-reasoning",
            "Publication name": "Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning",
            "Released at": "NIPS",
            "Interacts with": "KeypointNet",
            "Authors": "Supasorn Suwajanakorn, Noah Snavely, Jonathan J. Tompson, Mohammad Norouzi",
            "Code link": "https://keypointnet.github.io",
            "Data": "Pascal3D+, ShapeNet"
        },
        {
            "id": "KeypointNet",
            "Using for": "Latent 3D Keypoints, 3D pose estimation ",
            "Based on": "RPEL, MVCL",
            "Method": "Keypoint Network",
            "Description": "Extracts 3D keypoints that are optimized for a downstream task, and discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category. Authors demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7476-discovery-of-latent-3d-keypoints-via-end-to-end-geometric-reasoning",
            "Publication name": "Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning",
            "Released at": "NIPS",
            "Interacts with": "CNN",
            "Authors": "Supasorn Suwajanakorn, Noah Snavely, Jonathan J. Tompson, Mohammad Norouzi",
            "Code link": "https://keypointnet.github.io",
            "Data": "Pascal3D+, ShapeNet"
        },
        {
            "id": "VB",
            "Using for": "Inference",
            "Based on": "BO, VARINF",
            "Method": "Variational Bayes",
            "Description": "Variational approximations to Bayesian posteriors are a popular tool for obtaining fast, scalable but\napproximate Bayesian posterior distributions",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1601.00670",
            "Publication name": "Variational Inference: A Review for Statisticians.",
            "Released at": "Journal of the American Statistical Association,",
            "Interacts with": "None",
            "Authors": "David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "aNPL",
            "Using for": "Sampling",
            "Based on": "MC, VB",
            "Method": "Adaptive Nonparametric Learning",
            "Description": "A Bayesian nonparametric approach. At the heart of this approach is the notion of Bayesian updating via randomized objective functions through the posterior bootstrap. The posterior bootstrap acts on an augmented dataset, comprised of data and posterior pseudo-samples, under which randomized maximum likelihood estimators provide a well-motivated quantification of uncertainty while assuming little about the data-generating mechanism.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7477-nonparametric-learning-from-bayesian-models-with-randomized-objective-functions",
            "Publication name": "Nonparametric learning from Bayesian models with randomized objective functions",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Simon Lyddon, Stephen Walker, Chris C. Holmes",
            "Code link": "None",
            "Data": "Statlog German Credit dataset"
        },
        {
            "id": "SEGA",
            "Using for": "Optimization",
            "Based on": "SGD",
            "Method": "SkEtched GrAdient",
            "Description": "This method progressively throughout its iterations builds a variance- reduced estimate of the gradient from random linear measurements (sketches) of the gradient. In each iteration, SEGA updates the current estimate of the gradi- ent through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent meth- ods, such as coordinate descent, SEGA can be used for optimization problems with a non-separable proximal term.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7478-sega-variance-reduction-via-gradient-sketching",
            "Publication name": "SEGA: Variance Reduction via Gradient Sketching",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Filip Hanzely, Konstantin Mishchenko, Peter Richtarik",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "PCCoder",
            "Using for": "Automatic program synthesis",
            "Based on": "EMBD, MLP, POOL",
            "Method": "Predict and Collect Coder",
            "Description": "Authors consider the problem of generating automatic code given sample input-output pairs. Neural network trained to map from the current state and the outputs to the program\u2019s next statement. The neural network optimizes multiple tasks concurrently: the next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7479-automatic-program-synthesis-of-long-programs-with-a-learned-garbage-collector",
            "Publication name": "Automatic Program Synthesis of Long Programs with a Learned Garbage Collector",
            "Released at": "NIPS",
            "Interacts with": "RESN",
            "Authors": "Amit Zohar, Lior Wolf",
            "Code link": "https: //github.com/amitz25/PCCoder",
            "Data": "Domain Specific Language"
        },
        {
            "id": "OST",
            "Using for": "One-Shot Learning",
            "Based on": "ENCDR, DCDR, VAE, GAN",
            "Method": "One Shot Translation",
            "Description": "This method uses the two domains asymmetrically and employs two steps. First, a variational autoencoder is constructed for domain B. This allows us to encode samples from domain B effectively as well as generate new samples based on random latent space vectors. In order to encourage generality, further augment B with samples produced by a slight rotation and with a random horizontal translation.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7480-one-shot-unsupervised-cross-domain-translation",
            "Publication name": "One-Shot Unsupervised Cross Domain Translation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sagie Benaim, Lior Wolf",
            "Code link": "https://github.com/sagiebenaim/OneShotTranslation.",
            "Data": "SVHN, MNSIT"
        },
        {
            "id": "VCL",
            "Using for": "Optimization, Regularization",
            "Based on": "BN",
            "Method": "The Variance Constancy Loss\n",
            "Description": "This method employs small subsets of the mini-batch and seems to perform as well or better than batchnorm on the standard benchmarks tested. It therefore holds the promise of improving conditioning without imposing constraints on the optimization process. Since VCL is a regulariza- tion term and not a normalization mechanism, and since the statistics of sample moments is well understood, the new method could be compatible with a wider variety of optimization methods in comparison to bachnorm. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7481-regularizing-by-the-variance-of-the-activations-sample-variances",
            "Publication name": "Regularizing by the Variance of the Activations' Sample-Variances",
            "Released at": "NIPS",
            "Interacts with": "MLP, CNN",
            "Authors": "Etai Littwin, Lior Wolf",
            "Code link": "None",
            "Data": "CIFAR 10, CIFAR 100"
        },
        {
            "id": "SVM-cone",
            "Using for": "Clustering",
            "Based on": "SVM",
            "Method": "Support Vector Machine cone",
            "Description": "Authors showed that many distinct models for overlapping clustering can be placed under one general framework, where the data matrix is a noisy version of an ideal matrix and each row is a non-negative weighted sum of \u201cexemplars.\u201d In other words, the connection probabilities of one node to others in a network is a non-negative combination of the connection probabilities of K \u201cpure\u201d nodes to others in the network. Each pure node is an examplar of a single community, and we require one pure node from each of the K communities. This geometrically corresponds to a cone, with the pure nodes being its corners.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7482-overlapping-clustering-models-and-one-class-svm-to-bind-them-all",
            "Publication name": "Overlapping Clustering Models, and One (class) SVM to Bind Them All",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Xueyu Mao, Purnamrita Sarkar, Deepayan Chakrabarti",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "MGPP",
            "Using for": "Regression",
            "Based on": "GP",
            "Method": "Multi-output Gaussian process priors",
            "Description": "Multi-output Gaussian process priors which satisfy linear differential equations. This approach attempts to parametrize all solutions of the equations using Gr\u00f6bner bases. If successful, a push forward Gaussian process along the paramerization is the desired prior.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7483-algorithmic-linearly-constrained-gaussian-processes",
            "Publication name": "Algorithmic Linearly Constrained Gaussian Processes",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Markus Lange-Hegermann",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "WGAN",
            "Using for": "Adversarial Networks",
            "Based on": "WASSD, GAN",
            "Method": "Wassershtain Generative Adversarial Network",
            "Description": "An alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.",
            "Publication date": 2017.0,
            "Publication link": "http://proceedings.mlr.press/v70/arjovsky17a.html",
            "Publication name": "Wasserstein Generative Adversarial Networks",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DeepExposure",
            "Using for": "Expose Photos",
            "Based on": "DDPG, WGAN, DIS",
            "Method": "Expose Photos with Asynchronously Reinforced Adversarial Learning",
            "Description": "Based on deep reinforcement learning, authors propose an exposure-blending-based framework which can retouch local areas of images flexibly with only exposure operation.There exists a non-differential operation in the whole process. They leverage the generative loss approximation to make it differential and asynchronous learning to make the learning process stable. By asynchronously reinforced adversarial learning. The asynchronous update policy gradients aid the algorithm in tuning sequential exposures while adversarial learning facilitates to learn aesthetic evaluation function. The whole pipeline proceeds with image-unpaired training and can be efficiently performed with super-resolution in practice. This algorithm do not directly generate any pixels and can well preserve the details of the original image. For computers of limited memory when training, authors devise an algorithm to reuse the discriminator as the value function, effectively reducing memory occupation and accelerating the training speed.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7484-deepexposure-learning-to-expose-photos-with-asynchronously-reinforced-adversarial-learning",
            "Publication name": "DeepExposure: Learning to Expose Photos with Asynchronously Reinforced Adversarial Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Runsheng Yu, Wenyu Liu, Yasen Zhang, Zhi Qu, Deli Zhao, Bo Zhang",
            "Code link": "None",
            "Data": "MIT-Adobe FiveK"
        },
        {
            "id": "NORMT",
            "Using for": "Normalization",
            "Based on": "BN",
            "Method": "Norm matters",
            "Description": "In this work, authors analyzed common normalization techniques used in deep learning models, with BN as their prime representative. Authors considered a novel perspective on the role of these methods, as tools to decouple the weights\u2019 norm from training objective. This perspective allowed us to re-evaluate the necessity of regularization methods such as weight decay, and to suggest new methods for normalization, targeting the computational, numerical and task-specific deficiencies of current techniques.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7485-norm-matters-efficient-and-accurate-normalization-schemes-in-deep-networks",
            "Publication name": "Norm matters: efficient and accurate normalization schemes in deep networks",
            "Released at": "NIPS",
            "Interacts with": "TRNSFRMR, RESN",
            "Authors": "Elad Hoffer, Ron Banner, Itay Golan, Daniel Soudry",
            "Code link": "https://github.com/eladhoffer/norm_matters",
            "Data": "WMT14, ImageNet"
        },
        {
            "id": "RANSAC",
            "Using for": "Dimensionality reduction",
            "Based on": "None",
            "Method": "Random Sampling And Consensus",
            "Description": "This algorithm Given a time budget, computes at each iteration a d-dimensional subspace as the span of d randomly chosen points, and outputs the subspace that agrees with the largest number of points.",
            "Publication date": 1981.0,
            "Publication link": "https://www.sri.com/sites/default/files/publications/ransac-publication.pdf",
            "Publication name": "RANSAC random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography.",
            "Released at": "Communications of the ACM",
            "Interacts with": "None",
            "Authors": "M. A. Fischler and R. C. Bolles",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "DPCP",
            "Using for": "Dimensionality reduction",
            "Based on": "PCA",
            "Method": "Dual Principal Component Pursuit",
            "Description": "This method can provably handle subspaces of high dimension by solving a non-convex L1 optimization problem on the sphere. Towards that end, the main idea of Dual Principal Component Pursuit is to first compute a hyperplane H1 that contains all the inliers X . Such a hyperplane can be used to discard a potentially very large number of outliers, after which a method such as Random Sampling And Consensus may successfully be applied to the reduced dataset ",
            "Publication date": 2010.0,
            "Publication link": "https://arxiv.org/abs/1010.4237",
            "Publication name": "Robust pca via outlier pursuit.",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "H. Xu, C. Caramanis, and S. Sanghavi.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "DPCP-PSGM",
            "Using for": "Dimensionality reduction",
            "Based on": "DPCP",
            "Method": "Dual Principal Component Pursuit Projected Sub-Gradient Method",
            "Description": "Method for solving the Dual Principal Component Pursuit problem and show that it achieves linear convergence even though the underlying optimization problem is non-convex and non-smooth.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7486-dual-principal-component-pursuit-improved-analysis-and-efficient-algorithms",
            "Publication name": "Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Zhihui Zhu, Yifan Wang, Daniel Robinson, Daniel Naiman, Rene Vidal, Manolis Tsakiris",
            "Code link": "None",
            "Data": "KITTI dataset"
        },
        {
            "id": "MULAN",
            "Using for": "Blind and off-grid recovery of echo locations",
            "Based on": "None",
            "Method": "MULtichannel ANnihilation",
            "Description": "First method enabling blind and off-grid recovery of echo locations and weights from discrete-time multichannel measurements, to the best of the authors\u2019 knowledge.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7487-mulan-a-blind-and-off-grid-method-for-multichannel-echo-retrieval",
            "Publication name": "MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Helena Peic Tukuljac, Antoine Deleforge, Remi Gribonval",
            "Code link": "https://github.com/epfl-lts2/mulan.",
            "Data": "None"
        },
        {
            "id": "HRMC",
            "Using for": "Recommender systems",
            "Based on": "None",
            "Method": "High-rank matrix completion",
            "Description": "Subspace clustering with missing data. This more general model assumes that each column of X comes from one of several low-rank matrices, thus allowing several types of users.",
            "Publication date": 2016.0,
            "Publication link": "http://proceedings.mlr.press/v48/pimentel-alarcon16.pdf",
            "Publication name": "The information-theoretic requirements of subspace cluster- ing with missing data",
            "Released at": "ICML",
            "Interacts with": "None",
            "Authors": "D. Pimentel-Alarc\u00f3n and R. Nowak,",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "MMC",
            "Using for": "Recommender systems",
            "Based on": "HRMC",
            "Method": "Mixture Matrix Completion",
            "Description": "Is a accurate model for recommender systems, and brings more flexibility to other completion and clustering problems.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7488-mixture-matrix-completion",
            "Publication name": "Mixture Matrix Completion",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Daniel Pimentel-Alarcon",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "3DCNN",
            "Using for": "Classification",
            "Based on": "CNN",
            "Method": "3d convolutional neural networks.",
            "Description": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1412.0767",
            "Publication name": "Learning spatiotemporal features with 3d convolutional networks.",
            "Released at": "ICCV",
            "Interacts with": "NaN",
            "Authors": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "TRJCTRN",
            "Using for": "Action Recognition",
            "Based on": "3DCNN",
            "Method": "TrajectoryNet",
            "Description": "This method incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the existing temporal convolution. This operation explicitly takes into account the changes in contents caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths, trajectories.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7489-trajectory-convolution-for-action-recognition",
            "Publication name": "Trajectory Convolution for Action Recognition",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": " Yue Zhao, Yuanjun Xiong, Dahua Lin",
            "Code link": "None",
            "Data": "Something-Something V1, Kinetics"
        },
        {
            "id": "DLDL",
            "Using for": "The Description",
            "Based on": "VARINF",
            "Method": "The Description Length of Deep Learning model",
            "Description": "Authors show that the traditional method to estimate Minimum Description Length codelengths in deep learning, variational inference, yields surprisingly inefficient codelengths for deep models, despite explicitly minimizing this criterion. This might explain why variational inference as a regularization method often does not reach optimal test performance. Authors introduce new practical ways to compute tight compression bounds in deep learning models, based on the MDL toolbox they show that pre- quential coding on top of standard learning, yields much better codelengths than variational inference, correlating better with test set performance. Thus, despite their many parame- ters, deep learning models do compress the data well, even when accounting for the cost of describing the model.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7490-the-description-length-of-deep-learning-models",
            "Publication name": "The Description Length of Deep Learning models",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "L\u00e9onard Blier, Yann Ollivier",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "NECB",
            "Using for": "Linear Contextual Bandit Problem",
            "Based on": "None",
            "Method": "Natural Exploration In Contextual Bandits",
            "Description": "A greedy algorithm that can be rate optimal in cumulative regret for a two-armed contextual bandit as long as the contexts satisfy covariate diversity. Greedy algorithms are significantly preferable when exploration is costly or unethical. Furthermore, the greedy algorithm is entirely parameter-free, which makes it desirable in settings where tuning is difficult or where there is limited knowledge of problem parameters",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/pdf/1704.09011.pdf",
            "Publication name": "Exploiting the Natural Exploration In Contextual Bandits",
            "Released at": "arXiv",
            "Interacts with": "None",
            "Authors": "H. Bastani, M. Bayati,  K. Khosravi.",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SAGA",
            "Using for": "Linear Contextual Bandit Problem",
            "Based on": "NECB",
            "Method": "Smoothed Analysis of the Greedy Algorithm",
            "Description": "The authors goal is to show that the greedy algorithm achieves no regret against any perturbed adversary in both the single-parameter and multiple-parameter settings. The key idea is to show that the distribution on contexts generated by perturbed adversaries satisfy certain conditions which suffice to prove a regret bound.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7491-a-smoothed-analysis-of-the-greedy-algorithm-for-the-linear-contextual-bandit-problem",
            "Publication name": "A Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sampath Kannan, Jamie H. Morgenstern, Aaron Roth, Bo Waggoner, Zhiwei Steven Wu",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "AP",
            "Using for": "Decomposable Submodular Function Minimization",
            "Based on": "None",
            "Method": "Alternating Projection",
            "Description": "Alternating projections is a very simple algorithm for computing a point in the intersection of some convex sets, using a sequence of projections onto the sets. Like a gradient or subgradient method, alternating projections can be slow, but the method can be useful when we have some efficient method, such as an analytical formula, for carrying out the projections. In these notes, we use only the Euclidean norm, Euclidean distance, and Euclidean projection.",
            "Publication date": 2014.0,
            "Publication link": "https://papers.nips.cc/paper/6825-inhomogeneous-hypergraph-clustering-with-applications.pdf",
            "Publication name": "On the convergence rate of decomposable submod- ular function minimization",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "R. Nishihara, S. Jegelka, and M. I. Jordan, ",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "IAP",
            "Using for": "Decomposable Submodular Function Minimization",
            "Based on": "AP",
            "Method": "The Incidence Relation Alternating Projection",
            "Description": "New approach to decomposable submodular function minimization that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions, and when properly utilized, they allow for improving the convergence rates of decomposable submodular function minimization solvers.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7492-revisiting-decomposable-submodular-function-minimization-with-incidence-relations",
            "Publication name": "Revisiting Decomposable Submodular Function Minimization with Incidence Relations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Pan Li, Olgica Milenkovic",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "PADCOD",
            "Using for": "Clustering,  Outlier Detection",
            "Based on": "PAC",
            "Method": "Practical Algorithm for Distributed Clustering and Outlier Detection",
            "Description": "In this paper authors consider the collaborative PAC learning problem. They have proved the optimal overhead ratio and sample complexity, and conducted experimental studies to show the superior performance of our proposed algorithms.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7493-a-practical-algorithm-for-distributed-clustering-and-outlier-detection",
            "Publication name": "A Practical Algorithm for Distributed Clustering and Outlier Detection",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Jiecao Chen, Erfan Sadeqi Azer, Qin Zhang",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "SHPNT",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "ShapeNet",
            "Description": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1512.03012",
            "Publication name": "ShapeNet: an information-rich 3D model repository",
            "Released at": "arXiv",
            "Interacts with": "NaN",
            "Authors": "Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GenRe",
            "Using for": "Reconstruct Shapes",
            "Based on": "SHPNT",
            "Method": "Generalizable Reconstruction",
            "Description": "Thus  model comprises three cascaded, learnable modules connected by fixed geometric projections. First, a single-view depth estimator predicts depth from a 2D image (2D\u21922.5D); the depth map is then projected into a spherical map (2.5D\u2192S). Second, a spherical map inpainting network inpaints the partial spherical map (cS\u2192S); the inpainted spherical map is then projected into 3D voxels (2.5D\u21923D). Finally, authors introduce an additional voxel refinement network to refine the estimated 3D shape in voxel space. This neural modules only have to model object geometry for reconstruction, without having to learn geometric projections. This enhances generalizability, along with several other factors: during training, modularized design forces each module of the network to use features from the previous module, instead of directly memorizing shapes from the training classes; also, each module only predicts outputs that are in the same domain as its inputs (image-based or voxel-based), which leads to more regular mappings.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7494-learning-to-reconstruct-shapes-from-unseen-classes",
            "Publication name": "Learning to Reconstruct Shapes from Unseen Classes",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh Tenenbaum, Bill Freeman, Jiajun Wu",
            "Code link": "http://genre.csail.mit.edu",
            "Data": "Pix3D"
        },
        {
            "id": "GMM",
            "Using for": "NaN",
            "Based on": "GP",
            "Method": "Gaussian mixture model",
            "Description": "Probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "None",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BourGAN",
            "Using for": "Generation, Adversarial Networks",
            "Based on": "GAN, GP, EMBD",
            "Method": "Bourgain Generative Networks",
            "Description": "Instead of using a standard Gaussian to sample latent space, authors propose to use a Gaussian mixture model constructed using metric embeddings for metric embeddings in both theoretical and machine learning fronts. Unlike all previous methods that require the latent-space dimensionality to be specified a priori, this algorithm automatically determines its dimensionality from the real dataset.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7495-bourgan-generative-networks-with-metric-embeddings",
            "Publication name": "BourGAN: Generative Networks with Metric Embeddings",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Chang Xiao, Peilin Zhong, Changxi Zheng",
            "Code link": "None",
            "Data": "MNIST"
        },
        {
            "id": "PASDP",
            "Using for": "Semidefinite Programs",
            "Based on": "NaN",
            "Method": "Programming Algorithm fo Semidefinite Programs",
            "Description": "In this paper, we present a nonlinear programming algorithm for solving semidefinite programs (SDPs) in standard form. The algorithm\u2019s distinguishing feature is a change of variables that replaces the symmetric, positive semidefinite variable X of the SDP with a rectangular variable R according to the factorization X = RRT . The rank of the factorization, i.e., the number of columns of R, is chosen minimally so as to enhance computational speed while maintaining equivalence with the SDP. Fundamental results concerning the convergence of the algorithm are derived, and encouraging computational results on some large-scale test problems are also presented.",
            "Publication date": 2003.0,
            "Publication link": "http://www.optimization-online.org/DB_FILE/2001/03/296.pdf",
            "Publication name": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization.",
            "Released at": "Mathematical Programming",
            "Interacts with": "NaN",
            "Authors": "S. Burer, R. D. Monteiro",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LASSDP",
            "Using for": "Semidefinite Programs",
            "Based on": "PASP",
            "Method": "Low-rank Approach for Smooth Semidefinite Programs",
            "Description": "Authors considered the low-rank (or Burer\u2013Monteiro) approach to solve equality-constrained semidefinite programs. Key assumptions are that the search space of the semidefinite programs is compact, and  the search space of its low-rank version is smooth (the actual condition is slightly stronger). ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7496-smoothed-analysis-of-the-low-rank-approach-for-smooth-semidefinite-programs",
            "Publication name": "Smoothed analysis of the low-rank approach for smooth semidefinite programs",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Thomas Pumir, Samy Jelassi, Nicolas Boumal",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "OOMDP",
            "Using for": "Reinforcement Learning",
            "Based on": "MDP",
            "Method": "Object-Oriented MDPs",
            "Description": "Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers impor- tant generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the well- known Taxi domain, plus a real-life videogame.",
            "Publication date": 2008.0,
            "Publication link": "http://carlosdiuk.github.io/papers/OORL.pdf",
            "Publication name": "An object-oriented representation for efficient reinforce- ment learning.",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "C. Diuk, A. Cohen, and M. L. Littman.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DOORMAX",
            "Using for": "Reinforcement Learning",
            "Based on": "OOMDP",
            "Method": "Deterministic Object-Oriented Rmax",
            "Description": "Propositional OO-MDPs introduce a learning algorithm called DOORMAX  for deterministic transition dynamics that, under certain assumptions, has provably efficient KWIK bounds. Moreover, DOORMAX is able to learn from multiple hypothesised effect sets and determine the correct effect set for each attribute and action.",
            "Publication date": 2008.0,
            "Publication link": "http://carlosdiuk.github.io/papers/OORL.pdf",
            "Publication name": "An object-oriented representation for efficient reinforcement learning",
            "Released at": "ICML",
            "Interacts with": "None",
            "Authors": "C. Diuk, A. Cohen, and M. L. Littman",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "DOOMDP",
            "Using for": "Reinforcement Learning, Zero-Shot Transfer",
            "Based on": "OOMDP",
            "Method": "Deictic Object-Oriented MDPs",
            "Description": "The key insight behind Deictic OO-MDPs is the concept of deictic predicates. Deictic predicates are grounded only with respect to a central\ndeictic object, therefore that object may relate itself to non-grounded object classes, but not to other grounded objects. Returning to the Sokoban domain, a deictic predicate over boxes allows a specific box to ascertain whether any wall is adjacent to it, but not whether a specific wall is adjacent to it. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7497-zero-shot-transfer-with-deictic-object-oriented-representation-in-reinforcement-learning",
            "Publication name": "Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ofir Marom, Benjamin Rosman",
            "Code link": "None",
            "Data": "Sokoban, Taxi "
        },
        {
            "id": "DOORMAXD",
            "Using for": "Reinforcement Learning, Zero-Shot Transfer",
            "Based on": "DOOMAX, DOOMDP",
            "Method": "Deterministic Object-Oriented Rmax Deictic",
            "Description": "The main difference for DOORMAXD to  DOORMAX is that authors remove the notion of a global failure condition because this depends on a grounded state comprised of grounded objects. Meanwhile the transition dynamics of this representation are schema based to allow for transferability across tasks. Instead they require that all effects apply to a single attribute.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7497-zero-shot-transfer-with-deictic-object-oriented-representation-in-reinforcement-learning",
            "Publication name": "Zero-Shot Transfer with Deictic Object-Oriented Representation in Reinforcement Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Ofir Marom, Benjamin Rosman",
            "Code link": "None",
            "Data": "Sokoban, Taxi "
        },
        {
            "id": "PERT",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Pert-perfect random tree ensembles",
            "Description": "Ensemble classifiers originated in the machine learning community. They work by fitting many individual classifiers and combining them by weighted or unweighted voting. The ensemble classifier is often much more accurate than the individual classifiers from which it is built. In fact, ensemble classifiers are among the most accurate general-purpose classifiers available. We introduce a new ensemble method, PERT, in which each individual classifier is a perfectly-fit classification tree with random selection of splits. Compared to other ensemble methods, PERT is very fast to fit. Given the randomness of the split selection, PERT is surprisingly accurate. Calculations suggest that one reason why PERT works so well is that although the individual tree classifiers are extremely weak, they are almost uncorrelated. The simple probabilistic nature of the classifier lends itself to theoretical analysis. We show that PERT is fitting a continuous posterior probability surface for each class. As such, it can be viewed as a classification-via-regression procedure that fits a continuous interpolating surface. In theory, this surface could be found using a one-shot procedure.",
            "Publication date": 2001.0,
            "Publication link": "https://www.researchgate.net/publication/268424569_PERT-perfect_random_tree_ensembles",
            "Publication name": "Pert-perfect random tree ensembles",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RBCRR",
            "Using for": "NaN",
            "Based on": "PERT",
            "Method": "Risk bounds for classification and regression rules",
            "Description": "Authors conjecture that the simplicial interpolation scheme may provide insights into the properties of interpolating kernel machines and neural networks. In this paper, authors considered two types of algorithms, one based on simplicial interpolation and another based on interpolation by weighted nearest neighbor schemes. It may be useful to think of nearest neighbor schemes as direct methods, not requiring optimization, while our simplicial scheme is a simple example of an inverse method, using (local) matrix inversion to fit the data. Most popular machine learning methods, such as kernel machines, neural networks, and boosting, are inverse schemes. While nearest neighbor and Nadaraya-Watson methods often show adequate performance, they are rarely best-performing algorithms in practice. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-interpolate",
            "Publication name": "Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Mikhail Belkin, Daniel J. Hsu, Partha Mitra",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "SARAH",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Stochastic Variance Reduced Gradient",
            "Description": "NaN",
            "Publication date": 2017.0,
            "Publication link": "http://arxiv.org/abs/1703.00102",
            "Publication name": "A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient",
            "Released at": "arXiv",
            "Interacts with": "DNNs",
            "Authors": "L. M. Nguyen, J. Liu, K. Scheinberg, and M. Tak\u00e1\u010d",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "Prox-SVRG",
            "Using for": "Fast Finite-Sum Minimization",
            "Based on": "SARAH, SVRG",
            "Method": "Prox Stochastic Variance Reduced Gradient",
            "Description": "In this paper, we show that SVRG and SARAH can be modified to be fundamentally faster than all of the other standard algorithms that minimize the sum of n smooth functions.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7499-breaking-the-span-assumption-yields-fast-finite-sum-minimization",
            "Publication name": "Breaking the Span Assumption Yields Fast Finite-Sum Minimization",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Robert Hannah, Yanli Liu, Daniel O'Connor, Wotao Yin",
            "Code link": "None",
            "Data": "Cup 2012"
        },
        {
            "id": "ANT",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Approximate NewTon",
            "Description": "We present a novel Newton-type method for distributed optimization, which is particularly well suited for stochastic optimization and learning problems. For quadratic objectives, the method enjoys a linear rate of convergence which provably \\emph{improves} with the data size, requiring an essentially constant number of iterations under reasonable assumptions. We provide theoretical and empirical evidence of the advantages of our method compared to other approaches, such as one-shot parameter averaging and ADMM.",
            "Publication date": 2014.0,
            "Publication link": "https://arxiv.org/abs/1312.7853",
            "Publication name": "Communication-efficient distributed optimization using an\napproximate Newton-type method",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "Ohad Shamir, Nati Srebro, and Tong Zhang",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GIANT",
            "Using for": "Optimization",
            "Based on": "SGD, ANT",
            "Method": "Globally Improved Approximate Newton",
            "Description": "A practical Newton-type method, for empirical risk minimization in distributed computing environments. In comparison to similar methods, GIANT has three desirable advantages. First, GIANT is guaranteed to converge to high precision in a small number of iterations, provided that the number of training samples, n, is sufficiently large, relative to dm, where d is the number of features and m is the number of partitions. Second, GIANT is very communication efficient in that each iteration requires four or six rounds of communications, each with a complexity of merely O \u0303(d). Third, in contrast to all other alternates, GIANT is easy to use, as it involves tuning one parameter. Empirical studies also showed the superior performance of GIANT as compared several other methods.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7501-giant-globally-improved-approximate-newton-method-for-distributed-optimization",
            "Publication name": "GIANT: Globally Improved Approximate Newton Method for Distributed Optimization",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": " Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, Michael W. Mahoney",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "CRM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Completely random measures",
            "Description": "Completely random measures are distributions over measures with random (finite) total measure",
            "Publication date": 1993.0,
            "Publication link": "https://projecteuclid.org/euclid.pjm/1102992601",
            "Publication name": "Completely random measures.",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "J.F.C Kingman",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CCRMs",
            "Using for": "NaN",
            "Based on": "CRM",
            "Method": "Compound completely random measures",
            "Description": "NaN",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1410.0611",
            "Publication name": "Compound random measures and their use in Bayesian non-parametrics",
            "Released at": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
            "Interacts with": "None",
            "Authors": "J. E. Griffin and F. Leisen.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HIRM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Hawkes Infinite Relational Model",
            "Description": "A new class of dependent random measures which we call compound random measures are proposed and the use of normalized versions of these random measures as priors in Bayesian nonparametric mixture models is considered. Their tractability allows the properties of both compound random measures and normalized compound random measures to be derived. In particular, we show how compound random measures can be constructed with gamma, \u03c3-stable and generalized gamma process marginals. We also derive several forms of the Laplace exponent and characterize dependence through both the L\u00e9vy copula and correlation function. A slice sampler and an augmented P\u00f3lya urn scheme sampler are described for posterior inference when a normalized compound random measure is used as the mixing measure in a nonparametric mixture model and a data example is discussed.",
            "Publication date": 2012.0,
            "Publication link": "https://arxiv.org/abs/1410.0611",
            "Publication name": "Modelling reciprocating relationships with Hawkes processes.",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "C. Blundell, K. Heller, and J. Beck",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HCCRMs",
            "Using for": "Sparsity, Heterogeneity",
            "Based on": "CCRMs, HIRM",
            "Method": "Hawkes Compound Completely random measures",
            "Description": "A novel statistical model for temporal interaction data which captures multiple important features observed in such datasets, and shown that our approach outperforms competing models in link prediction. The model could be extended in several directions. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7502-modelling-sparsity-heterogeneity-reciprocity-and-community-structure-in-temporal-interaction-data",
            "Publication name": "Modelling sparsity, heterogeneity, reciprocity and community structure in temporal interaction data",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Xenia Miscouridou, Francois Caron, Yee Whye Teh",
            "Code link": "None",
            "Data": "Stanford Large Network Dataset Collection"
        },
        {
            "id": "P-Fantom",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Fast constrained submodular maximization",
            "Description": "Can we summarize multi-category data based on user preferences in a scalable manner? Many utility functions used for data summarization satisfy submodularity, a natural diminishing returns property. We cast personalized data summarization as an instance of a general submodular maximization problem subject to multiple constraints. We develop the first practical and FAst coNsTrained submOdular Maximization algorithm, FANTOM, with strong theoretical guarantees. FANTOM maximizes a submodular function (not necessarily monotone) subject to intersection of a p-system and l knapsacks constrains. It achieves a (1 + \u03b5)(p + 1)(2p + 2l + 1)/p approximation guarantee with only O(nrp log(n)/\u03b5) query complexity (n and r indicate the size of the ground set and the size of the largest feasible solution, respectively). We then show how we can use FANTOM for personalized data summarization. In particular, a p-system can model different aspects of data, such as categories or time stamps, from which the users choose. In addition, knapsacks encode users\u2019 constraints including budget or time. In our set of experiments, we consider several concrete applications: movie recommendation over 11K movies, personalized image summarization with 10K images, and revenue maximization on the YouTube social networks with 5000 communities. We observe that FANTOM constantly provides the highest utility against all the baselines.",
            "Publication date": 2016.0,
            "Publication link": "http://proceedings.mlr.press/v48/mirzasoleiman16.html",
            "Publication name": "Fast Constrained Submodular Maximization: Personalized Data Summarization",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BLITS",
            "Using for": "Recommendation system",
            "Based on": "P-Fantom",
            "Method": "BLock ITeration Submodular maximization algorithm",
            "Description": "Authors consider parallelization for applications whose objective can be expressed as maximizing a non-monotone submodular function under a cardinality constraint.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7503-non-monotone-submodular-maximization-in-exponentially-fewer-iterations",
            "Publication name": "Non-monotone Submodular Maximization in Exponentially Fewer Iterations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Eric Balkanski, Adam Breuer, Yaron Singer",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "MAML",
            "Using for": "Few-shot Learning",
            "Based on": "None",
            "Method": "Model-agnostic meta-learning",
            "Description": "This model trains a transferable initialization that is able to quickly adapt to any specific task with one step gradient descent.",
            "Publication date": 2017.0,
            "Publication link": "http://proceedings.mlr.press/v70/finn17a.html.",
            "Publication name": "Model-agnostic meta-learning for fast adaptation of deep networks.",
            "Released at": "ICML",
            "Interacts with": "GAN",
            "Authors": "Chelsea Finn, Pieter Abbeel, and Sergey Levine",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "MetaGAN",
            "Using for": "Few-shot Learning",
            "Based on": "GAN",
            "Method": "Meta Generative Adversrial Network",
            "Description": "By introducing an adversarial generator conditioned on tasks, authors augment vanilla few-shot classification models with the ability to discriminate between real and fake data. The key idea behind MetaGAN is that imperfect generators in GAN models can provide fake data between the manifolds of different real data classes, thus providing additional training signals to the classifier as well as making the decision boundaries much sharper. Different from previous work in semi-supervised few-shot learning, this algorithms can deal with semi-supervision at both sample-level and task-level.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7504-metagan-an-adversarial-approach-to-few-shot-learning",
            "Publication name": "MetaGAN: An Adversarial Approach to Few-Shot Learning",
            "Released at": "NIPS",
            "Interacts with": "MAML, RELN",
            "Authors": "Ruixiang ZHANG, Tong Che, Zoubin Ghahramani, Yoshua Bengio, Yangqiu Song",
            "Code link": "None",
            "Data": "Mini Imagenet"
        },
        {
            "id": "RAPPOR",
            "Using for": "Privacy",
            "Based on": "NaN",
            "Method": "Randomized aggregatable privacy-preserving ordinal response",
            "Description": "The first large-scale deployment of differential privacy inthelocalmodel. A heuristic memoization technique that impedes a certain straightforward attack but does not prevent the differential privacy loss from accumulating linearly in the number of times the protocol is run.",
            "Publication date": 2014.0,
            "Publication link": "https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/42852.pdf",
            "Publication name": "RAPPOR: Randomized Aggregatable Privacy-Preserving Ordinal Response",
            "Released at": "ACM SIGSAC conference on computer and communications security",
            "Interacts with": "NaN",
            "Authors": "\u00dalfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova.",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "THRESH",
            "Using for": "local differential privacy",
            "Based on": "RAPPOR",
            "Method": "THRESH",
            "Description": "In this paper, authors introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. The main idea of THRESH is therefore to update the global estimate only when it might become sufficiently inaccurate, and thus take advantage of the possibly small number of changes in the underlying statistic pt. The challenge is to privately identify when to update the global estimate.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7505-local-differential-privacy-for-evolving-data",
            "Publication name": "Local Differential Privacy for Evolving Data",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Matthew Joseph, Aaron Roth, Jonathan Ullman, Bo Waggoner",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "CDE",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Conditional Density Estimation",
            "Description": "Conditional density functions are a useful way to display uncertainty. This paper investigates nonpara- metric kernel methods for their estimation. The standard estimator is the ratio of the joint density estimate to the marginal density estimate. Our proposal is to instead use a two-step estimator, where the first step consists of estimation of the conditional mean, and the second step consists of estimating the conditional density of the regression error. If most of the dependence is captured by the conditional mean, the second step will require less smoothing, thereby reducing estimation variance.",
            "Publication date": 2004.0,
            "Publication link": "https://www.ssc.wisc.edu/~bhansen/papers/ncde.pdf",
            "Publication name": "Nonparametric Conditional Density Estimation",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Bruce E. Hansen",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GP-CDE",
            "Using for": "Datasets modeling",
            "Based on": "GP, CDE",
            "Method": "Gaussian Processes Conditional Density Estimation",
            "Description": "In this work, authors propose to extend the CDR model\u2019s input with latent variables and use Gaussian processes to map this augmented input onto samples from the conditional distribution. Authors perform Bayesian linear transformations on both input and output spaces to allow for the modeling of high-dimensional inputs and strongly-coupled outputs. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7506-gaussian-process-conditional-density-estimation",
            "Publication name": "Gaussian Process Conditional Density Estimation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Vincent Dutordoir, Hugh Salimbeni, James Hensman, Marc Deisenroth",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "IMPALA",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Importance Weighted Actor-Learner Architectures",
            "Description": "In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have devel- oped a new distributed agent IMPALA (Impor- tance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single- machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effec- tiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive trans- fer between tasks as a result of its multi-task ap- proach.",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1802.01561",
            "Publication name": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, Koray Kavukcuoglu",
            "Code link": "github.com/deepmind/scalable agent.",
            "Data": "NaN"
        },
        {
            "id": "UVFA",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Universal value function approximation",
            "Description": "Value functions are a core component of rein- forcement learning systems. The main idea is to to construct a single function approximator V (s; \u03b8) that estimates the long-term reward from any state s, using parameters \u03b8. In this paper we introduce universal value function approx- imators (UVFAs) V (s, g; \u03b8) that generalise not just over states s but also over goals g. We de- velop an efficient technique for supervised learn- ing of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a re- inforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully gener- alise to previously unseen goals",
            "Publication date": 2015.0,
            "Publication link": "http://proceedings.mlr.press/v37/schaul15.pdf",
            "Publication name": "Universal Value Function Approximators",
            "Released at": "ICML",
            "Interacts with": "NaN",
            "Authors": "Tom Schaul, Dan Horgan, Karol Gregor, David Silver",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MTGRDNT",
            "Using for": "Reinforcement Learning",
            "Based on": "EMBD, UVFA, ",
            "Method": "Meta-gradient",
            "Description": "In this work, authors discussed how to learn the meta-parameters of a return function. Their meta-learning algorithm runs online, while interacting with a single environment, and successfully adapts the return to produce better performance. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7507-meta-gradient-reinforcement-learning",
            "Publication name": "Meta-Gradient Reinforcement Learning",
            "Released at": "NIPS",
            "Interacts with": "A2C, IMPALA",
            "Authors": "Zhongwen Xu, Hado P. van Hasselt, David Silver",
            "Code link": "None",
            "Data": "ATARI"
        },
        {
            "id": "AMLE",
            "Using for": "Supervised learning",
            "Based on": "NaN",
            "Method": "Adaptive Mixture of Local Expert",
            "Description": "Supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.",
            "Publication date": 1991.0,
            "Publication link": "https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf",
            "Publication name": "Adaptive Mixtures of Local Experts",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "R. A. Jacobs",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "OLNN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Outrageously Large Neural Networks",
            "Description": "The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1701.06538",
            "Publication name": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-\nExperts Layer",
            "Released at": "ICLR",
            "Interacts with": "NaN",
            "Authors": "N. Shazeer et al",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MDLRN",
            "Using for": "Image Recognition, Language Modeling.",
            "Based on": "AMLE, OLNN",
            "Method": "Modular Networks",
            "Description": "Authors propose a novel way of training neural networks by automatically decomposing the functionality needed for solving a given task (or set of tasks) into reusable modules. We treat the choice of module as a latent variable in a probabilistic model and learn both the decomposition and module parameters end-to-end by maximizing a variational lower bound of the likelihood.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7508-modular-networks-learning-to-decompose-neural-computation",
            "Publication name": "Modular Networks: Learning to Decompose Neural Computation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Louis Kirsch, Julius Kunze, David Barber\n",
            "Code link": "None",
            "Data": "Penn Treebank,  CIFAR10"
        },
        {
            "id": "CITYNAV",
            "Using for": "Navigation, Reinforcement Learning",
            "Based on": "CNN, ENCDR, LSTM, IMPALA",
            "Method": "Learning to Navigate in Cities Without a Map",
            "Description": "AUthors have presented a city-scale real-world environment for training RL navigation agents, introduced and analysed a new courier task, demonstrated that deep RL algorithms can be applied to problems involving large-scale real-world data, and presented a multi-city neural network agent architecture that demonstrates transfer to new environments.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7509-learning-to-navigate-in-cities-without-a-map",
            "Publication name": "Learning to Navigate in Cities Without a Map",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, koray kavukcuoglu, Andrew Zisserman, Raia Hadsell",
            "Code link": "https://sites.google.com/view/learn-navigate-cities-nips18.",
            "Data": "None"
        },
        {
            "id": "PSL",
            "Using for": "Private Learning",
            "Based on": "NaN",
            "Method": "Private sequential learning.",
            "Description": "We formulate a private learning model to study an intrinsic tradeoff between privacy and query complexity in sequential learning. Our model involves a learner who aims to determine a scalar value, v\u2217, by sequentially querying an external database and receiving binary responses. In the meantime, an adversary observes the learner's queries, though not the responses, and tries to infer from them the value of v\u2217. The objective of the learner is to obtain an accurate estimate of v\u2217 using only a small number of queries, while simultaneously protecting her privacy by making v\u2217 provably difficult to learn for the adversary. Our main results provide tight upper and lower bounds on the learner's query complexity as a function of desired levels of privacy and estimation accuracy. We also construct explicit query strategies whose complexity is optimal up to an additive constant.",
            "Publication date": 2018.0,
            "Publication link": "https://arxiv.org/abs/1805.02136",
            "Publication name": "Private sequential learning.",
            "Released at": "Conference on Learning Theory (COLT)",
            "Interacts with": "NaN",
            "Authors": "John N Tsitsiklis, Kuang Xu, and Zhi Xu.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "QCBPL",
            "Using for": "Private Learning",
            "Based on": "BO, PSL",
            "Method": "Query Complexity of Bayesian Private Learning",
            "Description": "Authors study the query complexity of Bayesian Private Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary?\nThey main result is a query complexity lower bound that is tight up to the first order. The main contribution of the present paper is a tight query complexity lower bound for the Bayesian Private Learning problem. Authors shows that the learner\u2019s query complexity depends multiplicatively on the level of privacy, L: if an accurate learner wishes to ensure that an adversary\u2019s probability of making accurate estimation is at most 1/L, then she needs to employ on the order of L queries. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7510-query-complexity-of-bayesian-private-learning",
            "Publication name": "Query Complexity of Bayesian Private Learning",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Kuang Xu\n",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "TAS2N2O",
            "Using for": "Optimization",
            "Based on": "None",
            "Method": "A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization",
            "Description": "Authors study the set of continuous functions that admit no spurious local optima which we term global functions. Authors develop a new notion of global functions for which all local minima are global minima. Using certain properties of global functions, they show that the set of these functions include a class of nonconvex and nonsmooth functions that arise in matrix completion/sensing and tensor recovery/decomposition with Laplacian noise. This paper offers a new mathematical technique for the analysis of nonconvex and nonsmooth functions such as those involving l_1 norm and l_\u221e norm.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7511-a-theory-on-the-absence-of-spurious-solutions-for-nonconvex-and-nonsmooth-optimization",
            "Publication name": "A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Cedric Josz, Yi Ouyang, Richard Zhang, Javad Lavaei, Somayeh Sojoudi",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "WRLDMDLS",
            "Using for": "Reinforcement Learning",
            "Based on": "VAE, RNNs",
            "Method": "World Models",
            "Description": "Authors have demonstrated the possibility of training an agent to perform tasks entirely inside of its simulated latent space world.  For instance, video game engines typically require heavy compute resources for rendering the game states into image frames, or calculating physics not immediately relevant to the game. We may not want to waste cycles training an agent in the actual environment, but instead train the agent as many times as we want inside its simulated environment. Agents that are trained incrementally to simulate reality may prove to be useful for transferring policies back to the real world. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution",
            "Publication name": "Recurrent World Models Facilitate Policy Evolution",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": " David Ha, J\u00fcrgen Schmidhuber",
            "Code link": "https://worldmodels.github.io",
            "Data": "OpenAI Gym"
        },
        {
            "id": "PDLSS",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Provable Deterministic Leverage Score Sampling.",
            "Description": "We explain theoretically a curious empirical phenomenon: \"Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate\". In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.",
            "Publication date": 2014.0,
            "Publication link": "https://doi.org/10.1145/2623330.2623698",
            "Publication name": "Provable Deterministic Leverage Score Sampling.",
            "Released at": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD \u201914)",
            "Interacts with": "NaN",
            "Authors": "Dimitris Papailiopoulos, Anastasios Kyrillidis, and Christos Boutsidis. 2014. Provable Deterministic Leverage Score Sampling.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DRLS",
            "Using for": "NaN",
            "Based on": "PDLSS",
            "Method": "\u0412eterministic ridge leverage score",
            "Description": "Authors explore deterministic ridge leverage score  sampling for matrix approximation and for feature selection in concert with ridge regression. This work has two main motivations:  the advantages of ridge leverage scores over rank-k subspace leverage scores, and  the advantages of deterministic algorithms in some practical settings.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7513-ridge-regression-and-provable-deterministic-ridge-leverage-score-sampling",
            "Publication name": "Ridge Regression and Provable Deterministic Ridge Leverage Score Sampling",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Shannon McCurdy",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "WVI",
            "Using for": "Autoencoder, Generation",
            "Based on": "VARINF, WASSD",
            "Method": "Wasserstein Variational Inference",
            "Description": "A new form of approximate Bayesian inference based on optimal transport theory. Wasserstein variational inference uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7514-wasserstein-variational-inference",
            "Publication name": "Wasserstein Variational Inference",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Luca Ambrogioni, Umut G\u00fc\u00e7l\u00fc, Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk, Max Hinne, Marcel A. J. van Gerven, Eric Maris",
            "Code link": "None",
            "Data": "MNIST, Fashion-MNIST, Quick Sketch"
        },
        {
            "id": "WAE",
            "Using for": "NaN",
            "Based on": "AE, WASSD",
            "Method": "Wasserstein Autoencoder",
            "Description": "Algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, which leads to a different regularizer than the one used by the Variational Auto-Encoder (VAE). This regularizer encourages the encoded training distribution to match the prior. We compare our algorithm with several other techniques and show that it is a generalization of adversarial auto-encoders (AAE). Our experiments show that WAE shares many of the properties of VAEs (stable training, encoder-decoder architecture, nice latent manifold structure) while generating samples of better quality, as measured by the FID score.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1711.01558",
            "Publication name": "Wasserstein Autoencoder",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, Bernhard Schoelkopf",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "WVA",
            "Using for": "Autoencoder, Generation",
            "Based on": "WAE, WVI",
            "Method": "Wasserstein Variational Autoencoder",
            "Description": "A new form of autorencode based on optimal transport theory. Wasserstein variational autorncoder based on Wasserstein variational Inference, which uses a new family of divergences that includes both f-divergences and the Wasserstein distance as special cases. The gradients of the Wasserstein variational loss are obtained by backpropagating through the Sinkhorn iterations. This technique results in a very stable likelihood-free training method that can be used with implicit distributions and probabilistic programs. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7514-wasserstein-variational-inference",
            "Publication name": "Wasserstein Variational Inference",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Luca Ambrogioni, Umut G\u00fc\u00e7l\u00fc, Ya\u011fmur G\u00fc\u00e7l\u00fct\u00fcrk, Max Hinne, Marcel A. J. van Gerven, Eric Maris",
            "Code link": "None",
            "Data": "MNIST, Fashion-MNIST, Quick Sketch"
        },
        {
            "id": "BNHO",
            "Using for": "Normalization, Optimization",
            "Based on": "BN",
            "Method": "Batch Normalization Help Optimization",
            "Description": "In this work, authors have investigated the roots of BatchNorm\u2019s effectiveness as a technique for training deep neural networks. They find that the widely believed connection between the performance of BatchNorm and the internal covariate shift is tenuous, at best.  They demonstrate that existence of internal covariate shift, at least when viewed from the \u2013 generally adopted \u2013 distributional stability perspective, is not a good predictor of training performance. Also, authors show that, from an optimization viewpoint, BatchNorm might not be even reducing that shift.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization",
            "Publication name": "How Does Batch Normalization Help Optimization?",
            "Released at": "NIPS",
            "Interacts with": "DNNs",
            "Authors": "Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "DAGGER",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Dataset Aggregation",
            "Description": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.",
            "Publication date": 2011.0,
            "Publication link": "https://arxiv.org/pdf/1011.0686.pdf",
            "Publication name": "A reduction of imitation learning and structured prediction to no-regret online learning.",
            "Released at": "AISTATS",
            "Interacts with": "NaN",
            "Authors": "S.Ross, G.Gordon, D.Bagnell.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "Q-DAGGER",
            "Using for": "Reinforcement Learning",
            "Based on": "Q-L, DAGGER",
            "Method": "Q-DAGGER",
            "Description": "A novel imitation learning algorithm that extends DAGGER to use the Q-function for the DNN policy. This method is general policy extraction algorithm with theoretical guarantees improving on DAGGER\u2019s,",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7516-verifiable-reinforcement-learning-via-policy-extraction",
            "Publication name": "Verifiable Reinforcement Learning via Policy Extraction\n\n",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Osbert Bastani, Yewen Pu, Armando Solar-Lezama",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "VIPER",
            "Using for": "Reinforcement Learning",
            "Based on": "Q-DAGGER",
            "Method": "Verifiable Reinforcement Learning via Policy Extraction",
            "Description": "An algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy and its Q-function, and show that it substantially outperforms two baselines.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7516-verifiable-reinforcement-learning-via-policy-extraction",
            "Publication name": "Verifiable Reinforcement Learning via Policy Extraction\n\n",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Osbert Bastani, Yewen Pu, Armando Solar-Lezama",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "DEREJS",
            "Using for": "Regression",
            "Based on": "None",
            "Method": "Determinantal Rejection Sampling",
            "Description": "Suppose an n x d design matrix in a linear regression problem is given, but the response for each point is hidden unless explicitly requested. The goal is to sample only a small number  of the responses, and then produce a weight vector whose sum of squares loss over all points is at most 1 + e times the minimum. When k is very small (e.g., k = d), jointly sampling diverse subsets of points is crucial. One such method called volume sampling has a unique and desirable property that the weight vector it produces is an unbiased estimate of the optimum. Authors present new variant of volume sampling which produces the first known unbiased subsampled least-squares estimator with strong multiplicative loss bounds.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7517-leveraged-volume-sampling-for-linear-regression",
            "Publication name": "Leveraged volume sampling for linear regression",
            "Released at": "NIPS",
            "Interacts with": "LINR",
            "Authors": "Michal Derezinski, Manfred K. Warmuth, Daniel J. Hsu",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "SILO",
            "Using for": "Regression",
            "Based on": "NaN",
            "Method": "Local linear modeling techniques",
            "Description": "Techniques for nonparametric regression based on fitting small-scale local models at prediction time have long been studied in statistics and pattern recognition, but have received less attention in modern large-scale machine learning applications. In practice, such methods are generally applied to low-dimensional problems, but may falter with high-dimensional predictors if they use a Euclidean distance-based kernel. We propose a new method, SILO, for fitting prediction-time local models that uses supervised neighborhoods that adapt to the local shape of the regression surface. To learn such neighborhoods, we use a weight function between points derived from random forests. We prove the consistency of SILO, and demonstrate through simulations and real data that our method works well in both the serial and distributed settings. In the latter case, SILO learns the weighting function in a divide-and-conquer manner, entirely avoiding communication at training time.",
            "Publication date": 2016.0,
            "Publication link": "http://proceedings.mlr.press/v51/bloniarz16.html",
            "Publication name": "Supervised neighborhoods for distributed nonparametric regression.",
            "Released at": "International Conference on Artificial Intelligence and Statistics",
            "Interacts with": "NaN",
            "Authors": "Adam Bloniarz, Ameet Talwalkar, Bin Yu, Christopher Wu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DStump",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Dual interpretation of random forests",
            "Description": "Decision trees and random forests are well established models that not only offer good predictive performance, but also provide rich feature importance information. While practitioners often employ variable importance methods that rely on this impurity-based information, these methods remain poorly characterized from a theoretical perspective. We provide novel insights into the performance of these methods by deriving finite sample performance guarantees in a high-dimensional setting under various modeling assumptions. We further demonstrate the effectiveness of these impurity-based methods via an extensive set of simulations.",
            "Publication date": 2017.0,
            "Publication link": "https://papers.nips.cc/paper/6646-variable-importance-using-decision-trees",
            "Publication name": "Variable importance using decision trees.",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jalil Kazemitabar, Arash Amini, Adam Bloniarz, and Ameet S Talwalkar.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MAPLE",
            "Using for": "Detecting and modeling global patterns",
            "Based on": "SILO, DStump",
            "Method": "Model Agnostic Supervised Local Explanations",
            "Description": "Model interpretability is an increasingly important component of practical ma- chine learning. That model uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7518-model-agnostic-supervised-local-explanations",
            "Publication name": "Model Agnostic Supervised Local Explanations",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Gregory Plumb, Denali Molitor, Ameet S. Talwalkar",
            "Code link": "None",
            "Data": "UCI datasets"
        },
        {
            "id": "MONDT",
            "Using for": "Activive Learning",
            "Based on": "NaN",
            "Method": "Mondrian forests",
            "Description": "Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classification and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efficient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman's random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically re-trained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.",
            "Publication date": 2017.0,
            "Publication link": "https://arxiv.org/abs/1406.2673",
            "Publication name": "Mondrian forests: Efficient online random forests",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Lakshminarayanan, B., Roy, D. M., and Teh, Y. W",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "MONDTAR",
            "Using for": "Regression, Active Learning",
            "Based on": "MONDT",
            "Method": "Mondrian Tree Active Learning",
            "Description": "In this paper authors provide a theoretically justified active learning method for non-parametric regression which can take advantage of beneficial structure when present without being detrimental when such structure is absent.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7520-active-learning-for-non-parametric-regression-using-purely-random-trees",
            "Publication name": "Active Learning for Non-Parametric Regression Using Purely Random Trees",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Jack Goetz, Ambuj Tewari, Paul Zimmerman\n",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "TRLSTM",
            "Using for": "Recurrent Networks",
            "Based on": "DT, LSTM",
            "Method": "Tree Long Short Term Memory",
            "Description": "Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance.",
            "Publication date": 2015.0,
            "Publication link": "https://arxiv.org/abs/1511.00060",
            "Publication name": "Top-down Tree Long Short-Term Memory Networks",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Xingxing Zhang, Liang Lu, Mirella Lapata",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "T3N2",
            "Using for": "Program translation",
            "Based on": "LSTM, TRLSTM, ENCDR, DCDR, ATTN",
            "Method": "Tree-to-tree Neural Network",
            "Description": "Neural network approaches for the program translation problem, and are the first to demonstrate a successful design of tree-to-tree neural network combining both a tree-RNN encoder and a tree-RNN decoder for translation tasks. Extensive evaluation demonstrates that our tree-to-tree neural network outperforms several state-of-the-art models. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7521-tree-to-tree-neural-networks-for-program-translation",
            "Publication name": "Tree-to-tree Neural Networks for Program Translation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Xinyun Chen, Chang Liu, Dawn Song",
            "Code link": "None",
            "Data": "CoffeeScript to JavaScript,  Java to C#,  Imperative language to a functional language"
        },
        {
            "id": "BIN",
            "Using for": "Image Recognition",
            "Based on": "BN",
            "Method": "Batch-Instance Normalization",
            "Description": "Real-world image recognition is often challenged by the variability of visual styles including object textures, lighting conditions, filter effects, etc. Although these vari- ations have been deemed to be implicitly handled by more training data and deeper networks, recent advances in image style transfer suggest that it is also possible to explicitly manipulate the style information. Extending this idea to general visual recognition problems, authors present Batch-Instance Normalization to explicitly normalize unnecessary styles from images. Proposed method learns to selectively normalize only disturbing styles while preserving useful styles, and is easily incorporated into existing network architectures such as Residual Networks, and surprisingly improves the recognition performance in various sce- narios. It learns to control how much of the style information is propagated through each channel of features leveraging a learnable gate parameter. If the style associated with a feature map is irrelevant to or disturbs the task, BIN closes the gate to suppress the style using Instance Normalization. If the style carries important information to the task, on the other hand, BIN opens the gate to preserve the style though BN.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7522-batch-instance-normalization-for-adaptively-style-invariant-neural-networks",
            "Publication name": "Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks",
            "Released at": "NIPS",
            "Interacts with": "RESN",
            "Authors": "Hyeonseob Nam, Hyo-Eun Kim",
            "Code link": "https://github.com/hyeonseob-nam/Batch-Instance-Normalization",
            "Data": "CIFAR-10, CIFAR-100, ImageNe"
        },
        {
            "id": "SCM-MAB",
            "Using for": "Multi-Armed Bandit",
            "Based on": "None",
            "Method": "Structural Casual Multi-Armed Bandit",
            "Description": "Authors formulate a SCM-MAB problem, which is a structured multi-armed bandit instance within the causal framework. Then authors derive the structural properties of a SCM-MAB, which are computable from any causal model, including arms\u2019 equivalence based on do-calculus, and partial orderedness among sets of variables associated with arms in regards to the maximum rewards achievable. They characterize a special set of variables called POMIS (possibly-optimal minimal intervention set), which is worth intervening based on the aforementioned partial orders. We then introduce an algorithm that identifies a complete set of POMISs so that only the subset of arms associated with them can be explored in a MAB algorithm.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7523-structural-causal-bandits-where-to-intervene",
            "Publication name": "Structural Causal Bandits: Where to Intervene?",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sanghack Lee, Elias Bareinboim",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "POMIS",
            "Using for": "Multi-Armed Bandit",
            "Based on": "SCM-MAB",
            "Method": "Possibly-optimal minimal intervention set)",
            "Description": "Authors formulate a SCM-MAB problem, which is a structured multi-armed bandit instance within the causal framework. Then authors derive the structural properties of a SCM-MAB, which are computable from any causal model, including arms\u2019 equivalence based on do-calculus, and partial orderedness among sets of variables associated with arms in regards to the maximum rewards achievable. They characterize a special set of variables called POMIS (possibly-optimal minimal intervention set), which is worth intervening based on the aforementioned partial orders. We then introduce an algorithm that identifies a complete set of POMISs so that only the subset of arms associated with them can be explored in a MAB algorithm.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7523-structural-causal-bandits-where-to-intervene",
            "Publication name": "Structural Causal Bandits: Where to Intervene?",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sanghack Lee, Elias Bareinboim",
            "Code link": "None",
            "Data": "None"
        },
        {
            "id": "TOM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Theory of Mind",
            "Description": "An individual has a theory of mind if he imputes mental states to himself and others. A system of inferences of this kind is properly viewed as a theory because such states are not directly observable, and the system can be used to make predictions about the behavior of others. As to the mental states the chimpanzee may infer, consider those inferred by our own species, for example, purpose or intention, as well as knowledge, belief, thinking, doubt, guessing, pretending, liking, and so forth. To determine whether or not the chimpanzee infers states of this kind, we showed an adult chimpanzee a series of videotaped scenes of a human actor struggling with a variety of problems. Some problems were simple, involving inaccessible food \u2013 bananas vertically or horizontally out of reach, behind a box, and so forth \u2013 as in the original Kohler problems; others were more complex, involving an actor unable to extricate himself from a locked cage, shivering because of a malfunctioning heater, or unable to play a phonograph because it was unplugged. With each videotape the chimpanzee was given several photographs, one a solution to the problem, such as a stick for the inaccessible bananas, a key for the locked up actor, a lit wick for the malfunctioning heater. The chimpanzee's consistent choice of the correct photographs can be understood by assuming that the animal recognized the videotape as representing a problem, understood the actor's purpose, and chose alternatives compatible with that purpose.",
            "Publication date": 1978.0,
            "Publication link": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/does-the-chimpanzee-have-a-theory-of-mind/1E96B02CD9850016B7C93BC6D2FEF1D0",
            "Publication name": "Does the chimpanzee have a theory of mind",
            "Released at": "Behavioral and Brain Sciences,",
            "Interacts with": "NaN",
            "Authors": "David Premack, Guy Woodruff.",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "AQM",
            "Using for": "Goal-Oriented Visual Dialog",
            "Based on": "TOM",
            "Method": "Answerer in Questioner\u2019s Mind",
            "Description": "A novel information theoretic algorithm for goal-oriented dialog. With AQM, a questioner asks and infers based on an approximated probabilistic model of the answerer. The questioner figures out the answerer\u2019s intention via selecting a plausible question by explicitly calculating the information gain of the candidate intentions and possible answers to each question. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7524-answerer-in-questioners-mind-information-theoretic-approach-to-goal-oriented-visual-dialog",
            "Publication name": "Answerer in Questioner's Mind: Information Theoretic Approach to Goal-Oriented Visual Dialog",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Sang-Woo Lee, Yu-Jung Heo, Byoung-Tak Zhang",
            "Code link": "None",
            "Data": " \u201cMNIST Counting Dialog\u201d, \u201cGuessWhat?!\u201d"
        },
        {
            "id": "UFDN",
            "Using for": "Multi-domain image translation",
            "Based on": "ENCDR, GENN",
            "Method": "Unified Feature Disentanglement Network",
            "Description": "This method learns deep disentangled feature representation for multi-domain image translation and manipulation. UFDN views both data domains and image attributes of interest as latent factors to be disentangled, which realizes multi-domain image translation in a single unified framework. Continuous multi-domain image translation and manipulation can be performed using UFDN, while the disentangled feature representation shows promising ability in cross-domain classification tasks.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7525-a-unified-feature-disentangler-for-multi-domain-image-translation-and-manipulation",
            "Publication name": "A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": " Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, Yu-Chiang Frank Wang",
            "Code link": "None",
            "Data": "MNIST, USPS, SVHN"
        },
        {
            "id": "OLUFM",
            "Using for": "Online Learning",
            "Based on": "NaN",
            "Method": "Online Learning with an Unknown Fairness Metric",
            "Description": "Authors consider the problem of online learning in the linear contextual bandits setting,\nbut in which there are also strong individual fairness constraints governed by an\nunknown similarity metric. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7526-online-learning-with-an-unknown-fairness-metric",
            "Publication name": "Online Learning with an Unknown Fairness Metric",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Stephen Gillen, Christopher Jung, Michael Kearns, Aaron Roth",
            "Code link": "None",
            "Data": "NaN"
        },
        {
            "id": "KLD",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Kullback-Leibler Divergence",
            "Description": "Is a measure of how one probability distribution is different from a second, reference probability distribution. Applications include characterizing the relative (Shannon) entropy in information systems, randomness in continuous time-series, and information gain when comparing statistical models of inference. In contrast to variation of information, it is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread (it also does not satisfy the triangle inequality). In the simple case, a Kullback\u2013Leibler divergence of 0 indicates that the two distributions in question are identical. In simplified terms, it is a measure of surprise, with diverse applications such as applied statistics, fluid mechanics, neuroscience and machine learning.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Kullback\u2013Leibler_divergence",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ELBO",
            "Using for": "NaN",
            "Based on": "KLD, VI",
            "Method": "Evidence Lower Bound",
            "Description": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
            "Publication date": 2013.0,
            "Publication link": "https://arxiv.org/abs/1312.6114",
            "Publication name": "Auto-Encoding Variational Bayes",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Diederik P Kingma, Max Welling",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MIG",
            "Using for": "Generation, Metric",
            "Based on": "ELBO",
            "Method": "Mutual Information Gap",
            "Description": "This metric benefits from advances in efficient computation of mutual information and enforces compactness in addition to disentanglement. Key insight of this method  that the empirical mutual information between a latent variable zj and a ground truth factor vk can be estimated using the joint distribution",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders",
            "Publication name": "Isolating Sources of Disentanglement in Variational Autoencoders",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Tian Qi Chen, Xuechen Li, Roger B. Grosse, David K. Duvenaud",
            "Code link": "https://github.com/rtqichen/beta-tcvae",
            "Data": "CelebA"
        },
        {
            "id": "\u03b2-TCVAE",
            "Using for": "Generation",
            "Based on": "MIG, \u03b2-VAE",
            "Method": "Total Correlation Variational Autoencoder",
            "Description": "Simple method based on weighted mini-batches to stochastically train with arbitrary\nweights on the terms of our decomposition without any additional hyperparameters.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders",
            "Publication name": "Isolating Sources of Disentanglement in Variational Autoencoders",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Tian Qi Chen, Xuechen Li, Roger B. Grosse, David K. Duvenaud",
            "Code link": "https://github.com/rtqichen/beta-tcvae",
            "Data": "CelebA"
        },
        {
            "id": "LMC",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Langevin Monte Carlo",
            "Description": "Is an iterative algorithm used to generate samples from a distribution that is known only up to a normalizing constant.",
            "Publication date": 1981.0,
            "Publication link": "https://www.sciencedirect.com/science/article/pii/0550321381900560",
            "Publication name": "Correlation functions and computer simulations",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "G. Parisi",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "FTL",
            "Using for": "Optimization, Online Learning, Online Convex Optimization",
            "Based on": "NaN",
            "Method": "Follow-The-Leader",
            "Description": "Is a simple algorithm for solving online prediction problems. Imagine that you have a committee of experts, each of which suggests a strategy. At each time point, you pick an expert and follow his or her advice, which yields some associated cost (or reward) at the next time step. Your goal is to minimize the total cost/maximize your total reward.\n\nFollow-The-Leader uses a very simple approach: track the performance of all experts over all previous time steps, then select the expert/strategy/etc that has performed the best so far, and follow its advice on the next round. Update everything and choose again.",
            "Publication date": 2014.0,
            "Publication link": "https://courses.cs.washington.edu/courses/cse599s/14sp/scribes/lecture2/scribeNote.pdf",
            "Publication name": "Online Convex Optimization Example And Follow-The-Leader",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Brendan McMahan",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HINGE",
            "Using for": "Optimization, Classification",
            "Based on": "NaN",
            "Method": "HINGE Loss",
            "Description": "The hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines",
            "Publication date": 2011.0,
            "Publication link": "https://www.ttic.edu/sigml/symposium2011/papers/Moore+DeNero_Regularization.pdf",
            "Publication name": "L1 and L2 regularization for multiclass hinge loss models",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "Robert C. Moore, John DeNero",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HINGE-LMC",
            "Using for": "Optimization, Classification",
            "Based on": "LMC, HINGE",
            "Method": "HINGE-Langevin Monte Carlo",
            "Description": "This algorithm is based on the exponential weights update,\nalong with Langevin Monte Carlo for efficient sampling and a careful action selection scheme",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7528-contextual-bandits-with-surrogate-losses-margin-bounds-and-efficient-algorithms",
            "Publication name": "Contextual bandits with surrogate losses: Margin bounds and efficient algorithms",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Dylan J. Foster, Akshay Krishnamurthy",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "SMOOTHFTL",
            "Using for": "Optimization",
            "Based on": "FTL",
            "Method": "SMOOT Follow-The-Leader",
            "Description": "Simply Follow-The-Leader with uniform smoothing, this method applies to the stochastic setting with classes that have \u201chigh complexity\u201d ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7528-contextual-bandits-with-surrogate-losses-margin-bounds-and-efficient-algorithms",
            "Publication name": "Contextual bandits with surrogate losses: Margin bounds and efficient algorithms",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Dylan J. Foster, Akshay Krishnamurthy",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "CBSL",
            "Using for": "Classification",
            "Based on": "LCB, HINGE-LMC, SMOOTHFTL",
            "Method": "Contextual bandits with surrogate losses",
            "Description": "Using the ramp loss, authors derive new margin-based regret\nbounds in terms of standard sequential complexity measures of a benchmark class\nof real-valued regression functions",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7528-contextual-bandits-with-surrogate-losses-margin-bounds-and-efficient-algorithms",
            "Publication name": "Contextual bandits with surrogate losses: Margin bounds and efficient algorithms",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Dylan J. Foster, Akshay Krishnamurthy",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "PDDM",
            "Using for": "Deep Representation Learning, Similarity",
            "Based on": "NaN",
            "Method": "Position-dependent deep metric",
            "Description": "PDDM able to preserve the local similarity information and meanwhile\nachieve the balanced distributions in the latent space. In SITE, the PDDM component measures the local similarity of\ntwo units based on their relative and absolute positions in the latent space Z.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7529-representation-learning-for-treatment-effect-estimation-from-observational-data",
            "Publication name": "Representation Learning for Treatment Effect Estimation from Observational Data",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, Aidong Zhang",
            "Code link": "https://github.com/Osier-Yi/SITE.",
            "Data": " IHDP dataset, Synthetic Dataset"
        },
        {
            "id": "MPDM",
            "Using for": "Deep Representation Learning, Similarity",
            "Based on": "NaN",
            "Method": "Middle Point Distance Minimization",
            "Description": "MPDM able to preserve the local similarity information and meanwhile achieve the balanced distributions in the latent space. MPDM makes the units that belong to treated group close to the control group",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7529-representation-learning-for-treatment-effect-estimation-from-observational-data",
            "Publication name": "Representation Learning for Treatment Effect Estimation from Observational Data",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, Aidong Zhang",
            "Code link": "https://github.com/Osier-Yi/SITE.",
            "Data": " IHDP dataset, Synthetic Dataset"
        },
        {
            "id": "SITE",
            "Using for": "Deep Representation Learning, Estimating individual\ntreatment effect",
            "Based on": "PDDM, MPDM",
            "Method": "Similarity preserved individual treatment effect",
            "Description": "This estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. The key idea of SITE is to map the original pre-treatment covariate space X into a latent space Z learned by deep neural networks. Particularly, SITE attempts to enforce two special properties on the latent space Z, including the balanced distribution and preserved similarity",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7529-representation-learning-for-treatment-effect-estimation-from-observational-data",
            "Publication name": "Representation Learning for Treatment Effect Estimation from Observational Data",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, Aidong Zhang",
            "Code link": "https://github.com/Osier-Yi/SITE.",
            "Data": " IHDP dataset, Synthetic Dataset"
        },
        {
            "id": "MSE",
            "Using for": "Loss function, Metric",
            "Based on": "NaN",
            "Method": "Mean Squared Error",
            "Description": "Measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and what is estimated. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Mean_squared_error",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "CATE",
            "Using for": "Reinforcement Learning",
            "Based on": "NaN",
            "Method": "Conditional Averaged Treatment Effects",
            "Description": "This research aims to obtain precise estimates in the difference in outcomes for giving the treatment vs control intervention for an individual (state) and works in the contextual bandit setting with a single (typically binary) action choice.",
            "Publication date": "NaN",
            "Publication link": "https://www.tandfonline.com/doi/abs/10.1080/07350015.2014.975555",
            "Publication name": "Estimating Conditional Average Treatment Effects",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "None"
        },
        {
            "id": "RepBM",
            "Using for": "Off-policy policy evaluation, Reinforcement Learning",
            "Based on": "MSE, CATE",
            "Method": "Representation Balancing MDPs",
            "Description": "In reinforcement learning, off-policy (batch) policy evaluation is the task of estimating the performance of some evaluation policy given data gathered under a different behavior policy. Off-policy policy evaluation (OPPE) is essential when deploying a new policy might be costly or risky, such as in consumer marketing, healthcare, and education. In this work, authors build model-based estimators for OPPE that both do have theoretical guarantees and yield better empirical performance that model-based approaches that ignore the data distribution mismatch. in this paper authors develop an upper bound of the MSE for individual policy value estimates",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7530-representation-balancing-mdps-for-off-policy-policy-evaluation",
            "Publication name": "Representation Balancing MDPs for Off-policy Policy Evaluation",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo A. Faisal, Finale Doshi-Velez, Emma Brunskill",
            "Code link": "None",
            "Data": " Cart Pole, Montain Car"
        },
        {
            "id": "OOB",
            "Using for": "Visual Question Answering",
            "Based on": "GCN, LSTM, MLP, EMBD",
            "Method": "Out of the Box",
            "Description": "This is a method for \u2018reasoning\u2019 in factual visual question answering using graph convolution\nnets.  Authors attribute these improvements to \u2018joint reasoning about answers,\u2019 which facilitates sharing\nof information before making an informed decision. Further, they achieve this high increase in\nperformance by using only the ground truth relation and answer information, with no reliance on the\nground truth fact. ",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7531-out-of-the-box-reasoning-with-graph-convolution-nets-for-factual-visual-question-answering",
            "Publication name": "Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering",
            "Released at": "NIPS",
            "Interacts with": "None",
            "Authors": "Medhini Narasimhan, Svetlana Lazebnik, Alexander Schwing",
            "Code link": "None",
            "Data": "FVQA dataset"
        },
        {
            "id": "SVD",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "Singular Value Decomposition",
            "Description": "is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix (for example, a symmetric matrix with positive eigenvalues) to any m \u00d7n, matrix via an extension of the polar decomposition. It has many useful applications in signal processing and statistics.",
            "Publication date": "NaN",
            "Publication link": "https://en.wikipedia.org/wiki/Singular_value_decomposition",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GDMSPfEC",
            "Using for": "Optimization,  Spectral clustering",
            "Based on": "SGD, PCA, SVD",
            "Method": "Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation",
            "Description": "Shift-and-invert preconditioning, as a classic acceleration technique for the leading eigenvector computation, has received much attention again recently, owing to fast least-squares solvers for efficiently approximating matrix inversions in power iterations. In this work, we adopt an inexact Riemannian gradient descent perspective to investigate this technique on the effect of the step-size scheme.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7547-gradient-descent-meets-shift-and-invert-preconditioning-for-eigenvector-computation.pdf",
            "Publication name": "Gradient Descent Meets Shift-and-Invert Preconditioning for Eigenvector Computation",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Zhiqiang Xu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NsCPLC",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Solving Non-smooth Constrained Programs with Lower Complexity than O(1/\"): A Primal-Dual",
            "Description": "We propose a new primal-dual homotopy smoothing algorithm for a linearly constrained convex program, where neither the primal nor the dual function has to be smooth or strongly convex. The best known iteration complexity solving such a non-smooth problem is O(\" 1). In this paper, we show that by leveraging a local error bound condition on the dual function, the proposed algorithm can achieve a better primal convergence time of O \" 2/(2+ ) log2(\" 1) , where 2 (0, 1] is a local error bound parameter. As an example application of the general algorithm, we show that the distributed geometric median problem, which can be formulated as a constrained convex program, has its dual function non-smooth but satisfying the aforementioned local error bound condition with = 1/2, therefore enjoying a convergence time of O \" 4/5 log2(\" 1) . This result improves upon the O(\" 1) convergence time bound achieved by existing distributed optimization algorithms. Simulation experiments also demonstrate the performance of our proposed algorithm.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7655-solving-non-smooth-constrained-programs-with-lower-complexity-than-mathcalo1varepsilon-a-primal-dual-homotopy-smoothing-approach.pdf",
            "Publication name": "Solving Non-smooth Constrained Programs with Lower Complexity than O(1/\"): A Primal-Dual",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "AORWS",
            "Using for": "Image manipulation, peak signal-to-noise ratio",
            "Based on": "GAN",
            "Method": "Automatic Object Removal from Weak Supervision",
            "Description": "While great progress has been made recently in automatic image manipulation, it has been limited to object centric images like faces or structured scene datasets. In this work, we take a step towards general scene-level image editing by developing an automatic interaction-free object removal model. Our model learns to find and remove objects from general scene images using image-level labels and unpaired data in a generative adversarial network (GAN) framework. We achieve this with two key contributions: a two-stage editor architecture consisting of a mask generator and image in-painter that co-operate to remove objects, and a novel GAN based prior for the mask generator that allows us to flexibly incorporate knowledge about object shapes. We experimentally show on two datasets that our method effectively removes a wide variety of objects using weak supervision only.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7997-adversarial-scene-editing-automatic-object-removal-from-weak-supervision.pdf",
            "Publication name": "Adversarial Scene Editing: Automatic Object Removal from Weak Supervision",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Rakshith R. Shetty, Mario Fritz, Bernt Schiele",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "NNBSRM",
            "Using for": "Neural Machine Translation, Real Time Speech Recognition",
            "Based on": "LSTM, CNN",
            "Method": "Neural Network Based Speech Recognition on Mobile ",
            "Description": "Real-time automatic speech recognition (ASR) on mobile and embedded devices has been of great interests for many years. We present real-time speech recognition on smartphones or embedded systems by employing recurrent neural network (RNN) based acoustic models, RNN based language models, and beam-search decoding. The acoustic model is end-to-end trained with connectionist temporal classification (CTC) loss. The RNN implementation on embedded devices can suffer from excessive DRAM accesses because the parameter size of a neural network usually exceeds that of the cache memory and the parameters are used only once for each time step. To remedy this problem, we employ a multi-time step parallelization approach that computes multiple output samples at a time with the parameters fetched from the DRAM. Since the number of DRAM accesses can be reduced in proportion to the number of parallelization steps, we can achieve a high processing speed. However, conventional RNNs, such as long short-term memory (LSTM) or gated recurrent unit (GRU), do not permit multi-time step parallelization. We construct an acoustic model by combining simple recurrent units (SRUs) and depth-wise 1-dimensional convolution layers for multi-time step parallelization. Both the character and word piece models are developed for acoustic modeling, and the corresponding RNN based language models are used for beam search decoding. We achieve a competitive WER for WSJ corpus using the entire model size of around 15MB and achieve real-time speed using only a single core ARM without GPU or special hardware.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/8261-fully-neural-network-based-speech-recognition-on-mobile-and-embedded-devices.pdf",
            "Publication name": "Fully Neural Network Based Speech Recognition on Mobile and Embedded Devices",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Jinhwan Park, Yoonho Boo, Iksoo Choi, Sungho Shin, Wonyong Sung",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MBWC",
            "Using for": "Bandit Problem",
            "Based on": "NaN",
            "Method": "Multi-armed Bandits with Compensation",
            "Description": "We propose and study the known-compensation multi-arm bandit (KCMAB) problem, where a system controller offers a set of arms to many short-term players for\nT steps. In each step, one short-term player arrives to the system. Upon arrival, the player aims to select an arm with the current best average reward and receives a stochastic reward associated with the arm. In order to incentivize players to explore other arms, the controller provides a proper payment compensation to players. The objective of the controller is to maximize the total reward collected by players while minimizing the compensation. We first provide a compensation lower bound \u0398(",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7758-multi-armed-bandits-with-compensation.pdf",
            "Publication name": "Multi-armed Bandits with Compensation",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Siwei Wang, Longbo Huang",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MLD",
            "Using for": "Langevin dynamics, Convergence rate",
            "Based on": "NaN",
            "Method": "Mirrored Langevin Dynamics",
            "Description": "We consider the problem of sampling from constrained distributions, which has posed significant challenges to both non-asymptotic analysis and algorithmic design. We propose a unified framework, which is inspired by the classical mirror descent, to derive novel first-order sampling schemes. We prove that, for a general target distribution with strongly convex potential, our framework implies the existence of a first-order algorithm achieving O( \u22122d) convergence, suggesting that the state-of-the-art O( \u22126d5) can be vastly improved. With the important Latent Dirichlet Allocation (LDA) application in mind, we specialize our algorithm to sample from Dirichlet posteriors, and derive the first non-asymptotic O( \u22122d2) rate for first-order sampling. We further extend our framework to the mini-batch setting and prove convergence rates when only stochastic gradients are available. Finally, we report promising experimental results for LDA on real datasets.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7552-mirrored-langevin-dynamics.pdf",
            "Publication name": "Mirrored Langevin Dynamics",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Ya-Ping Hsieh, Ali Kavis, Paul Rolland, Volkan Cevher",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RELBO",
            "Using for": "Optimization",
            "Based on": "ELBO",
            "Method": "Residual ELBO",
            "Description": "Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple variational approximation. Borrowing ideas from the classic boosting framework, recent approaches attempt to boost VI by replacing the selection of a single density with an iteratively constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. These theoretical enhancements allow for black box implementation of the boosting subroutine. Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses and exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7600-boosting-black-box-variational-inference.pdf",
            "Publication name": "Boosting Black Box Variational Inference",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Francesco Locatello, Gideon Dresdner, Rajiv Khanna, Isabel Valera, Gunnar Raetsch",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "FSOAFLM",
            "Using for": "Optimization",
            "Based on": "NaN",
            "Method": "Faster Stochastic Optimization Algorithms for Finding Local Minima",
            "Description": "We propose stochastic optimization algorithms that can find local minima faster than existing algorithms for nonconvex optimization problems, by exploiting the third-order smoothness to escape non-degenerate saddle points more efficiently.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7704-third-order-smoothness-helps-faster-stochastic-optimization-algorithms-for-finding-local-minima.pdf",
            "Publication name": "Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Yaodong Yu, Pan Xu, Quanquan Gu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "USPSGD",
            "Using for": "Optimization, Active Learning",
            "Based on": "SGD",
            "Method": "Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss",
            "Description": "Uncertainty sampling, a popular active learning algorithm, is used to reduce the amount of data required to learn a classifier, but it has been observed in practice to converge to different parameters depending on the initialization and sometimes to even better parameters than standard training on all the data. In this work, we give a theoretical explanation of this phenomenon, showing that uncertainty sampling on a convex (e.g., logistic) loss can be interpreted as performing a preconditioned stochastic gradient step on the population zero-one loss. Experiments on synthetic and real datasets support this connection.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7927-uncertainty-sampling-is-preconditioned-stochastic-gradient-descent-on-zero-one-loss.pdf",
            "Publication name": "Uncertainty Sampling is Preconditioned Stochastic Gradient Descent on Zero-One Loss",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Stephen Mussmann, Percy S. Liang",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BEHPB",
            "Using for": "Reinforcement learning, multi armed bandit",
            "Based on": "BM",
            "Method": "Bayesian explanation of human pessimism bias ",
            "Description": "How humans make repeated choices among options with imperfectly known reward outcomes is an important problem in psychology and neuroscience. This is often studied using multi-armed bandits, which is also frequently studied in machine learning. We present data from a human stationary bandit experiment, in which we vary the average abundance and variability of reward availability (mean and variance of reward rate distributions). Surprisingly, we find subjects significantly underestimate prior mean of reward rates \u2013 based on their self-report, at the end of a game, on their reward expectation of non-chosen arms. Previously, human learning in the bandit task was found to be well captured by a Bayesian ideal learning model, the Dynamic Belief Model (DBM), albeit under an incorrect generative assumption of the temporal structure \u2013 humans assume reward rates can change over time even though they are truly fixed. We find that the \u201cpessimism bias\u201d in the bandit task is well captured by the prior mean of DBM when fitted to human choices; but it is poorly captured by the prior mean of the Fixed Belief Model (FBM), an alternative Bayesian model that (correctly) assumes reward rates to be constants. This pessimism bias is also incompletely captured by a simple reinforcement learning model (RL) commonly used in neuroscience and psychology, in terms of fitted initial Q-values. While it seems sub-optimal, and thus mysterious, that humans have an underestimated prior reward expectation, our simulations show that an underestimated prior mean helps to maximize long-term gain, if the observer assumes volatility when reward rates are stable and utilizes a softmax decision policy instead of the optimal one (obtainable by dynamic programming). This raises the intriguing possibility that the brain underestimates reward rates to compensate for the incorrect non-stationarity assumption in the generative model and a simplified decision policy.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7764-why-so-gloomy-a-bayesian-explanation-of-human-pessimism-bias-in-the-multi-armed-bandit-task.pdf",
            "Publication name": "Why so gloomy? A Bayesian explanation of human pessimism bias in the multi-armed bandit task",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Dalin Guo, Angela J. Yu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "OSL",
            "Using for": "Similarity, Classification",
            "Based on": "LSH\n",
            "Method": "Optimal Sparse Lifting",
            "Description": "Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in largescale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7302-fast-similarity-search-via-optimal-sparse-lifting.pdf",
            "Publication name": "Fast Similarity Search via Optimal Sparse Lifting",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Wenye Li, Jingwei Mao, Yin Zhang, Shuguang Cui",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DNNBNS",
            "Using for": "Unsupervised learning",
            "Based on": "CNN, BAYSEM",
            "Method": "Deep Neural Networks by Bayesian Network Structure Learning",
            "Description": "We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy\u2014state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7568-constructing-deep-neural-networks-by-bayesian-network-structure-learning.pdf",
            "Publication name": "Constructing Deep Neural Networks by Bayesian Network Structure Learning",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "QwNN",
            "Using for": "Reinforcement Learning",
            "Based on": "Q-L, KNN",
            "Method": "Q-learning with Nearest Neighbors",
            "Description": "We consider model-free reinforcement learning for infinite-horizon discounted Markov Decision Processes (MDPs) with a continuous state space and unknown transition kernel, when only a single sample path under an arbitrary policy of the system is available. We consider the Nearest Neighbor Q-Learning (NNQL) algorithm to learn the optimal Q function using nearest neighbor regression method. As the main contribution, we provide tight finite sample analysis of the convergence rate. In particular, for MDPs with a d-dimensional state space and the discounted factor 2 (0, 1), given an arbitrary sample path with \u201ccovering time\u201d L, we establish that the algorithm is guaranteed to output an \"-accurate estimate of the optimal Q-function using Oe L/(\"3(1 )7) samples. For instance, for a wellbehaved MDP, the covering time of the sample path under the purely random policy scales as Oe 1/\"d , so the sample complexity scales as Oe 1/\"d+3 . Indeed, we establish a lower bound that argues that the dependence of \u2326e 1/\"d+2 is necessary.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7574-q-learning-with-nearest-neighbors.pdf",
            "Publication name": "Q-learning with Nearest Neighbors",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Devavrat Shah Massachusetts Institute of Technology devavrat@mit.edu",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GRMG",
            "Using for": "Adversarial Networks",
            "Based on": "GAN",
            "Method": "Gradient Regularizers for MMD GANs",
            "Description": "We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on 160 \u00d7 160 CelebA and 64 \u00d7 64 unconditional ImageNet.",
            "Publication date": 2018.0,
            "Publication link": "https://papers.nips.cc/paper/7904-on-gradient-regularizers-for-mmd-gans.pdf",
            "Publication name": "On gradient regularizers for MMD GANs",
            "Released at": "NIPS",
            "Interacts with": "NaN",
            "Authors": "Michael Arbel Gatsby Computational Neuroscience Unit",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "ADVELT",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BAYSEM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BILSTM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BM",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BO",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "BSD",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DNN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DNNs",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DOMAP",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "DOOMAX",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "GRU",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "HOGWILD!-SGD",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "K-NNs",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "LCB",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "MCMC",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "PASP",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "POOL",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "R-CNN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RESN",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RESNED ENCDR",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "RNNs",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "S2S",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "Spatio-Temporal Low Count Processes",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "Text ENCDR",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VI",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        },
        {
            "id": "VRT",
            "Using for": "NaN",
            "Based on": "NaN",
            "Method": "NaN",
            "Description": "NaN",
            "Publication date": "NaN",
            "Publication link": "NaN",
            "Publication name": "NaN",
            "Released at": "NaN",
            "Authors": "NaN",
            "Code link": "NaN",
            "Data": "NaN"
        }
    ],
    "links": [
        {
            "source": "LSTM",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "STAC",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "KN",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "KN",
            "target": "GN",
            "type": "a"
        },
        {
            "source": "KN",
            "target": "NCMN",
            "type": "a"
        },
        {
            "source": "KN",
            "target": "DRPT",
            "type": "a"
        },
        {
            "source": "HOGWILDG",
            "target": "HOGWILD!-SGD",
            "type": "a"
        },
        {
            "source": "AE",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "AE",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "GAN",
            "target": "GENN",
            "type": "a"
        },
        {
            "source": "GAN",
            "target": "DIS",
            "type": "a"
        },
        {
            "source": "TADIS",
            "target": "DIS",
            "type": "a"
        },
        {
            "source": "TAGAN",
            "target": "GENN",
            "type": "a"
        },
        {
            "source": "TAGAN",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "TAGAN",
            "target": "Text ENCDR",
            "type": "a"
        },
        {
            "source": "TAGAN",
            "target": "TADIS",
            "type": "a"
        },
        {
            "source": "IVI",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "ADAPTHISTLOSS",
            "target": "ADEM",
            "type": "a"
        },
        {
            "source": "ADAPTHISTLOSS",
            "target": "HISTLOSS",
            "type": "a"
        },
        {
            "source": "ADAPTPROTONET",
            "target": "ADEM",
            "type": "a"
        },
        {
            "source": "ADAPTPROTONET",
            "target": "PROTONET",
            "type": "a"
        },
        {
            "source": "EMPW",
            "target": "EMPTD",
            "type": "a"
        },
        {
            "source": "ACE",
            "target": "AC",
            "type": "a"
        },
        {
            "source": "ACE",
            "target": "EMPW",
            "type": "a"
        },
        {
            "source": "SAE",
            "target": "LNN",
            "type": "a"
        },
        {
            "source": "SAE",
            "target": "AE",
            "type": "a"
        },
        {
            "source": "DCGAN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DCGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "WGAN-GP",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "LSGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "RESNED",
            "target": "AE",
            "type": "a"
        },
        {
            "source": "RESNED",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "3D-GAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "VON",
            "target": "3D-GAN",
            "type": "a"
        },
        {
            "source": "VON",
            "target": "RESNED ENCDR",
            "type": "a"
        },
        {
            "source": "VON",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "WN-DNN",
            "target": "DNNs",
            "type": "a"
        },
        {
            "source": "HGRU",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "HGRU",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "SRFEN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "SRFEN",
            "target": "SCON",
            "type": "a"
        },
        {
            "source": "SRIRN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "SRIRN",
            "target": "DWT",
            "type": "a"
        },
        {
            "source": "SRCLIN",
            "target": "SRFEN",
            "type": "a"
        },
        {
            "source": "SRCLIN",
            "target": "SRIRN",
            "type": "a"
        },
        {
            "source": "OPSL",
            "target": "LSH",
            "type": "a"
        },
        {
            "source": "FASF",
            "target": "OPSL",
            "type": "a"
        },
        {
            "source": "QMC",
            "target": "MC",
            "type": "a"
        },
        {
            "source": "GCMC",
            "target": "MC",
            "type": "a"
        },
        {
            "source": "GCMC",
            "target": "QMC",
            "type": "a"
        },
        {
            "source": "CHSU",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "CHSU",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "CHSU",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "EPA",
            "target": "DP",
            "type": "a"
        },
        {
            "source": "EPA",
            "target": "SPAV",
            "type": "a"
        },
        {
            "source": "SATNET",
            "target": "SATSEENET",
            "type": "a"
        },
        {
            "source": "SATNET",
            "target": "SATTHNET",
            "type": "a"
        },
        {
            "source": "COR",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "COR",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "SSMAX",
            "target": "SIGM",
            "type": "a"
        },
        {
            "source": "SSMAX",
            "target": "SMAX",
            "type": "a"
        },
        {
            "source": "METAAN",
            "target": "RCNN",
            "type": "a"
        },
        {
            "source": "CE",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "CE",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "CA",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "CA",
            "target": "CE",
            "type": "a"
        },
        {
            "source": "GMCNN",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "GMCNN",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "GMCNN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "GMCNN",
            "target": "WGAN-GP",
            "type": "a"
        },
        {
            "source": "A^2NET",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "PCONCOV",
            "target": "CONCOV",
            "type": "a"
        },
        {
            "source": "VAE",
            "target": "ELBO",
            "type": "a"
        },
        {
            "source": "IRG",
            "target": "VRT",
            "type": "a"
        },
        {
            "source": "SGWG",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "RSNvsLP",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "DDPAE",
            "target": "AE",
            "type": "a"
        },
        {
            "source": "MLTASMOO",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "GCN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "COGCNTS",
            "target": "GCN",
            "type": "a"
        },
        {
            "source": "COGCNTS",
            "target": "TS",
            "type": "a"
        },
        {
            "source": "VGG",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "SEENET",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "SEENET",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "SEENET",
            "target": "VGG",
            "type": "a"
        },
        {
            "source": "SEENET",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "LinkNet",
            "target": "GLEM",
            "type": "a"
        },
        {
            "source": "LinkNet",
            "target": "GCEM",
            "type": "a"
        },
        {
            "source": "HitNet",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "HitNet",
            "target": "GRU",
            "type": "a"
        },
        {
            "source": "NCMN",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "OLMC",
            "target": "MCMC",
            "type": "a"
        },
        {
            "source": "DBIIR",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "DBIIR",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "DBIIR",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DBIIR",
            "target": "GRU",
            "type": "a"
        },
        {
            "source": "DBIIR",
            "target": "K-NNs",
            "type": "a"
        },
        {
            "source": "SPIDER",
            "target": "NGD",
            "type": "a"
        },
        {
            "source": "SPIDER+",
            "target": "NGD",
            "type": "a"
        },
        {
            "source": "GANES",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "FRCNN",
            "target": "RPN",
            "type": "a"
        },
        {
            "source": "FRCNN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "CoordConv",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "QSGD",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "PQASGD",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "PQASGD",
            "target": "QSGD",
            "type": "a"
        },
        {
            "source": "DCMA",
            "target": "DNNs",
            "type": "a"
        },
        {
            "source": "INFOGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "\u03b2-VAE",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "JOINTVAE",
            "target": "\u03b2-VAE",
            "type": "a"
        },
        {
            "source": "TEN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "TEN",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "TEN",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "TADAM",
            "target": "TEN",
            "type": "a"
        },
        {
            "source": "TADAM",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "TADAM",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "TADAM",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "TADAM",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "STLCP",
            "target": "Spatio-Temporal Low Count Processes",
            "type": "a"
        },
        {
            "source": "FISHNet",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "FISHNet",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "RPF",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "RPF",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "VISMEM",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "VISMEM",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "KDGAN",
            "target": "KD",
            "type": "a"
        },
        {
            "source": "KDGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "RSNNs",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "RSNNs",
            "target": "SPIKN",
            "type": "a"
        },
        {
            "source": "LSNNs",
            "target": "RSNNs",
            "type": "a"
        },
        {
            "source": "LSNNs",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "DSDH",
            "target": "KNN",
            "type": "a"
        },
        {
            "source": "DTSH",
            "target": "KNN",
            "type": "a"
        },
        {
            "source": "GREH",
            "target": "DSDH",
            "type": "a"
        },
        {
            "source": "GREH",
            "target": "DTSH",
            "type": "a"
        },
        {
            "source": "INFF",
            "target": "Rel-MMD",
            "type": "a"
        },
        {
            "source": "INFF",
            "target": "IDFMTP",
            "type": "a"
        },
        {
            "source": "INFF",
            "target": "LTKG",
            "type": "a"
        },
        {
            "source": "X-CONV",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "X-CONV",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "POINTCNN",
            "target": "X-CONV",
            "type": "a"
        },
        {
            "source": "POINTCNN",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "INCVCV",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "ENCTC",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "ENCTC",
            "target": "INCVCV",
            "type": "a"
        },
        {
            "source": "DAL",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "DAL",
            "target": "RELU",
            "type": "a"
        },
        {
            "source": "DCP",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DCP",
            "target": "DAL",
            "type": "a"
        },
        {
            "source": "RCL",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "UA",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "UA",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "UA",
            "target": "VARINF",
            "type": "a"
        },
        {
            "source": "DropMax",
            "target": "DRPT",
            "type": "a"
        },
        {
            "source": "DropMax",
            "target": "SMAX",
            "type": "a"
        },
        {
            "source": "DropMax",
            "target": "VARINF",
            "type": "a"
        },
        {
            "source": "SS-DL",
            "target": "RELU",
            "type": "a"
        },
        {
            "source": "SS-DL",
            "target": "BAYESM",
            "type": "a"
        },
        {
            "source": "BPR+",
            "target": "BAYESM",
            "type": "a"
        },
        {
            "source": "DBPR+",
            "target": "BPR+",
            "type": "a"
        },
        {
            "source": "DBPR+",
            "target": "DNN",
            "type": "a"
        },
        {
            "source": "ERMNLD",
            "target": "Eps-LDP",
            "type": "a"
        },
        {
            "source": "NBS",
            "target": "PROTONET",
            "type": "a"
        },
        {
            "source": "CP-AAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "CP-AAN",
            "target": "MITIT",
            "type": "a"
        },
        {
            "source": "CP-AAN",
            "target": "NBS",
            "type": "a"
        },
        {
            "source": "WMD",
            "target": "WASSD",
            "type": "a"
        },
        {
            "source": "VSF",
            "target": "WMD",
            "type": "a"
        },
        {
            "source": "VSF",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "VSFRR",
            "target": "VSF",
            "type": "a"
        },
        {
            "source": "VSFRR",
            "target": "LRR",
            "type": "a"
        },
        {
            "source": "DIPL",
            "target": "USE",
            "type": "a"
        },
        {
            "source": "DIPL",
            "target": "SCR",
            "type": "a"
        },
        {
            "source": "NS-VQA",
            "target": "R-CNN",
            "type": "a"
        },
        {
            "source": "NS-VQA",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "NS-VQA",
            "target": "S2S",
            "type": "a"
        },
        {
            "source": "NS-VQA",
            "target": "BILSTM",
            "type": "a"
        },
        {
            "source": "NS-VQA",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "BSG",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "MVP",
            "target": "BSD",
            "type": "a"
        },
        {
            "source": "NNNN",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "NNNN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "NNNN",
            "target": "KNN",
            "type": "a"
        },
        {
            "source": "ISL",
            "target": "QBC",
            "type": "a"
        },
        {
            "source": "VIT",
            "target": "MDP",
            "type": "a"
        },
        {
            "source": "Q-L",
            "target": "MDP",
            "type": "a"
        },
        {
            "source": "PCQL",
            "target": "Q-L",
            "type": "a"
        },
        {
            "source": "PCQL",
            "target": "INFOS",
            "type": "a"
        },
        {
            "source": "PCVI",
            "target": "VIT",
            "type": "a"
        },
        {
            "source": "PCVI",
            "target": "INFOS",
            "type": "a"
        },
        {
            "source": "I3D",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "VTV",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "VTV",
            "target": "PROGL",
            "type": "a"
        },
        {
            "source": "NCSGD",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "SYNPO",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "ADVVUL",
            "target": "ADVPER",
            "type": "a"
        },
        {
            "source": "ERL",
            "target": "DDPG",
            "type": "a"
        },
        {
            "source": "ERL",
            "target": "EA",
            "type": "a"
        },
        {
            "source": "TODLEAR",
            "target": "VGG",
            "type": "a"
        },
        {
            "source": "TAO",
            "target": "DT",
            "type": "a"
        },
        {
            "source": "SIREID",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "FD-GAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "FD-GAN",
            "target": "SIREID",
            "type": "a"
        },
        {
            "source": "FD-GAN",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "HSGD",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "WORSHSGD",
            "target": "HSGD",
            "type": "a"
        },
        {
            "source": "ULVIAR",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "ULVIAR",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "ULVIAR",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "ULVIAR",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "GM-SOP",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "CDA",
            "target": "AE",
            "type": "a"
        },
        {
            "source": "CDD",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "CDD",
            "target": "CDA",
            "type": "a"
        },
        {
            "source": "GSCEDO",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "ROCK",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "ROCK",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "ROCK",
            "target": "RESN",
            "type": "a"
        },
        {
            "source": "TRNSFRMR",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "FRAGE",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "FRAGE",
            "target": "DIS",
            "type": "a"
        },
        {
            "source": "VNMT",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "VNMT",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "VNMT",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "GNMT",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "GNMT",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "GNMT",
            "target": "VNMT",
            "type": "a"
        },
        {
            "source": "UMVC",
            "target": "SBM",
            "type": "a"
        },
        {
            "source": "JCVE",
            "target": "Q-L",
            "type": "a"
        },
        {
            "source": "RLNs",
            "target": "COUL",
            "type": "a"
        },
        {
            "source": "SMTBoost",
            "target": "BOOST",
            "type": "a"
        },
        {
            "source": "SUGAR",
            "target": "DM",
            "type": "a"
        },
        {
            "source": "SLAYER",
            "target": "SPIKN",
            "type": "a"
        },
        {
            "source": "VALOR",
            "target": "LSVEE",
            "type": "a"
        },
        {
            "source": "BNN",
            "target": "BAYESM",
            "type": "a"
        },
        {
            "source": "BNN",
            "target": "DNN",
            "type": "a"
        },
        {
            "source": "SBN",
            "target": "MCMC",
            "type": "a"
        },
        {
            "source": "SBN",
            "target": "BNN",
            "type": "a"
        },
        {
            "source": "IDME",
            "target": "Q-L",
            "type": "a"
        },
        {
            "source": "AccNet",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "AccNet",
            "target": "SBS",
            "type": "a"
        },
        {
            "source": "AIMT",
            "target": "AMT",
            "type": "a"
        },
        {
            "source": "CFFAD",
            "target": "SVM",
            "type": "a"
        },
        {
            "source": "CADL",
            "target": "CFFAD",
            "type": "a"
        },
        {
            "source": "CADL",
            "target": "SVM",
            "type": "a"
        },
        {
            "source": "PacGAN",
            "target": "DCGAN",
            "type": "a"
        },
        {
            "source": "VMED",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "VMED",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "VMED",
            "target": "MANNs",
            "type": "a"
        },
        {
            "source": "SCMD",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "HRGR-A",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "HRGR-A",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "HRGR-A",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "HRGR-A",
            "target": "REINFORCE",
            "type": "a"
        },
        {
            "source": "VQAAR",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "VQAAR",
            "target": "ADVELT",
            "type": "a"
        },
        {
            "source": "Bottom-Up",
            "target": "FRCNN",
            "type": "a"
        },
        {
            "source": "BAN",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "Conv-MKL",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "SMSD-MKL",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "GRUI",
            "target": "GRU",
            "type": "a"
        },
        {
            "source": "SVCO",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "CVF",
            "target": "SVCO",
            "type": "a"
        },
        {
            "source": "ASMI",
            "target": "BFGS",
            "type": "a"
        },
        {
            "source": "DifNet",
            "target": "RANDW",
            "type": "a"
        },
        {
            "source": "CDANs",
            "target": "DOMAP",
            "type": "a"
        },
        {
            "source": "CDANs",
            "target": "ADVELT",
            "type": "a"
        },
        {
            "source": "RECHRISF",
            "target": "CHRISF",
            "type": "a"
        },
        {
            "source": "NLRN",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "GPP",
            "target": "BAYESM",
            "type": "a"
        },
        {
            "source": "GPP",
            "target": "GP",
            "type": "a"
        },
        {
            "source": "VPSS",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "VPSS",
            "target": "AE",
            "type": "a"
        },
        {
            "source": "VPSS",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DWL",
            "target": "WASSD",
            "type": "a"
        },
        {
            "source": "PPO",
            "target": "TRPO",
            "type": "a"
        },
        {
            "source": "G2Ns",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "G2Ns",
            "target": "GENA",
            "type": "a"
        },
        {
            "source": "UCRL",
            "target": "MDP",
            "type": "a"
        },
        {
            "source": "LINUCRL",
            "target": "UCRL",
            "type": "a"
        },
        {
            "source": "PLPMBL",
            "target": "MARBL",
            "type": "a"
        },
        {
            "source": "ARS",
            "target": "BRS",
            "type": "a"
        },
        {
            "source": "AIM",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "AIM",
            "target": "VIMO",
            "type": "a"
        },
        {
            "source": "LBR",
            "target": "LLL",
            "type": "a"
        },
        {
            "source": "DVAE++",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DVAE++",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "DVAE#",
            "target": "DVAE++",
            "type": "a"
        },
        {
            "source": "3D-SDN",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "3D-SDN",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "F",
            "target": "MCMC",
            "type": "a"
        },
        {
            "source": "SVRG",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "SVRGOL",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "SVRGOL",
            "target": "SVRG",
            "type": "a"
        },
        {
            "source": "VIRSMAX",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "VIRSMAX",
            "target": "SMAX",
            "type": "a"
        },
        {
            "source": "ConvLSTM",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "ConvLSTM",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "ATTNConvLSTM",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "ATTNConvLSTM",
            "target": "ConvLSTM",
            "type": "a"
        },
        {
            "source": "DENSENET",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DENSENET",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "PELEENET",
            "target": "DENSENET",
            "type": "a"
        },
        {
            "source": "BMS",
            "target": "BAYESM",
            "type": "a"
        },
        {
            "source": "HSG-HT",
            "target": "HSGD",
            "type": "a"
        },
        {
            "source": "PCA",
            "target": "SVD",
            "type": "a"
        },
        {
            "source": "ACA",
            "target": "PCA",
            "type": "a"
        },
        {
            "source": "SplineNets",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "SplineNets",
            "target": "ACA",
            "type": "a"
        },
        {
            "source": "GZSL",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "GZSL",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "DCN",
            "target": "GZSL",
            "type": "a"
        },
        {
            "source": "NASBOT",
            "target": "GP",
            "type": "a"
        },
        {
            "source": "NASBOT",
            "target": "OTMANN",
            "type": "a"
        },
        {
            "source": "GQEs",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "TS2CE",
            "target": "RNN",
            "type": "a"
        },
        {
            "source": "TS2CE",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "TS2CE",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "KeypointNet",
            "target": "RPEL",
            "type": "a"
        },
        {
            "source": "KeypointNet",
            "target": "MVCL",
            "type": "a"
        },
        {
            "source": "VB",
            "target": "BO",
            "type": "a"
        },
        {
            "source": "VB",
            "target": "VARINF",
            "type": "a"
        },
        {
            "source": "aNPL",
            "target": "MC",
            "type": "a"
        },
        {
            "source": "aNPL",
            "target": "VB",
            "type": "a"
        },
        {
            "source": "SEGA",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "PCCoder",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "PCCoder",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "PCCoder",
            "target": "POOL",
            "type": "a"
        },
        {
            "source": "OST",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "OST",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "OST",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "OST",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "VCL",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "SVM-cone",
            "target": "SVM",
            "type": "a"
        },
        {
            "source": "MGPP",
            "target": "GP",
            "type": "a"
        },
        {
            "source": "WGAN",
            "target": "WASSD",
            "type": "a"
        },
        {
            "source": "WGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "DeepExposure",
            "target": "DDPG",
            "type": "a"
        },
        {
            "source": "DeepExposure",
            "target": "WGAN",
            "type": "a"
        },
        {
            "source": "DeepExposure",
            "target": "DIS",
            "type": "a"
        },
        {
            "source": "NORMT",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "DPCP",
            "target": "PCA",
            "type": "a"
        },
        {
            "source": "DPCP-PSGM",
            "target": "DPCP",
            "type": "a"
        },
        {
            "source": "MMC",
            "target": "HRMC",
            "type": "a"
        },
        {
            "source": "3DCNN",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "TRJCTRN",
            "target": "3DCNN",
            "type": "a"
        },
        {
            "source": "DLDL",
            "target": "VARINF",
            "type": "a"
        },
        {
            "source": "SAGA",
            "target": "NECB",
            "type": "a"
        },
        {
            "source": "IAP",
            "target": "AP",
            "type": "a"
        },
        {
            "source": "PADCOD",
            "target": "PAC",
            "type": "a"
        },
        {
            "source": "GenRe",
            "target": "SHPNT",
            "type": "a"
        },
        {
            "source": "GMM",
            "target": "GP",
            "type": "a"
        },
        {
            "source": "BourGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "BourGAN",
            "target": "GP",
            "type": "a"
        },
        {
            "source": "BourGAN",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "LASSDP",
            "target": "PASP",
            "type": "a"
        },
        {
            "source": "OOMDP",
            "target": "MDP",
            "type": "a"
        },
        {
            "source": "DOORMAX",
            "target": "OOMDP",
            "type": "a"
        },
        {
            "source": "DOOMDP",
            "target": "OOMDP",
            "type": "a"
        },
        {
            "source": "DOORMAXD",
            "target": "DOOMAX",
            "type": "a"
        },
        {
            "source": "DOORMAXD",
            "target": "DOOMDP",
            "type": "a"
        },
        {
            "source": "RBCRR",
            "target": "PERT",
            "type": "a"
        },
        {
            "source": "Prox-SVRG",
            "target": "SARAH",
            "type": "a"
        },
        {
            "source": "Prox-SVRG",
            "target": "SVRG",
            "type": "a"
        },
        {
            "source": "GIANT",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "GIANT",
            "target": "ANT",
            "type": "a"
        },
        {
            "source": "CCRMs",
            "target": "CRM",
            "type": "a"
        },
        {
            "source": "HCCRMs",
            "target": "CCRMs",
            "type": "a"
        },
        {
            "source": "HCCRMs",
            "target": "HIRM",
            "type": "a"
        },
        {
            "source": "BLITS",
            "target": "P-Fantom",
            "type": "a"
        },
        {
            "source": "MetaGAN",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "THRESH",
            "target": "RAPPOR",
            "type": "a"
        },
        {
            "source": "GP-CDE",
            "target": "GP",
            "type": "a"
        },
        {
            "source": "GP-CDE",
            "target": "CDE",
            "type": "a"
        },
        {
            "source": "MTGRDNT",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "MTGRDNT",
            "target": "UVFA",
            "type": "a"
        },
        {
            "source": "MDLRN",
            "target": "AMLE",
            "type": "a"
        },
        {
            "source": "MDLRN",
            "target": "OLNN",
            "type": "a"
        },
        {
            "source": "CITYNAV",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "CITYNAV",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "CITYNAV",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "CITYNAV",
            "target": "IMPALA",
            "type": "a"
        },
        {
            "source": "QCBPL",
            "target": "BO",
            "type": "a"
        },
        {
            "source": "QCBPL",
            "target": "PSL",
            "type": "a"
        },
        {
            "source": "WRLDMDLS",
            "target": "VAE",
            "type": "a"
        },
        {
            "source": "WRLDMDLS",
            "target": "RNNs",
            "type": "a"
        },
        {
            "source": "DRLS",
            "target": "PDLSS",
            "type": "a"
        },
        {
            "source": "WVI",
            "target": "VARINF",
            "type": "a"
        },
        {
            "source": "WVI",
            "target": "WASSD",
            "type": "a"
        },
        {
            "source": "WAE",
            "target": "AE",
            "type": "a"
        },
        {
            "source": "WAE",
            "target": "WASSD",
            "type": "a"
        },
        {
            "source": "WVA",
            "target": "WAE",
            "type": "a"
        },
        {
            "source": "WVA",
            "target": "WVI",
            "type": "a"
        },
        {
            "source": "BNHO",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "Q-DAGGER",
            "target": "Q-L",
            "type": "a"
        },
        {
            "source": "Q-DAGGER",
            "target": "DAGGER",
            "type": "a"
        },
        {
            "source": "VIPER",
            "target": "Q-DAGGER",
            "type": "a"
        },
        {
            "source": "MAPLE",
            "target": "SILO",
            "type": "a"
        },
        {
            "source": "MAPLE",
            "target": "DStump",
            "type": "a"
        },
        {
            "source": "MONDTAR",
            "target": "MONDT",
            "type": "a"
        },
        {
            "source": "TRLSTM",
            "target": "DT",
            "type": "a"
        },
        {
            "source": "TRLSTM",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "T3N2",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "T3N2",
            "target": "TRLSTM",
            "type": "a"
        },
        {
            "source": "T3N2",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "T3N2",
            "target": "DCDR",
            "type": "a"
        },
        {
            "source": "T3N2",
            "target": "ATTN",
            "type": "a"
        },
        {
            "source": "BIN",
            "target": "BN",
            "type": "a"
        },
        {
            "source": "POMIS",
            "target": "SCM-MAB",
            "type": "a"
        },
        {
            "source": "AQM",
            "target": "TOM",
            "type": "a"
        },
        {
            "source": "UFDN",
            "target": "ENCDR",
            "type": "a"
        },
        {
            "source": "UFDN",
            "target": "GENN",
            "type": "a"
        },
        {
            "source": "ELBO",
            "target": "KLD",
            "type": "a"
        },
        {
            "source": "ELBO",
            "target": "VI",
            "type": "a"
        },
        {
            "source": "MIG",
            "target": "ELBO",
            "type": "a"
        },
        {
            "source": "\u03b2-TCVAE",
            "target": "MIG",
            "type": "a"
        },
        {
            "source": "\u03b2-TCVAE",
            "target": "\u03b2-VAE",
            "type": "a"
        },
        {
            "source": "HINGE-LMC",
            "target": "LMC",
            "type": "a"
        },
        {
            "source": "HINGE-LMC",
            "target": "HINGE",
            "type": "a"
        },
        {
            "source": "SMOOTHFTL",
            "target": "FTL",
            "type": "a"
        },
        {
            "source": "CBSL",
            "target": "LCB",
            "type": "a"
        },
        {
            "source": "CBSL",
            "target": "HINGE-LMC",
            "type": "a"
        },
        {
            "source": "CBSL",
            "target": "SMOOTHFTL",
            "type": "a"
        },
        {
            "source": "SITE",
            "target": "PDDM",
            "type": "a"
        },
        {
            "source": "SITE",
            "target": "MPDM",
            "type": "a"
        },
        {
            "source": "RepBM",
            "target": "MSE",
            "type": "a"
        },
        {
            "source": "RepBM",
            "target": "CATE",
            "type": "a"
        },
        {
            "source": "OOB",
            "target": "GCN",
            "type": "a"
        },
        {
            "source": "OOB",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "OOB",
            "target": "MLP",
            "type": "a"
        },
        {
            "source": "OOB",
            "target": "EMBD",
            "type": "a"
        },
        {
            "source": "GDMSPfEC",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "GDMSPfEC",
            "target": "PCA",
            "type": "a"
        },
        {
            "source": "GDMSPfEC",
            "target": "SVD",
            "type": "a"
        },
        {
            "source": "AORWS",
            "target": "GAN",
            "type": "a"
        },
        {
            "source": "NNBSRM",
            "target": "LSTM",
            "type": "a"
        },
        {
            "source": "NNBSRM",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "RELBO",
            "target": "ELBO",
            "type": "a"
        },
        {
            "source": "USPSGD",
            "target": "SGD",
            "type": "a"
        },
        {
            "source": "BEHPB",
            "target": "BM",
            "type": "a"
        },
        {
            "source": "OSL",
            "target": "LSH",
            "type": "a"
        },
        {
            "source": "DNNBNS",
            "target": "CNN",
            "type": "a"
        },
        {
            "source": "DNNBNS",
            "target": "BAYSEM",
            "type": "a"
        },
        {
            "source": "QwNN",
            "target": "Q-L",
            "type": "a"
        },
        {
            "source": "QwNN",
            "target": "KNN",
            "type": "a"
        },
        {
            "source": "GRMG",
            "target": "GAN",
            "type": "a"
        }
    ],
    "dict": {
        "3D Scene Parsing": [
            "PRIM"
        ],
        "3D pose estimation": [
            "KeypointNet"
        ],
        "Action Recognition": [
            "TRJCTRN"
        ],
        "Activation": [
            "WNLL",
            "DropMax",
            "SS-DL"
        ],
        "Activation Function": [
            "SIGM",
            "RELU",
            "SMAX",
            "SSMAX"
        ],
        "Active Learning": [
            "ISL",
            "MONDTAR",
            "USPSGD"
        ],
        "Activive Learning": [
            "MONDT"
        ],
        "Adversarial Attacks": [
            "PAC",
            "SARDNN",
            "HMAD"
        ],
        "Adversarial Examples": [
            "ADVVUL"
        ],
        "Adversarial Networks": [
            "GENN",
            "DIS",
            "GAN",
            "TADIS",
            "TAGAN",
            "DCGAN",
            "WGAN-GP",
            "LSGAN",
            "3D-GAN",
            "SGWG",
            "GANES",
            "INFOGAN",
            "KDGAN",
            "FD-GAN",
            "PacGAN",
            "AIM",
            "WGAN",
            "BourGAN",
            "GRMG"
        ],
        "Adversarial Robustness": [
            "PAC",
            "SARDNN",
            "DEDEF",
            "HMAD"
        ],
        "Anomaly Detection": [
            "CFFAD",
            "CADL"
        ],
        "Asynchronicity": [
            "HOGWILDG"
        ],
        "Asynchronicity. Functions estimating": [
            "HOGWILDS"
        ],
        "Attention": [
            "CA",
            "RECIL"
        ],
        "Autoencoder": [
            "WVI",
            "WVA"
        ],
        "Autoencoders": [
            "ENCDR",
            "DCDR",
            "AE",
            "DDPAE"
        ],
        "Automatic program synthesis": [
            "PCCoder"
        ],
        "Bandit Problem": [
            "MBWC"
        ],
        "Base-novel class pairs extracting": [
            "NBS"
        ],
        "Blind and off-grid recovery of echo locations": [
            "MULAN"
        ],
        "Boundary Detection": [
            "BMS"
        ],
        "Cassification": [
            "NNNN",
            "SGR"
        ],
        "Changepoint detection": [
            "RBOCPD"
        ],
        "Classification": [
            "CNN",
            "MLP",
            "STAC",
            "SCON",
            "FSTL",
            "VGG",
            "MEFGC",
            "KNN",
            "CoordConv",
            "DCMA",
            "FISHNet",
            "X-CONV",
            "POINTCNN",
            "INCVCV",
            "LMDN",
            "DAL",
            "DCP",
            "RCL",
            "UA",
            "DropMax",
            "TAO",
            "GM-SOP",
            "COUL",
            "RLNs",
            "SMTBoost",
            "SBS",
            "SVM",
            "Conv-MKL",
            "SMSD-MKL",
            "SVCO",
            "CVF",
            "MARBL",
            "VIRSMAX",
            "ConvLSTM",
            "DENSENET",
            "PELEENET",
            "SplineNets",
            "3DCNN",
            "HINGE",
            "HINGE-LMC",
            "CBSL",
            "OSL"
        ],
        "Clustering": [
            "SVM-cone",
            "PADCOD"
        ],
        "Connectionist Temporal Classification": [
            "ENCTC"
        ],
        "Continual Learning": [
            "RCL"
        ],
        "Convergence rate": [
            "MLD"
        ],
        "Convex Optimization": [
            "ADER"
        ],
        "Cross-view Action Recognition": [
            "ULVIAR"
        ],
        "DOMAP": [
            "CDANs"
        ],
        "Datasets modeling": [
            "GP-CDE"
        ],
        "Decomposable Submodular Function Minimization": [
            "AP",
            "IAP"
        ],
        "Deep Representation Learning": [
            "PDDM",
            "MPDM",
            "SITE"
        ],
        "Denoising": [
            "AccNet"
        ],
        "Detecting and modeling global patterns": [
            "MAPLE"
        ],
        "Detection": [
            "VLS"
        ],
        "Dialog System": [
            "DBIIR"
        ],
        "Dimensionality Reduction": [
            "DM"
        ],
        "Dimensionality reduction": [
            "PCA",
            "RANSAC",
            "DPCP",
            "DPCP-PSGM"
        ],
        "Dissimilarity": [
            "PIP",
            "VSF",
            "LRR",
            "VSFRR"
        ],
        "Distillation": [
            "KD",
            "KDGAN"
        ],
        "Domain Adaptation": [
            "METAREG"
        ],
        "Embedding": [
            "EMBD",
            "SRFEN",
            "CE",
            "USE",
            "FRAGE",
            "GQEs"
        ],
        "Empirical Risk Minimization": [
            "Eps-LDP",
            "ERMNLD"
        ],
        "Estimating individual\ntreatment effect": [
            "SITE"
        ],
        "Expose Photos": [
            "DeepExposure"
        ],
        "Fast Finite-Sum Minimization": [
            "Prox-SVRG"
        ],
        "Feature Selection": [
            "CEM"
        ],
        "Few-Shot Learning": [
            "PROTONET"
        ],
        "Few-shot Learning": [
            "MAML",
            "MetaGAN"
        ],
        "Few-shot learning": [
            "MATN",
            "TEN",
            "TADAM",
            "CP-AAN"
        ],
        "Functions estimating": [
            "HOGWILDG"
        ],
        "GP": [
            "ARD"
        ],
        "Generalization": [
            "SAE"
        ],
        "Generation": [
            "IVI",
            "VON",
            "SSG360",
            "JOINTVAE",
            "SUGAR",
            "PacGAN",
            "VMED",
            "HRGR-A",
            "DVAE++",
            "DVAE#",
            "BourGAN",
            "WVI",
            "WVA",
            "MIG",
            "\u03b2-TCVAE"
        ],
        "Goal-Oriented Visual Dialog": [
            "AQM"
        ],
        "Graph matching": [
            "GGM"
        ],
        "Hashing": [
            "DSDH",
            "DTSH",
            "GREH"
        ],
        "Health": [
            "UA"
        ],
        "Heterogeneity": [
            "HCCRMs"
        ],
        "High Resolution": [
            "FISHNet"
        ],
        "High-resolution": [
            "SRCLIN"
        ],
        "Image Captioning": [
            "PS3"
        ],
        "Image Description": [
            "GLEM",
            "GCEM",
            "LinkNet"
        ],
        "Image Inpainting": [
            "MSNPS",
            "GMCNN"
        ],
        "Image Recognition": [
            "A^2NET",
            "METAREG",
            "MDLRN",
            "BIN"
        ],
        "Image Report": [
            "HRGR-A"
        ],
        "Image Restoration": [
            "NLRN"
        ],
        "Image Search": [
            "DBIIR"
        ],
        "Image classification": [
            "TADAM"
        ],
        "Image manipulation": [
            "AORWS"
        ],
        "Image preprocessing": [
            "DWT"
        ],
        "Image-To-Image Translation": [
            "MITIT",
            "CP-AAN",
            "CDA",
            "CDD"
        ],
        "Inference": [
            "VARINF",
            "VB"
        ],
        "Information maximization": [
            "VIMO"
        ],
        "Initialization": [
            "FM1",
            "FM2"
        ],
        "Isotonic Regression": [
            "SPAV",
            "EPA"
        ],
        "Kernel Methods": [
            "RECHRISF"
        ],
        "Langevin dynamics": [
            "MLD"
        ],
        "Language Modeling": [
            "HitNet",
            "LSNNs"
        ],
        "Language Modeling.": [
            "MDLRN"
        ],
        "Language Modelling": [
            "NS-VQA"
        ],
        "Latent 3D Keypoints": [
            "MVCL",
            "RPEL",
            "KeypointNet"
        ],
        "Learning Optimal Reserve Price": [
            "LORPNB"
        ],
        "Learning from point clouds": [
            "X-CONV",
            "POINTCNN"
        ],
        "Linear Contextual Bandit Problem": [
            "NECB",
            "SAGA"
        ],
        "Loss function": [
            "MSE"
        ],
        "Machine Translation": [
            "HitNet",
            "LSNNs",
            "TRNSFRMR",
            "VNMT",
            "GNMT"
        ],
        "Manipulating images using natural language description": [
            "TADIS",
            "TAGAN"
        ],
        "Measure": [
            "WAADM"
        ],
        "Meta-Learning": [
            "METAREG"
        ],
        "Metric": [
            "WASSD",
            "WMD",
            "TSPRE",
            "TSRE",
            "MIG",
            "MSE"
        ],
        "Misinformation Containmen": [
            "CASP"
        ],
        "Model Comparison": [
            "Rel-MMD",
            "INFF"
        ],
        "Multi-Armed Bandit": [
            "SCM-MAB",
            "POMIS"
        ],
        "Multi-Class Learning": [
            "Conv-MKL",
            "SMSD-MKL"
        ],
        "Multi-Task Learning": [
            "ADGB",
            "MLTWL",
            "MLTASMOO"
        ],
        "Multi-domain image translation": [
            "UFDN"
        ],
        "Multiagent scenarios": [
            "DBPR+"
        ],
        "Navigation": [
            "CITYNAV"
        ],
        "Neural Architecture Search": [
            "OTMANN",
            "NASBOT"
        ],
        "Neural Machine Translation": [
            "NNBSRM"
        ],
        "Normalization": [
            "BN",
            "GN",
            "KN",
            "WN-DNN",
            "NCMN",
            "NORMT",
            "BNHO"
        ],
        "Obect detection": [
            "TS2CE"
        ],
        "Object Detection": [
            "RCNN",
            "METAAN",
            "RPN",
            "FRCNN",
            "ROCK",
            "HKRM"
        ],
        "Object recognition": [
            "TODLEAR"
        ],
        "Off-policy policy evaluation": [
            "RepBM"
        ],
        "One-Shot Learning": [
            "OST"
        ],
        "Online Convex Optimization": [
            "FTL"
        ],
        "Online Learning": [
            "FICOF",
            "FIOL",
            "ADER",
            "OLUFM",
            "FTL"
        ],
        "Optimization": [
            "HOGWILDS",
            "HOGWILDG",
            "CONCOV",
            "PCONCOV",
            "SRS",
            "NGD",
            "SNVRG",
            "SGD",
            "SPIDER",
            "SPIDER+",
            "QSGD",
            "PQASGD",
            "BSG",
            "NCSGD",
            "HSGD",
            "WORSHSGD",
            "LINOG",
            "GSCEDO",
            "SLAYER",
            "GDSNN",
            "SBN",
            "SCMD",
            "BFGS",
            "ASMI",
            "SVRG",
            "SVRGOL",
            "HSG-HT",
            "SEGA",
            "VCL",
            "ANT",
            "GIANT",
            "TAS2N2O",
            "BNHO",
            "FTL",
            "HINGE",
            "HINGE-LMC",
            "SMOOTHFTL",
            "GDMSPfEC",
            "NsCPLC",
            "RELBO",
            "FSOAFLM",
            "USPSGD"
        ],
        "Outlier Detection": [
            "PADCOD"
        ],
        "PageRank": [
            "QDSFM"
        ],
        "Person Re-identification": [
            "SIREID",
            "FD-GAN"
        ],
        "Pipelines Learning": [
            "N&B"
        ],
        "Planted Vertex Cover Problem": [
            "UMVC"
        ],
        "Pose estimation": [
            "FISHNet"
        ],
        "Prediction": [
            "MANNs"
        ],
        "Privacy": [
            "RAPPOR"
        ],
        "Private Learning": [
            "PSL",
            "QCBPL"
        ],
        "Probabilistic Methods": [
            "BLR"
        ],
        "Program translation": [
            "T3N2"
        ],
        "Pruning": [
            "BA-FDNP"
        ],
        "Quadratic Assignment Problem": [
            "GGM"
        ],
        "Quality measures": [
            "F"
        ],
        "Query by Commite": [
            "ISL"
        ],
        "Question Answering": [
            "HitNet",
            "LSNNs"
        ],
        "Question-Answering": [
            "NS-VQA",
            "ISL"
        ],
        "Real Time Speech Recognition": [
            "NNBSRM"
        ],
        "Reasoning": [
            "COR"
        ],
        "Recognition": [
            "HGRU",
            "SCR",
            "ATTNConvLSTM",
            "DENSENET",
            "PELEENET"
        ],
        "Recommendation Systems": [
            "SCP"
        ],
        "Recommendation system": [
            "BLITS"
        ],
        "Recommender systems": [
            "LINUCRL",
            "HRMC",
            "MMC"
        ],
        "Reconstruct Shapes": [
            "GenRe"
        ],
        "Reconstruction": [
            "SRIRN"
        ],
        "Recurrent Networks": [
            "LSTM",
            "RNN",
            "TRLSTM"
        ],
        "Regression": [
            "FOSD",
            "MLP",
            "BLR",
            "NIR",
            "KNN",
            "CoordConv",
            "DCMA",
            "SURF",
            "HTDL1R",
            "SVM",
            "LLL",
            "LBR",
            "ConvLSTM",
            "MGPP",
            "DEREJS",
            "SILO",
            "MONDTAR"
        ],
        "Regularization": [
            "ARINDNM",
            "DRPT",
            "DropMax",
            "SS-DL",
            "METAREG",
            "VCL"
        ],
        "Reinforcement Learning": [
            "GTD",
            "LSTD",
            "EMPTD",
            "AC",
            "EMPW",
            "ACE",
            "DBIIR",
            "CoordConv",
            "RPF",
            "VISMEM",
            "MVP",
            "MDP",
            "VIT",
            "Q-L",
            "INFOS",
            "PCQL",
            "PCVI",
            "SYNPO",
            "DDPG",
            "ERL",
            "LSVEE",
            "VALOR",
            "IDME",
            "REINFORCE",
            "HRGR-A",
            "TRPO",
            "PPO",
            "G2Ns",
            "UCRL",
            "TEMPR",
            "ARS",
            "OOMDP",
            "DOORMAX",
            "DOOMDP",
            "DOORMAXD",
            "IMPALA",
            "UVFA",
            "MTGRDNT",
            "CITYNAV",
            "WRLDMDLS",
            "Q-DAGGER",
            "VIPER",
            "CATE",
            "RepBM",
            "QwNN"
        ],
        "Reinforcement learning": [
            "BEHPB"
        ],
        "Relational Learning": [
            "RELN"
        ],
        "Reparameterization": [
            "IRG"
        ],
        "Representations": [
            "HBFP"
        ],
        "Risk-sensitive Tasks": [
            "MVP"
        ],
        "Sampling": [
            "MC",
            "QMC",
            "GCMC",
            "aNPL"
        ],
        "Scene Manipulation": [
            "3D-SDN"
        ],
        "Scene Understanding": [
            "CHSU"
        ],
        "Second-Order Optimization": [
            "ASMI"
        ],
        "Segmentation": [
            "SEENET",
            "X-CONV",
            "POINTCNN",
            "DifNet",
            "FCJS"
        ],
        "Semantic Scene Completion": [
            "SATSEENET",
            "SATTHNET",
            "SSCSDI",
            "SATNET"
        ],
        "Semantic Segmentation": [
            "SGR"
        ],
        "Semi-supervised Learning": [
            "QDSFM",
            "PLPMBL"
        ],
        "Semidefinite Programs": [
            "PASDP",
            "LASSDP"
        ],
        "Sequential Learning": [
            "HTQF"
        ],
        "Sequential decision making": [
            "EAFHDM"
        ],
        "Similarity": [
            "HISTLOSS",
            "PIP",
            "PDDM",
            "MPDM",
            "OSL"
        ],
        "Similarity search": [
            "OPSL",
            "FASF"
        ],
        "Software Framework": [
            "SNAPML"
        ],
        "Sparsity": [
            "HCCRMs"
        ],
        "Spectral clustering": [
            "GDMSPfEC"
        ],
        "Speech Recognition": [
            "HitNet",
            "LSNNs"
        ],
        "Statistical Behavior": [
            "NNREVG"
        ],
        "Supervised Learning": [
            "SAE",
            "DSDH",
            "DTSH"
        ],
        "Supervised learning": [
            "AMLE"
        ],
        "Task conditioning": [
            "TEN"
        ],
        "Text Generation": [
            "AIM"
        ],
        "The Description": [
            "DLDL"
        ],
        "Time Series Imputation": [
            "GRUI"
        ],
        "Training": [
            "AMT",
            "AIMT"
        ],
        "Transfer Learning": [
            "ADEM",
            "ADAPTHISTLOSS",
            "ADAPTPROTONET",
            "SYNPO"
        ],
        "Unsupervised learning": [
            "ACA",
            "DNNBNS"
        ],
        "VQA": [
            "Bottom-Up",
            "BAN"
        ],
        "Variance Reduction": [
            "SPIDER"
        ],
        "Variance reduction": [
            "SPIDER+"
        ],
        "Variational Inference": [
            "IVI"
        ],
        "Video Prediction": [
            "VPSS"
        ],
        "Video Recognition": [
            "A^2NET"
        ],
        "Visual Question Answering": [
            "NS-VQA",
            "VQAAR",
            "OOB"
        ],
        "Visual question answering": [
            "CBIP",
            "COR"
        ],
        "Word Embedding": [
            "FRAGE"
        ],
        "Zero-Shot Learning": [
            "PROTONET"
        ],
        "Zero-Shot Transfer": [
            "DOOMDP",
            "DOORMAXD"
        ],
        "Zero-shot Learning": [
            "SCR",
            "DIPL"
        ],
        "Zero-shot learning": [
            "GZSL",
            "DCN"
        ],
        "detection": [
            "SplineNets"
        ],
        "few-shot learning": [
            "FSTL"
        ],
        "local differential privacy": [
            "THRESH"
        ],
        "multi armed bandit": [
            "BEHPB"
        ],
        "peak signal-to-noise ratio": [
            "AORWS"
        ],
        "recognition": [
            "SplineNets"
        ]
    }
}